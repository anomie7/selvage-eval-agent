# Selvage 평가 에이전트 - 아키텍처

## 프로젝트 개요
AI 기반 코드 리뷰 도구인 Selvage를 평가하는 자동화 에이전트입니다. 4단계 워크플로우를 통해 모델별 성능과 프롬프트 버전 효과성을 정량적으로 측정합니다.

## 에이전트 아키텍처

### 설계 원칙
- **모듈화**: 각 단계를 독립적인 모듈로 구현
- **재현성**: JSON 기반 데이터 저장으로 테스트 재현 가능
- **확장성**: 새로운 모델 및 평가 지표 추가 용이
- **견고성**: 에러 처리 및 재시도 로직 내장

### 핵심 구현 요구사항
- **Python 3.10+** (타입 힌팅 필수)
- **Google 스타일 독스트링** (한국어 주석)
- **PEP 8 준수**

## Single Agent 아키텍처 패러다임

### ReAct (Reasoning + Acting) 패턴
Selvage 평가 에이전트는 단일 에이전트가 ReAct 패턴으로 두 가지 모드를 지원합니다:
1. **자동 실행 모드**: 4단계 워크플로우 동기적 순차 실행
2. **대화형 모드**: 사용자 요청에 따른 동기적 액션 실행

### 동기 처리 아키텍처 장점
- **단순성**: async/await 복잡성 제거로 코드 이해도 향상
- **디버깅 용이성**: 동기적 스택 트레이스로 문제 추적 간소화
- **유지보수성**: 일반적인 Python 프로그래밍 패턴 사용
- **성능**: I/O 병목이 없는 환경에서 오버헤드 감소

### Interactive Agent Interface

에이전트는 터미널에서 사용자와 실시간으로 상호작용하며 다음과 같은 요청을 처리합니다:

#### 지원하는 상호작용 유형
1. **Phase 관련 작업**
   - "Phase 1 상태 확인해줘"
   - "Phase 2 실행해줘" 
   - "어떤 단계까지 완료됐어?"

2. **저장된 Commit 관련 질문**
   - "cline 저장소 commit 목록 보여줘"
   - "선별된 commit들의 상세 정보는?"
   - "commit scoring 결과는?"

3. **리뷰 결과 데이터 질문**
   - "gemini-2.5-pro 리뷰 결과 보여줘"
   - "실패한 리뷰들은 어떤 것들이야?"
   - "모델별 리뷰 완료 현황은?"

4. **LLM Eval 실행 요청**
   - "deepeval 실행해줘"
   - "특정 모델 결과만 평가해줘"
   - "평가 재실행해줘"

5. **LLM Eval Result 분석**
   - "평가 결과 요약해줘"
   - "모델별 성능 비교해줘"
   - "어떤 모델이 가장 좋아?"

## 4단계 워크플로우

### Phase 1 - Commit Collection
- **목적**: meaningful한 커밋들을 자동 식별 및 배점
- **결과**: 평가 가치가 높은 커밋 리스트

### Phase 2 - Review Execution
- **목적**: 선별된 커밋에 대해 다중 모델로 Selvage 리뷰 실행
- **결과**: 모델별 리뷰 결과 로그

### Phase 3 - DeepEval Conversion
- **목적**: 리뷰 결과를 DeepEval 형식으로 변환 및 평가
- **결과**: 정량화된 평가 메트릭

### Phase 4 - Analysis & Insights
- **목적**: 통계 분석을 통한 actionable insights 도출 (복잡한 추론 필요)
- **결과**: 실행 가능한 권장사항 및 인사이트

## Phase 실행 전략
- **Phase 1-3**: 주로 도구 호출과 데이터 처리 중심
- **Phase 4**: AI 추론을 통한 패턴 분석 및 인사이트 도출
- 각 단계의 결과는 다음 단계의 입력으로 사용
- 실패 시 재시도 로직 내장

## 결정 원칙
- **데이터 기반**: 모든 결정은 정량적 데이터에 근거
- **재현성**: 동일 조건에서 동일 결과 보장
- **효율성**: 적절한 도구 선택 및 캐싱 활용
- **신뢰성**: 에러 처리 및 복구 메커니즘 내장