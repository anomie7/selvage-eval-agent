# Selvage í‰ê°€ ì—ì´ì „íŠ¸ - DeepEval ê²°ê³¼ ë¶„ì„ ì „ëµ (5ë‹¨ê³„)

## ê°œìš”

DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ, ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„, Selvage ë²„ì „ë³„ ê°œì„  ì¶”ì ì„ ìœ„í•œ ì¢…í•© ë¶„ì„ ì „ëµì„ ì œì‹œí•©ë‹ˆë‹¤.

## 1. DeepEval ê²°ê³¼ íŒŒì¼ êµ¬ì¡° ë¶„ì„

### 1.1 ì‹¤ì œ íŒŒì¼ êµ¬ì¡°

**ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°:**
```
~/Library/selvage-eval/deepeval_results/{session_id}/
â”œâ”€â”€ metadata.json                    # ì„¸ì…˜ ë©”íƒ€ë°ì´í„°
â”œâ”€â”€ {model_name_1}/                  # ëª¨ë¸ë³„ ë””ë ‰í† ë¦¬
â”‚   â”œâ”€â”€ test_run_20250708_004754.log # ê°œë³„ í‰ê°€ ì‹¤í–‰ ë¡œê·¸
â”‚   â”œâ”€â”€ test_run_20250708_005356.log
â”‚   â””â”€â”€ ...
â”œâ”€â”€ {model_name_2}/
â”‚   â”œâ”€â”€ test_run_20250708_004441.log
â”‚   â””â”€â”€ ...
â””â”€â”€ ...
```

**ì‹¤ì œ í™•ì¸ëœ ëª¨ë¸ ë””ë ‰í† ë¦¬:**
- `gemini-2.5-flash/`
- `gemini-2.5-pro/`
- `o3/`
- `o4-mini-high/`
(ì£¼ì˜ : ëª¨ë¸ ë””ë ‰í† ë¦¬ëŠ” ê³ ì •ëœ ê²ƒì´ ì•„ë‹Œ ì„¸ì…˜ë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ )

### 1.2 ë©”íƒ€ë°ì´í„° ìŠ¤í‚¤ë§ˆ

**metadata.json êµ¬ì¡°:**
```json
{
  "selvage_version": "selvage 0.1.2",
  "execution_date": "2025-07-08T01:07:09.148180",
  "session_id": "eval_20250707_174243_e4df05f6",
  "deep_eval_test_case_path": "/Users/demin_coder/Library/selvage-eval/deep_eval_test_case/eval_20250707_174243_e4df05f6",
  "tool_name": "deepeval_executor",
  "created_by": "DeepEvalExecutorTool"
}
```

### 1.3 ë¡œê·¸ íŒŒì¼ ë‚´ìš© êµ¬ì¡°

**íŒŒì¼ íŠ¹ì„±:**
- í¬ê¸°: í‰ê·  500KB+ (ëŒ€ìš©ëŸ‰ íŒŒì¼)
- í˜•ì‹: í…ìŠ¤íŠ¸ ë¡œê·¸ (JSON í˜•íƒœ ì•„ë‹˜)
- ì¸ì½”ë”©: UTF-8

**ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ êµ¬ì¡°:**
```
======================================================================

Metrics Summary

  - âœ…/âŒ Correctness [GEval] (score: 1.0, threshold: 0.7, strict: False, evaluation model: gemini-2.5-pro, reason: "ìƒì„¸í•œ í‰ê°€ ì´ìœ ...", error: None)
  - âœ…/âŒ Clarity [GEval] (score: 1.0, threshold: 0.7, strict: False, evaluation model: gemini-2.5-pro, reason: "ìƒì„¸í•œ í‰ê°€ ì´ìœ ...", error: None)  
  - âœ…/âŒ Actionability [GEval] (score: 1.0, threshold: 0.7, strict: False, evaluation model: gemini-2.5-pro, reason: "ìƒì„¸í•œ í‰ê°€ ì´ìœ ...", error: None)
  - âœ…/âŒ Json Correctness (score: 1.0, threshold: 1.0, strict: True, evaluation model: None, reason: "ìƒì„¸í•œ í‰ê°€ ì´ìœ ...", error: None)

For test case:

  - input: [ì‹œìŠ¤í…œ ë©”ì‹œì§€ ë° ì‚¬ìš©ì ì…ë ¥ JSON]
  - actual output: {Selvageê°€ ìƒì„±í•œ ì‹¤ì œ ë¦¬ë·° ê²°ê³¼ JSON}
```

## 2. ë°ì´í„° ì¶”ì¶œ ë° ì²˜ë¦¬ ì „ëµ

### 2.1 ëŒ€ìš©ëŸ‰ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì „ëµ

**ë¬¸ì œì :**
- ë‹¨ì¼ íŒŒì¼ í¬ê¸°ê°€ 500KB+ ë¡œ ë§¤ìš° í¼
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬ í•„ìš”
- êµ¬ì¡°í™”ë˜ì§€ ì•Šì€ í…ìŠ¤íŠ¸ í˜•íƒœ

**í•´ê²° ë°©ì•ˆ:**

```python
from typing import Iterator, Dict, Any, Optional
import re
from pathlib import Path

class DeepEvalLogParser:
    """ëŒ€ìš©ëŸ‰ DeepEval ë¡œê·¸ íŒŒì¼ íŒŒì„œ"""
    
    def __init__(self):
        self.test_case_separator = "=" * 70
        self.metrics_pattern = re.compile(
            r'(âœ…|âŒ)\s+(\w+)\s+.*?\(score:\s+([\d.]+),.*?reason:\s+"([^"]*)".*?error:\s+([^)]*)\)'
        )
        
    def parse_log_file(self, log_path: Path) -> Iterator[Dict[str, Any]]:
        """ë¡œê·¸ íŒŒì¼ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ íŒŒì‹±"""
        with open(log_path, 'r', encoding='utf-8') as f:
            current_test_case = None
            buffer = []
            
            for line in f:
                if self.test_case_separator in line:
                    if current_test_case:
                        yield self._parse_test_case(buffer)
                    current_test_case = {}
                    buffer = []
                else:
                    buffer.append(line)
            
            # ë§ˆì§€ë§‰ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì²˜ë¦¬
            if buffer:
                yield self._parse_test_case(buffer)
    
    def _parse_test_case(self, lines: list[str]) -> Dict[str, Any]:
        """ê°œë³„ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒŒì‹±"""
        content = ''.join(lines)
        
        # ë©”íŠ¸ë¦­ ì •ë³´ ì¶”ì¶œ
        metrics = {}
        for match in self.metrics_pattern.finditer(content):
            status_icon, metric_name, score, reason, error = match.groups()
            metrics[metric_name.lower()] = {
                'status': status_icon,
                'score': float(score),
                'reason': reason,
                'error': error if error != 'None' else None,
                'passed': status_icon == 'âœ…'
            }
        
        # ì…ë ¥/ì¶œë ¥ ë°ì´í„° ì¶”ì¶œ
        input_match = re.search(r'input:\s*(\[.*?\])', content, re.DOTALL)
        output_match = re.search(r'actual output:\s*(\{.*?\})', content, re.DOTALL)
        
        return {
            'metrics': metrics,
            'input': input_match.group(1) if input_match else None,
            'actual_output': output_match.group(1) if output_match else None,
            'raw_content': content
        }
```

### 2.2 ë©”íŠ¸ë¦­ ì ìˆ˜ ì§‘ê³„ ì•Œê³ ë¦¬ì¦˜

```python
from dataclasses import dataclass
from typing import List, Dict
import numpy as np

@dataclass
class MetricScore:
    """ê°œë³„ ë©”íŠ¸ë¦­ ì ìˆ˜"""
    score: float
    passed: bool
    reason: str
    error: Optional[str] = None

@dataclass
class TestCaseResult:
    """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²°ê³¼"""
    correctness: MetricScore
    clarity: MetricScore
    actionability: MetricScore
    json_correctness: MetricScore
    input_data: str
    actual_output: str

class MetricAggregator:
    """ë©”íŠ¸ë¦­ ì ìˆ˜ ì§‘ê³„ê¸°"""
    
    def aggregate_model_performance(self, 
                                  test_results: List[TestCaseResult]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ê³„ì‚°"""
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        aggregated = {}
        
        for metric in metrics:
            scores = [getattr(result, metric).score for result in test_results]
            passed_count = sum(1 for result in test_results 
                             if getattr(result, metric).passed)
            
            aggregated[metric] = {
                'mean_score': np.mean(scores),
                'median_score': np.median(scores),
                'std_score': np.std(scores),
                'min_score': np.min(scores),
                'max_score': np.max(scores),
                'pass_rate': passed_count / len(test_results),
                'total_cases': len(test_results),
                'passed_cases': passed_count,
                'failed_cases': len(test_results) - passed_count
            }
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
        weights = {
            'correctness': 0.4,
            'clarity': 0.25, 
            'actionability': 0.25,
            'json_correctness': 0.1
        }
        
        overall_score = sum(
            aggregated[metric]['mean_score'] * weight 
            for metric, weight in weights.items()
        )
        
        aggregated['overall'] = {
            'weighted_score': overall_score,
            'grade': self._assign_grade(overall_score),
            'consistency': 1.0 - np.mean([aggregated[m]['std_score'] for m in metrics])
        }
        
        return aggregated
    
    def _assign_grade(self, score: float) -> str:
        """ì ìˆ˜ ê¸°ë°˜ ë“±ê¸‰ í• ë‹¹"""
        if score >= 0.9: return 'A+'
        elif score >= 0.85: return 'A'
        elif score >= 0.8: return 'B+'
        elif score >= 0.75: return 'B'
        elif score >= 0.7: return 'C+'
        elif score >= 0.65: return 'C'
        elif score >= 0.6: return 'D'
        else: return 'F'
```

## 3. ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì²´ê³„

### 3.1 ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ë¥˜ ì²´ê³„

```python
from typing import List, Dict, Tuple
import numpy as np
import json

# FailureCategory enum ì œê±°: Geminiê°€ ììœ ë¡­ê²Œ ì¹´í…Œê³ ë¦¬ë¥¼ ìƒì„±í•˜ë„ë¡ í•¨
# ë” ìœ ì—°í•˜ê³  ì •í™•í•œ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ì„ ìœ„í•´ ë¯¸ë¦¬ ì •ì˜ëœ ì¹´í…Œê³ ë¦¬ ì œí•œì„ ì œê±°

class GeminiFailureAnalyzer:
    """Gemini ê¸°ë°˜ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.gemini_client = self._initialize_gemini_client()
        self.cache = {}  # ë¹„ìš© íš¨ìœ¨ì„±ì„ ìœ„í•œ ìºì‹œ
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ ë°œìƒ
        if not self.gemini_client:
            raise RuntimeError(
                "Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. "
                "GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ê³  API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
        
    def _initialize_gemini_client(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (structured output ì§€ì›)"""
        try:
            import google.generativeai as genai
            import os
            
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                print("ERROR: GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
                
            genai.configure(api_key=api_key)
            
            # structured outputì„ ìœ„í•œ ìƒì„± ì„¤ì •
            generation_config = {
                "response_mime_type": "application/json",
                "response_schema": {
                    "type": "object",
                    "properties": {
                        "category": {
                            "type": "string",
                            "description": "ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ë¶„ë¥˜í•œ ì¹´í…Œê³ ë¦¬ëª… (ì˜ì–´ ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤)"
                        },
                        "confidence": {
                            "type": "number",
                            "minimum": 0.0,
                            "maximum": 1.0,
                            "description": "ë¶„ë¥˜ ì‹ ë¢°ë„ (0.0-1.0)"
                        },
                        "explanation": {
                            "type": "string",
                            "description": "ì¹´í…Œê³ ë¦¬ ì„ íƒ ì´ìœ  ë° Selvage ê°œì„  ë°©í–¥"
                        }
                    },
                    "required": ["category", "confidence", "explanation"]
                }
            }
            
            model = genai.GenerativeModel(
                'gemini-2.5-flash',
                generation_config=generation_config
            )
            
            # structured output í…ŒìŠ¤íŠ¸
            test_prompt = """
í…ŒìŠ¤íŠ¸ìš© ì‹¤íŒ¨ ì‚¬ìœ : "The review output format is incorrect"
ìœ„ ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.
            """
            test_response = model.generate_content(test_prompt)
            
            if not test_response or not test_response.text:
                print("ERROR: Gemini structured output í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
                return None
                
            # JSON íŒŒì‹± í…ŒìŠ¤íŠ¸
            try:
                json.loads(test_response.text)
                print("Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ (structured output ì§€ì›)")
            except json.JSONDecodeError:
                print("WARNING: structured output í…ŒìŠ¤íŠ¸ì—ì„œ JSON íŒŒì‹± ì‹¤íŒ¨, ê³„ì† ì§„í–‰")
                
            return model
            
        except Exception as e:
            print(f"ERROR: Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return None
    
    def categorize_failure(self, reason: str, failed_metric: str = None) -> Tuple[str, float]:
        """ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ììœ  í˜•ì‹ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜ (ì‹ ë¢°ë„ ì ìˆ˜ í¬í•¨)
        
        Args:
            reason: ì‹¤íŒ¨ ì‚¬ìœ  í…ìŠ¤íŠ¸
            failed_metric: ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­ ì´ë¦„ (correctness, clarity, actionability, json_correctness)
        """
        # ìºì‹œ í‚¤ì— failed_metric í¬í•¨
        cache_key = hash(f"{reason}:{failed_metric or 'unknown'}")
        if cache_key in self.cache:
            return self.cache[cache_key]
        
        # Gemini ë¶„ë¥˜ ì‹¤í–‰ (í•„ìˆ˜)
        try:
            category, confidence = self._gemini_categorize_failure(reason, failed_metric)
            self.cache[cache_key] = (category, confidence)
            return category, confidence
        except Exception as e:
            error_msg = f"Gemini ë¶„ë¥˜ ì‹¤íŒ¨ - ë©”íŠ¸ë¦­: {failed_metric or 'unknown'}, ì‹¤íŒ¨ ì‚¬ìœ : '{reason[:100]}{'...' if len(reason) > 100 else ''}', ì˜¤ë¥˜: {e}"
            print(f"ERROR: {error_msg}")
            raise RuntimeError(error_msg) from e
    
    def _gemini_categorize_failure(self, reason: str, failed_metric: str = None) -> Tuple[str, float]:
        """Geminië¥¼ ì‚¬ìš©í•œ ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ë¥˜"""
        # ë©”íŠ¸ë¦­ë³„ ì „ë¬¸ ì»¨í…ìŠ¤íŠ¸ ì„¤ëª…
        metric_contexts = {
            'correctness': {
                'description': 'ë¦¬ë·°ì˜ ì •í™•ì„± - ì½”ë“œ ì´ìŠˆë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‹ë³„í–ˆëŠ”ê°€?',
                'common_failures': 'ì´ìŠˆ ë¯¸íƒì§€, ì˜ëª»ëœ ì§„ë‹¨, ì¤‘ìš”ë„ ì˜¤íŒ, ê·¸ë§ì‹¸ ì˜¤íƒ'
            },
            'clarity': {
                'description': 'ë¦¬ë·°ì˜ ëª…í™•ì„± - ê°œë°œìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì¸ê°€?',
                'common_failures': 'ëª¨í˜¸í•œ ì„¤ëª…, ì „ë¬¸ìš©ì–´ ë‚¨ìš©, ë¶ˆëª…í™•í•œ í‘œí˜„, ì´í•´í•˜ê¸° ì–´ë ¤ìš´ êµ¬ì¡°'
            },
            'actionability': {
                'description': 'ì‹¤í–‰ê°€ëŠ¥ì„± - êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí–ˆëŠ”ê°€?',
                'common_failures': 'ì¶”ìƒì  ì œì•ˆ, ë¹„í˜„ì‹¤ì  ì¡°ì¹˜, êµ¬ì²´ì„± ë¶€ì¡±, ì‹¤í–‰ ë°©ë²• ë¯¸ì œì‹œ'
            },
            'json_correctness': {
                'description': 'JSON í˜•ì‹ ìœ íš¨ì„± - ì˜¬ë°”ë¥¸ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ëŠ”ê°€?',
                'common_failures': 'JSON êµ¬ë¬¸ ì˜¤ë¥˜, ìŠ¤í‚¤ë§ˆ ë¶ˆì¼ì¹˜, í•„ë“œ ëˆ„ë½, ë°ì´í„° íƒ€ì… ì˜¤ë¥˜'
            }
        }
        
        # ë©”íŠ¸ë¦­ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        metric_context = ""
        if failed_metric and failed_metric in metric_contexts:
            context = metric_contexts[failed_metric]
            metric_context = f"""
## ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­: {failed_metric.upper()}
**ë©”íŠ¸ë¦­ ì„¤ëª…:** {context['description']}
**ì¼ë°˜ì  ì‹¤íŒ¨ íŒ¨í„´:** {context['common_failures']}

ì´ ë©”íŠ¸ë¦­ì—ì„œ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
        elif failed_metric:
            metric_context = f"""
## ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­: {failed_metric.upper()}
ì´ ë©”íŠ¸ë¦­ì—ì„œ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ì—¬ ë¶„ì„í•´ì£¼ì„¸ìš”.
"""
        
        prompt = f"""
## ì»¨í…ìŠ¤íŠ¸
ë‹¹ì‹ ì€ LLM ê¸°ë°˜ ì½”ë“œ ë¦¬ë·° ë„êµ¬ 'Selvage'ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

### í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¡°:
1. **Selvage**: AI ê¸°ë°˜ ì½”ë“œ ë¦¬ë·° ë„êµ¬ (í”¼í‰ê°€ ì‹œìŠ¤í…œ)
   - ì½”ë“œ ë³€ê²½ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ ë¦¬ë·° í”¼ë“œë°± ìƒì„±
   - JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ë¦¬ë·° ê²°ê³¼ ì¶œë ¥

2. **DeepEval**: LLM í‰ê°€ í”„ë ˆì„ì›Œí¬ (í‰ê°€ ì‹œìŠ¤í…œ)
   - Selvageê°€ ìƒì„±í•œ ë¦¬ë·°ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‰ê°€
   - ê° ë©”íŠ¸ë¦­ì€ 0.7 ì´ìƒ ì ìˆ˜ ì‹œ í†µê³¼, ë¯¸ë‹¬ ì‹œ ì‹¤íŒ¨

### DeepEval í‰ê°€ ë©”íŠ¸ë¦­:
- **Correctness**: ë¦¬ë·°ì˜ ì •í™•ì„± (ì½”ë“œ ì´ìŠˆë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‹ë³„í–ˆëŠ”ê°€?)
- **Clarity**: ë¦¬ë·°ì˜ ëª…í™•ì„± (ê°œë°œìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„¤ëª…ì¸ê°€?)
- **Actionability**: ì‹¤í–‰ê°€ëŠ¥ì„± (êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆì„ ì œì‹œí–ˆëŠ”ê°€?)
- **JSON Correctness**: JSON í˜•ì‹ ìœ íš¨ì„± (ì˜¬ë°”ë¥¸ JSON ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥´ëŠ”ê°€?)
{metric_context}
## ë¶„ì„ ëŒ€ìƒ
ë‹¤ìŒì€ DeepEvalì´ Selvageì˜ ë¦¬ë·° ê²°ê³¼ë¥¼ í‰ê°€í•œ í›„, íŠ¹ì • ë©”íŠ¸ë¦­ì—ì„œ **ì‹¤íŒ¨(threshold 0.7 ë¯¸ë‹¬)**í•œ ì‚¬ë¡€ì˜ ì‹¤íŒ¨ ì‚¬ìœ ì…ë‹ˆë‹¤.

**ì‹¤íŒ¨ ì‚¬ìœ :**
{reason}

## ë¶„ì„ ìš”ì²­
ìœ„ ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ë¶„ì„í•˜ì—¬ Selvageì˜ ì½”ë“œ ë¦¬ë·° í’ˆì§ˆ ë¬¸ì œë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” ì¹´í…Œê³ ë¦¬ëª…ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

### ë¶„ë¥˜ ê´€ì :
- **ì½”ë“œ ë¶„ì„ ëŠ¥ë ¥**: ì½”ë“œ ì´ìŠˆ íƒì§€, ë¼ì¸ ë²ˆí˜¸ ì •í™•ì„±, ì¤‘ìš”ë„ íŒë‹¨ ë“±
- **ì˜ì‚¬ì†Œí†µ ëŠ¥ë ¥**: ì„¤ëª…ì˜ ëª…í™•ì„±, ê°œë°œì ì¹œí™”ì  í‘œí˜„, ì „ë¬¸ìš©ì–´ ì‚¬ìš© ë“±  
- **ì‹¤ìš©ì„±**: êµ¬ì²´ì  í•´ê²°ë°©ì•ˆ ì œì‹œ, ì‹¤í–‰ ê°€ëŠ¥í•œ ì œì•ˆ, ìš°ì„ ìˆœìœ„ ê°€ì´ë“œ ë“±
- **ê¸°ìˆ ì  ì •í™•ì„±**: JSON í˜•ì‹ ì¤€ìˆ˜, ìŠ¤í‚¤ë§ˆ ì¼ì¹˜ì„±, êµ¬ì¡°ì  ì™„ì„±ë„ ë“±
- **ë„ë©”ì¸ íŠ¹í™”**: ë³´ì•ˆ, ì„±ëŠ¥, ê°€ë…ì„±, ìœ ì§€ë³´ìˆ˜ì„± ë“± íŠ¹ì • ì˜ì—­ ì „ë¬¸ì„±

### ì§€ì¹¨:
- ì‹¤íŒ¨ì˜ ê·¼ë³¸ ì›ì¸ì„ ë°˜ì˜í•˜ëŠ” êµ¬ì²´ì ì¸ ì¹´í…Œê³ ë¦¬ëª… ìƒì„±
- ì˜ì–´ ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤ ì‚¬ìš© (ì˜ˆ: "missing_security_vulnerabilities", "vague_improvement_suggestions")
- Selvage ê°œì„ ì— ì§ì ‘ì ìœ¼ë¡œ ë„ì›€ì´ ë˜ëŠ” ì‹¤í–‰ ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬ëª… ì„ í˜¸
- ìœ ì‚¬í•œ ì‹¤íŒ¨ íŒ¨í„´ë“¤ì„ ê·¸ë£¹í•‘í•  ìˆ˜ ìˆëŠ” ì¼ë°˜ì  ìˆ˜ì¤€ì˜ ì¶”ìƒí™”

ì‘ë‹µ í˜•ì‹:
JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¼ ì‘ë‹µí•´ì£¼ì„¸ìš”:
```json
{
  "category": "ì¹´í…Œê³ ë¦¬ëª… (ì˜ì–´ ìŠ¤ë„¤ì´í¬ì¼€ì´ìŠ¤)",
  "confidence": 0.95,
  "explanation": "ì¹´í…Œê³ ë¦¬ ì„ íƒ ì´ìœ  ë° Selvage ê°œì„  ë°©í–¥"
}
\`\`\`
"""
        
        response = self.gemini_client.generate_content(prompt)
        return self._parse_gemini_response(response.text)
    
    def _parse_gemini_response(self, response_text: str) -> Tuple[str, float]:
        """Gemini structured output ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON íŒŒì‹±
            response_data = json.loads(response_text.strip())
            
            category = response_data.get('category', 'unknown_failure')
            confidence = float(response_data.get('confidence', 0.8))
            explanation = response_data.get('explanation', '')
            
            # ì¹´í…Œê³ ë¦¬ëª… ì •ê·œí™” (ê³µë°±ì„ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ ë³€í™˜, ì†Œë¬¸ì ë³€í™˜)
            category = category.lower().replace(' ', '_').replace('-', '_')
            
            # ì‹ ë¢°ë„ ë²”ìœ„ ê²€ì¦
            confidence = max(0.0, min(1.0, confidence))
            
            print(f"Gemini ë¶„ë¥˜ ê²°ê³¼: {category} (ì‹ ë¢°ë„: {confidence:.3f}) - {explanation[:100]}")
            
            return category, confidence
            
        except json.JSONDecodeError as e:
            print(f"WARNING: Gemini JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"Raw response: {response_text[:200]}...")
            
            # fallback: ê¸°ë³¸ ë¬¸ìì—´ íŒŒì‹± ì‹œë„
            return self._fallback_parse_response(response_text)
        
        except (KeyError, ValueError, TypeError) as e:
            print(f"WARNING: Gemini ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._fallback_parse_response(response_text)
    
    def _fallback_parse_response(self, response_text: str) -> Tuple[str, float]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback íŒŒì‹±"""
        # ê¸°ë³¸ ë¬¸ìì—´ íŒŒì‹± ì‹œë„
        lines = response_text.strip().split('\n')
        category_str = "unknown_failure"
        confidence = 0.8
        
        for line in lines:
            line = line.strip()
            if 'ì¹´í…Œê³ ë¦¬' in line and ':' in line:
                category_str = line.split(':')[1].strip()
            elif 'ì‹ ë¢°ë„' in line and ':' in line:
                try:
                    confidence = float(line.split(':')[1].strip())
                except ValueError:
                    pass
        
        # ì¹´í…Œê³ ë¦¬ëª… ì •ê·œí™”
        category_str = category_str.lower().replace(' ', '_').replace('-', '_')
        print(f"Fallback íŒŒì‹± ê²°ê³¼: {category_str} (ì‹ ë¢°ë„: {confidence})")
        
        return category_str, confidence


class FailurePatternAnalyzer:
    """ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ê¸° (í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
    
    def __init__(self):
        self.gemini_analyzer = GeminiFailureAnalyzer()
    
    def analyze_failure_patterns(self, 
                               failed_cases: List[TestCaseResult]) -> Dict[str, Any]:
        """ì‹¤íŒ¨ íŒ¨í„´ ì¢…í•© ë¶„ì„"""
        patterns = {
            'total_failures': len(failed_cases),
            'by_metric': {},
            'by_category': {},
            'critical_patterns': [],
            'confidence_scores': {}  # ë¶„ë¥˜ ì‹ ë¢°ë„ ì¶”ê°€
        }
        
        # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„ì„
        for metric in ['correctness', 'clarity', 'actionability', 'json_correctness']:
            metric_failures = [
                case for case in failed_cases 
                if not getattr(case, metric).passed
            ]
            
            categories = {}
            confidences = []
            for case in metric_failures:
                reason = getattr(case, metric).reason
                category, confidence = self.gemini_analyzer.categorize_failure(reason, metric)
                categories[category] = categories.get(category, 0) + 1
                confidences.append(confidence)
            
            patterns['by_metric'][metric] = {
                'total_failures': len(metric_failures),
                'failure_rate': len(metric_failures) / len(failed_cases) if failed_cases else 0,
                'categories': categories,
                'worst_cases': self._extract_worst_cases(metric_failures, metric),
                'avg_confidence': np.mean(confidences) if confidences else 0
            }
        
        # ì „ì²´ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        all_categories = {}
        all_confidences = []
        for case in failed_cases:
            for metric in ['correctness', 'clarity', 'actionability', 'json_correctness']:
                if not getattr(case, metric).passed:
                    reason = getattr(case, metric).reason
                    category, confidence = self.gemini_analyzer.categorize_failure(reason, metric)
                    all_categories[category] = all_categories.get(category, 0) + 1
                    all_confidences.append(confidence)
        
        patterns['by_category'] = all_categories
        patterns['confidence_scores']['overall'] = np.mean(all_confidences) if all_confidences else 0
        
        
        return patterns
    
    def _extract_worst_cases(self, failures: List[TestCaseResult], 
                           metric: str) -> List[Dict[str, Any]]:
        """ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ì¶”ì¶œ"""
        metric_scores = [(case, getattr(case, metric).score) for case in failures]
        worst_cases = sorted(metric_scores, key=lambda x: x[1])[:5]
        
        return [
            {
                'score': score,
                'reason': getattr(case, metric).reason,
                'input_preview': case.input_data[:200] + '...' if len(case.input_data) > 200 else case.input_data
            }
            for case, score in worst_cases
        ]
    
    # ê°œì„  ì œì•ˆ ê¸°ëŠ¥ ì œê±° - í˜„ì¬ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë¸ë³„/ë²„ì „ë³„ ë¹„êµ ë¶„ì„ì— ì§‘ì¤‘
    # ê°œì„  ì œì•ˆì€ ì¶”í›„ í•„ìš” ì‹œ ì¶”ê°€ ì˜ˆì •
    

## 4. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„

### 4.1 í†µê³„ì  ë¹„êµ ë°©ë²•ë¡ 

```python
from scipy import stats
from typing import List, Dict, Tuple
import pandas as pd

class ModelPerformanceComparator:
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.significance_level = 0.05
        self.effect_size_thresholds = {
            'small': 0.2,
            'medium': 0.5, 
            'large': 0.8
        }
    
    def compare_models(self, 
                      model_results: Dict[str, List[TestCaseResult]]) -> Dict[str, Any]:
        """nê°œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë¶„ì„"""
        model_names = list(model_results.keys())
        
        # ëª¨ë¸ë³„ ê¸°ë³¸ í†µê³„
        model_stats = {}
        for model_name, results in model_results.items():
            aggregator = MetricAggregator()
            model_stats[model_name] = aggregator.aggregate_model_performance(results)
        
        # ì „ì²´ ìˆœìœ„ ê³„ì‚°
        rankings = self._calculate_model_rankings(model_stats)
        
        # ì¢…í•© ë¹„êµ í‘œ ìƒì„±
        comparison_table = self._create_comparison_table(model_stats, rankings)
        
        # n-ëª¨ë¸ í†µê³„ ë¶„ì„ (ANOVA)
        statistical_analysis = self._n_model_statistical_analysis(model_results)
        
        return {
            'model_statistics': model_stats,
            'comparison_table': comparison_table,
            'rankings': rankings,
            'statistical_analysis': statistical_analysis,
            'recommendations': self._generate_model_recommendations(model_stats, rankings)
        }
    
    def _create_comparison_table(self, 
                               model_stats: Dict[str, Any],
                               rankings: Dict[str, Any]) -> Dict[str, Any]:
        """nê°œ ëª¨ë¸ ì¢…í•© ë¹„êµ í‘œ ìƒì„±"""
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        models = list(model_stats.keys())
        
        # ë¹„êµ í‘œ ë°ì´í„° êµ¬ì„±
        table_data = []
        for model in models:
            stats = model_stats[model]
            row = {
                'model': model,
                'overall_score': stats['overall']['weighted_score'],
                'overall_rank': next((entry['rank'] for entry in rankings['overall'] 
                                    if entry['model'] == model), 'N/A'),
                'total_test_cases': stats['overall']['total_cases'],
                'pass_rate': stats['overall']['pass_rate'],
                'failure_count': stats['overall']['total_failures'],
                'grade': stats['overall']['grade'],
                'tier': self._get_model_tier(stats['overall']['weighted_score'])
            }
            
            # ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ì ìˆ˜ ì¶”ê°€
            for metric in metrics:
                metric_data = stats[metric]
                row[f'{metric}_score'] = metric_data['mean_score']
                row[f'{metric}_rank'] = next((entry['rank'] for entry in rankings['by_metric'][metric] 
                                            if entry['model'] == model), 'N/A')
                row[f'{metric}_failures'] = metric_data['failure_count']
            
            table_data.append(row)
        
        # ì ìˆ˜ë³„ ì •ë ¬
        table_data.sort(key=lambda x: x['overall_score'], reverse=True)
        
        return {
            'table_data': table_data,
            'column_headers': self._get_table_headers(),
            'summary_stats': self._calculate_table_summary(table_data),
            'performance_gaps': self._calculate_performance_gaps(table_data)
        }
    
    def _get_model_tier(self, score: float) -> str:
        """ì ìˆ˜ ê¸°ë°˜ ëª¨ë¸ ë“±ê¸‰ ê²°ì •"""
        if score >= 0.85:
            return "Tier 1 (ìš°ìˆ˜)"
        elif score >= 0.75:
            return "Tier 2 (ì–‘í˜¸)"
        elif score >= 0.65:
            return "Tier 3 (ë³´í†µ)"
        else:
            return "Tier 4 (ê°œì„ í•„ìš”)"
    
    def _get_table_headers(self) -> List[str]:
        """ë¹„êµ í‘œ í—¤ë” ì •ì˜"""
        return [
            'Model', 'Overall Score', 'Rank', 'Test Cases', 'Pass Rate', 
            'Failures', 'Grade', 'Tier', 'Correctness', 'Clarity', 
            'Actionability', 'JSON Correctness'
        ]
    
    def _calculate_table_summary(self, table_data: List[Dict]) -> Dict[str, Any]:
        """ë¹„êµ í‘œ ìš”ì•½ í†µê³„"""
        scores = [row['overall_score'] for row in table_data]
        return {
            'best_model': table_data[0]['model'] if table_data else None,
            'worst_model': table_data[-1]['model'] if table_data else None,
            'score_range': {
                'max': max(scores) if scores else 0,
                'min': min(scores) if scores else 0,
                'mean': np.mean(scores) if scores else 0,
                'std': np.std(scores) if scores else 0
            },
            'tier_distribution': self._count_tier_distribution(table_data)
        }
    
    def _count_tier_distribution(self, table_data: List[Dict]) -> Dict[str, int]:
        """ë“±ê¸‰ë³„ ëª¨ë¸ ë¶„í¬ ê³„ì‚°"""
        tier_count = {}
        for row in table_data:
            tier = row['tier']
            tier_count[tier] = tier_count.get(tier, 0) + 1
        return tier_count
    
    def _calculate_performance_gaps(self, table_data: List[Dict]) -> Dict[str, float]:
        """ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„"""
        if len(table_data) < 2:
            return {}
        
        best_score = table_data[0]['overall_score']
        worst_score = table_data[-1]['overall_score']
        
        return {
            'max_gap': best_score - worst_score,
            'relative_gap_percentage': ((best_score - worst_score) / worst_score) * 100 if worst_score > 0 else 0,
            'tier1_threshold_gap': max(0, 0.85 - worst_score),
            'competitive_threshold': 0.1  # ìƒìœ„ 10% ì´ë‚´ë¥¼ ê²½ìŸë ¥ ìˆëŠ” ëª¨ë¸ë¡œ ê°„ì£¼
        }
    
    def _n_model_statistical_analysis(self, 
                                    model_results: Dict[str, List[TestCaseResult]]) -> Dict[str, Any]:
        """nê°œ ëª¨ë¸ í†µê³„ ë¶„ì„ (ANOVA/Kruskal-Wallis)"""
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        analysis_results = {}
        
        for metric in metrics:
            # ê° ëª¨ë¸ë³„ ì ìˆ˜ ìˆ˜ì§‘
            model_scores = {}
            for model_name, results in model_results.items():
                scores = [getattr(result, metric).score for result in results]
                model_scores[model_name] = scores
            
            # ANOVA ê²€ì • (ì •ê·œì„± ê°€ì •)
            score_groups = list(model_scores.values())
            if len(score_groups) >= 2 and all(len(group) > 0 for group in score_groups):
                try:
                    f_stat, p_value_anova = stats.f_oneway(*score_groups)
                    
                    # Kruskal-Wallis ê²€ì • (ë¹„ëª¨ìˆ˜ì  ëŒ€ì•ˆ)
                    h_stat, p_value_kw = stats.kruskal(*score_groups)
                    
                    analysis_results[metric] = {
                        'anova': {
                            'f_statistic': f_stat,
                            'p_value': p_value_anova,
                            'significant': p_value_anova < self.significance_level
                        },
                        'kruskal_wallis': {
                            'h_statistic': h_stat,
                            'p_value': p_value_kw,
                            'significant': p_value_kw < self.significance_level
                        },
                        'interpretation': self._interpret_n_model_test_results(p_value_anova, p_value_kw)
                    }
                except Exception as e:
                    analysis_results[metric] = {
                        'error': f"í†µê³„ ë¶„ì„ ì‹¤íŒ¨: {str(e)}",
                        'interpretation': "ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
                    }
        
        return {
            'by_metric': analysis_results,
            'overall_conclusion': self._generate_statistical_conclusion(analysis_results)
        }
    
    def _interpret_n_model_test_results(self, p_anova: float, p_kw: float) -> str:
        """n-ëª¨ë¸ í†µê³„ ê²€ì • ê²°ê³¼ í•´ì„"""
        if p_anova < 0.001 and p_kw < 0.001:
            return "ëª¨ë¸ ê°„ ë§¤ìš° ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ì°¨ì´ ì¡´ì¬"
        elif p_anova < 0.01 and p_kw < 0.01:
            return "ëª¨ë¸ ê°„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ì°¨ì´ ì¡´ì¬"
        elif p_anova < 0.05 or p_kw < 0.05:
            return "ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ê°€ ì¡´ì¬í•  ê°€ëŠ¥ì„± ìˆìŒ"
        else:
            return "ëª¨ë¸ ê°„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ì°¨ì´ ì—†ìŒ"
    
    def _generate_statistical_conclusion(self, analysis_results: Dict[str, Any]) -> str:
        """í†µê³„ ë¶„ì„ ì¢…í•© ê²°ë¡ """
        significant_metrics = []
        for metric, result in analysis_results.items():
            if isinstance(result, dict) and 'anova' in result:
                if result['anova']['significant'] or result['kruskal_wallis']['significant']:
                    significant_metrics.append(metric)
        
        if len(significant_metrics) >= 3:
            return "ëŒ€ë¶€ë¶„ ë©”íŠ¸ë¦­ì—ì„œ ëª¨ë¸ ê°„ ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ ì°¨ì´ê°€ í™•ì¸ë¨"
        elif len(significant_metrics) >= 1:
            return f"{', '.join(significant_metrics)} ë©”íŠ¸ë¦­ì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ ì°¨ì´ í™•ì¸"
        else:
            return "ëª¨ë“  ë©”íŠ¸ë¦­ì—ì„œ ëª¨ë¸ ê°„ ì„±ëŠ¥ì´ ìœ ì‚¬í•¨"
    
    # ê¸°ì¡´ pairwise comparison ê´€ë ¨ ë©”ì„œë“œë“¤ - n-model ë¹„êµë¡œ ë³€ê²½í•˜ë©´ì„œ ì œê±°ë¨
    # def _calculate_cohens_d(): Cohen's dëŠ” 2ê°œ ê·¸ë£¹ ë¹„êµìš©ì´ë¯€ë¡œ n-model ë¶„ì„ì—ì„œëŠ” ë¶ˆí•„ìš”
    # def _interpret_effect_size(): ë§ˆì°¬ê°€ì§€ë¡œ pairwise comparisonìš©
    # def _assess_practical_significance(): ì‹¤ìš©ì  ìœ ì˜ì„±ë„ pairwise comparison ë§¥ë½
    
    def _calculate_model_rankings(self, model_stats: Dict[str, Any]) -> Dict[str, Any]:
        """ëª¨ë¸ ìˆœìœ„ ê³„ì‚°"""
        models = list(model_stats.keys())
        
        # ë©”íŠ¸ë¦­ë³„ ìˆœìœ„
        metric_rankings = {}
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness', 'overall']
        
        for metric in metrics:
            if metric == 'overall':
                scores = [(model, stats['overall']['weighted_score']) for model, stats in model_stats.items()]
            else:
                scores = [(model, stats[metric]['mean_score']) for model, stats in model_stats.items()]
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë‚´ë¦¼ì°¨ìˆœ)
            ranked = sorted(scores, key=lambda x: x[1], reverse=True)
            metric_rankings[metric] = [
                {
                    'rank': i + 1,
                    'model': model,
                    'score': score,
                    'grade': model_stats[model].get('overall', {}).get('grade', 'N/A') if metric == 'overall' else 'N/A'
                }
                for i, (model, score) in enumerate(ranked)
            ]
        
        # ì¢…í•© ìˆœìœ„ (ê°€ì¤‘ í‰ê· )
        overall_ranking = metric_rankings['overall']
        
        return {
            'by_metric': metric_rankings,
            'overall': overall_ranking,
            'champion': overall_ranking[0] if overall_ranking else None,
            'summary': self._create_ranking_summary(metric_rankings)
        }
    
    def _create_ranking_summary(self, metric_rankings: Dict[str, List]) -> Dict[str, Any]:
        """ìˆœìœ„ ìš”ì•½ ìƒì„±"""
        summary = {}
        
        # ê° ë©”íŠ¸ë¦­ë³„ 1ìœ„ ëª¨ë¸
        champions = {}
        for metric, ranking in metric_rankings.items():
            if ranking:
                champions[metric] = ranking[0]['model']
        
        # ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸ (ì—¬ëŸ¬ ë©”íŠ¸ë¦­ì—ì„œ ìƒìœ„ê¶Œ)
        model_positions = {}
        for metric, ranking in metric_rankings.items():
            for entry in ranking:
                model = entry['model']
                rank = entry['rank']
                if model not in model_positions:
                    model_positions[model] = []
                model_positions[model].append(rank)
        
        # í‰ê·  ìˆœìœ„ ê³„ì‚°
        avg_rankings = {
            model: np.mean(positions) 
            for model, positions in model_positions.items()
        }
        most_consistent = min(avg_rankings.items(), key=lambda x: x[1])[0]
        
        return {
            'metric_champions': champions,
            'most_consistent_model': most_consistent,
            'average_rankings': avg_rankings,
            'performance_tiers': self._create_performance_tiers(metric_rankings['overall'])
        }
    
    def _create_performance_tiers(self, overall_ranking: List[Dict]) -> Dict[str, List[str]]:
        """ì„±ëŠ¥ ë“±ê¸‰ë³„ ëª¨ë¸ ë¶„ë¥˜"""
        tiers = {
            'Tier 1 (ìš°ìˆ˜)': [],
            'Tier 2 (ì–‘í˜¸)': [],
            'Tier 3 (ë³´í†µ)': [],
            'Tier 4 (ê°œì„ í•„ìš”)': []
        }
        
        for entry in overall_ranking:
            score = entry['score']
            model = entry['model']
            
            if score >= 0.85:
                tiers['Tier 1 (ìš°ìˆ˜)'].append(model)
            elif score >= 0.75:
                tiers['Tier 2 (ì–‘í˜¸)'].append(model)
            elif score >= 0.65:
                tiers['Tier 3 (ë³´í†µ)'].append(model)
            else:
                tiers['Tier 4 (ê°œì„ í•„ìš”)'].append(model)
        
        return tiers
    
    def _generate_model_recommendations(self, 
                                      model_stats: Dict[str, Any],
                                      rankings: Dict[str, Any]) -> List[str]:
        """ëª¨ë¸ ì„ íƒ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì „ì²´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_overall = rankings['overall'][0] if rankings['overall'] else None
        if best_overall:
            recommendations.append(
                f"ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: {best_overall['model']} "
                f"(ì¢…í•© ì ìˆ˜: {best_overall['score']:.3f}, ë“±ê¸‰: {best_overall['grade']})"
            )
        
        # ë©”íŠ¸ë¦­ë³„ íŠ¹í™” ëª¨ë¸
        metric_champions = rankings['summary']['metric_champions']
        for metric, champion in metric_champions.items():
            if metric != 'overall':
                score = model_stats[champion][metric]['mean_score']
                recommendations.append(
                    f"ğŸ“Š {metric.title()} ìµœê³ : {champion} (ì ìˆ˜: {score:.3f})"
                )
        
        # ì¼ê´€ì„± ìµœê³  ëª¨ë¸
        most_consistent = rankings['summary']['most_consistent_model']
        avg_rank = rankings['summary']['average_rankings'][most_consistent]
        recommendations.append(
            f"ğŸ¯ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸: {most_consistent} (í‰ê·  ìˆœìœ„: {avg_rank:.1f})"
        )
        
        # ì„±ëŠ¥ ê°œì„  í•„ìš” ëª¨ë¸
        poor_performers = rankings['summary']['performance_tiers']['Tier 4 (ê°œì„ í•„ìš”)']
        if poor_performers:
            recommendations.append(
                f"âš ï¸ ì„±ëŠ¥ ê°œì„  í•„ìš”: {', '.join(poor_performers)}"
            )
        
        return recommendations

### 4.2 ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„

```python
class TechStackAnalyzer:
    """ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„"""
    
    def __init__(self):
        self.tech_stack_mapping = {
            'cline': 'TypeScript/JavaScript',
            'ecommerce-microservices': 'Java/Spring',
            'kotlin-realworld': 'Kotlin/JPA',
            'selvage-deprecated': 'Python'
        }
    
    def analyze_tech_stack_performance(self, 
                                     repo_results: Dict[str, Dict[str, List[TestCaseResult]]]) -> Dict[str, Any]:
        """ì €ì¥ì†Œ/ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„"""
        
        analysis = {
            'by_tech_stack': {},
            'cross_stack_comparison': {},
            'recommendations': {}
        }
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„
        for repo_name, model_results in repo_results.items():
            tech_stack = self.tech_stack_mapping.get(repo_name, 'Unknown')
            
            # í•´ë‹¹ ê¸°ìˆ ìŠ¤íƒì—ì„œì˜ ëª¨ë¸ë³„ ì„±ëŠ¥
            stack_performance = {}
            for model_name, results in model_results.items():
                aggregator = MetricAggregator()
                stack_performance[model_name] = aggregator.aggregate_model_performance(results)
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„
            best_model = max(
                stack_performance.items(),
                key=lambda x: x[1]['overall']['weighted_score']
            )
            
            analysis['by_tech_stack'][tech_stack] = {
                'repository': repo_name,
                'model_performance': stack_performance,
                'best_model': {
                    'name': best_model[0],
                    'score': best_model[1]['overall']['weighted_score'],
                    'grade': best_model[1]['overall']['grade']
                },
                'performance_gap': self._calculate_performance_gap(stack_performance),
                'recommendations': self._generate_tech_stack_recommendations(
                    tech_stack, stack_performance
                )
            }
        
        # ê¸°ìˆ ìŠ¤íƒ ê°„ êµì°¨ ë¹„êµ
        analysis['cross_stack_comparison'] = self._cross_stack_comparison(analysis['by_tech_stack'])
        
        return analysis
    
    def _calculate_performance_gap(self, stack_performance: Dict[str, Any]) -> Dict[str, float]:
        """ì„±ëŠ¥ ê²©ì°¨ ê³„ì‚°"""
        overall_scores = [
            perf['overall']['weighted_score'] 
            for perf in stack_performance.values()
        ]
        
        return {
            'max_score': max(overall_scores),
            'min_score': min(overall_scores),
            'gap': max(overall_scores) - min(overall_scores),
            'coefficient_variation': np.std(overall_scores) / np.mean(overall_scores)
        }
    
    def _generate_tech_stack_recommendations(self, 
                                           tech_stack: str,
                                           performance: Dict[str, Any]) -> List[str]:
        """ê¸°ìˆ ìŠ¤íƒë³„ ê¶Œì¥ì‚¬í•­"""
        recommendations = []
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì²œ
        best_model = max(
            performance.items(),
            key=lambda x: x[1]['overall']['weighted_score']
        )
        recommendations.append(
            f"{tech_stack}ì—ëŠ” {best_model[0]} ëª¨ë¸ ì‚¬ìš© ê¶Œì¥ "
            f"(ì ìˆ˜: {best_model[1]['overall']['weighted_score']:.3f})"
        )
        
        # íŠ¹ì • ë©”íŠ¸ë¦­ ê°•í™” í•„ìš”ì„±
        for model_name, perf in performance.items():
            weak_metrics = [
                metric for metric, data in perf.items() 
                if metric != 'overall' and data.get('mean_score', 0) < 0.7
            ]
            if weak_metrics:
                recommendations.append(
                    f"{model_name}ì˜ {', '.join(weak_metrics)} ì„±ëŠ¥ ê°œì„  í•„ìš”"
                )
        
        return recommendations
```

## 5. Selvage ë²„ì „ë³„ ì„±ëŠ¥ ë³€í™” ì¶”ì 

### 5.1 ë²„ì „ë³„ ë¹„êµ ë¶„ì„ ë°©ë²•ë¡ 

```python
from datetime import datetime, timedelta
import json
from pathlib import Path

class VersionComparisonAnalyzer:
    """Selvage ë²„ì „ë³„ ì„±ëŠ¥ ë³€í™” ë¶„ì„"""
    
    def __init__(self):
        self.version_pattern = re.compile(r'selvage\s+([\d.]+)')
        
    def collect_version_data(self, base_path: str) -> Dict[str, List[Dict]]:
        """ë²„ì „ë³„ í‰ê°€ ë°ì´í„° ìˆ˜ì§‘"""
        base_path = Path(base_path).expanduser()
        version_data = {}
        
        # ëª¨ë“  í‰ê°€ ì„¸ì…˜ ìŠ¤ìº”
        for session_dir in base_path.glob('eval_*'):
            if not session_dir.is_dir():
                continue
                
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
            metadata_file = session_dir / 'metadata.json'
            if not metadata_file.exists():
                continue
                
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            version = metadata.get('selvage_version', 'unknown')
            execution_date = metadata.get('execution_date')
            
            # í•´ë‹¹ ì„¸ì…˜ì˜ ëª¨ë“  ëª¨ë¸ ê²°ê³¼ ìˆ˜ì§‘
            session_results = self._collect_session_results(session_dir)
            
            if version not in version_data:
                version_data[version] = []
            
            version_data[version].append({
                'session_id': session_dir.name,
                'execution_date': execution_date,
                'metadata': metadata,
                'results': session_results
            })
        
        return version_data
    
    def analyze_version_progression(self, 
                                  version_data: Dict[str, List[Dict]]) -> Dict[str, Any]:
        """ë²„ì „ë³„ ì„±ëŠ¥ ë°œì „ ë¶„ì„"""
        
        analysis = {
            'version_timeline': [],
            'performance_trends': {},
            'regression_analysis': {},
            'improvement_highlights': [],
            'version_recommendations': {}
        }
        
        # ë²„ì „ë³„ ì¢…í•© ì„±ëŠ¥ ê³„ì‚°
        version_performance = {}
        for version, sessions in version_data.items():
            aggregated_results = self._aggregate_version_results(sessions)
            
            aggregator = MetricAggregator()
            version_performance[version] = {
                'performance': aggregator.aggregate_model_performance(aggregated_results),
                'session_count': len(sessions),
                'latest_date': max(s['execution_date'] for s in sessions if s['execution_date'])
            }
        
        # ì‹œê°„ìˆœ ì •ë ¬
        sorted_versions = self._sort_versions_chronologically(version_performance)
        analysis['version_timeline'] = sorted_versions
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        analysis['performance_trends'] = self._analyze_performance_trends(sorted_versions)
        
        # íšŒê·€ ë¶„ì„ (ì„±ëŠ¥ ì €í•˜ íƒì§€)
        analysis['regression_analysis'] = self._detect_regressions(sorted_versions)
        
        # ê°œì„  í•˜ì´ë¼ì´íŠ¸
        analysis['improvement_highlights'] = self._identify_improvements(sorted_versions)
        
        # ë²„ì „ë³„ ê¶Œì¥ì‚¬í•­
        analysis['version_recommendations'] = self._generate_version_recommendations(
            sorted_versions, analysis
        )
        
        return analysis
    
    def _collect_session_results(self, session_dir: Path) -> List[TestCaseResult]:
        """ì„¸ì…˜ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘"""
        all_results = []
        
        for model_dir in session_dir.iterdir():
            if not model_dir.is_dir() or model_dir.name == 'metadata.json':
                continue
                
            parser = DeepEvalLogParser()
            for log_file in model_dir.glob('*.log'):
                for test_case_data in parser.parse_log_file(log_file):
                    # TestCaseResult ê°ì²´ë¡œ ë³€í™˜
                    result = self._convert_to_test_case_result(test_case_data)
                    if result:
                        all_results.append(result)
        
        return all_results
    
    def _aggregate_version_results(self, sessions: List[Dict]) -> List[TestCaseResult]:
        """ë²„ì „ ë‚´ ëª¨ë“  ì„¸ì…˜ ê²°ê³¼ í†µí•©"""
        all_results = []
        for session in sessions:
            all_results.extend(session['results'])
        return all_results
    
    def _sort_versions_chronologically(self, 
                                     version_performance: Dict[str, Any]) -> List[Dict]:
        """ë²„ì „ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬"""
        version_list = []
        
        for version, data in version_performance.items():
            # ë²„ì „ ë²ˆí˜¸ íŒŒì‹± (ì˜ˆ: "selvage 0.1.2" -> [0, 1, 2])
            version_match = self.version_pattern.search(version)
            if version_match:
                version_number = version_match.group(1)
                version_parts = [int(x) for x in version_number.split('.')]
            else:
                version_parts = [0, 0, 0]  # ì•Œ ìˆ˜ ì—†ëŠ” ë²„ì „
            
            version_list.append({
                'version': version,
                'version_parts': version_parts,
                'latest_date': data['latest_date'],
                'performance': data['performance'],
                'session_count': data['session_count']
            })
        
        # ë²„ì „ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
        return sorted(version_list, key=lambda x: x['version_parts'])
    
    def _analyze_performance_trends(self, sorted_versions: List[Dict]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
        trends = {}
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for metric in metrics:
            scores = []
            versions = []
            
            for version_data in sorted_versions:
                score = version_data['performance'][metric]['mean_score']
                scores.append(score)
                versions.append(version_data['version'])
            
            # ì„ í˜• íšŒê·€ë¥¼ í†µí•œ íŠ¸ë Œë“œ ë¶„ì„
            if len(scores) >= 2:
                x = np.arange(len(scores))
                z = np.polyfit(x, scores, 1)
                trend_slope = z[0]
                
                trends[metric] = {
                    'scores': scores,
                    'versions': versions,
                    'trend_slope': trend_slope,
                    'trend_direction': 'improving' if trend_slope > 0.01 else 'declining' if trend_slope < -0.01 else 'stable',
                    'total_change': scores[-1] - scores[0] if scores else 0,
                    'best_version': versions[scores.index(max(scores))] if scores else None,
                    'worst_version': versions[scores.index(min(scores))] if scores else None
                }
        
        # ì „ì²´ íŠ¸ë Œë“œ ìš”ì•½
        overall_trend = np.mean([trends[m]['trend_slope'] for m in metrics])
        trends['overall'] = {
            'trend_slope': overall_trend,
            'trend_direction': 'improving' if overall_trend > 0.01 else 'declining' if overall_trend < -0.01 else 'stable'
        }
        
        return trends
    
    def _detect_regressions(self, sorted_versions: List[Dict]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íšŒê·€(ì €í•˜) íƒì§€"""
        regressions = {
            'detected_regressions': [],
            'regression_summary': {},
            'severity_assessment': {}
        }
        
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for i in range(1, len(sorted_versions)):
            current_version = sorted_versions[i]
            previous_version = sorted_versions[i-1]
            
            version_regressions = []
            
            for metric in metrics:
                current_score = current_version['performance'][metric]['mean_score']
                previous_score = previous_version['performance'][metric]['mean_score']
                
                # ì˜ë¯¸ìˆëŠ” ì„±ëŠ¥ ì €í•˜ ì„ê³„ê°’: 5% ì´ìƒ
                regression_threshold = 0.05
                
                if previous_score - current_score > regression_threshold:
                    severity = self._assess_regression_severity(
                        previous_score, current_score, metric
                    )
                    
                    version_regressions.append({
                        'metric': metric,
                        'previous_score': previous_score,
                        'current_score': current_score,
                        'regression_amount': previous_score - current_score,
                        'regression_percentage': ((previous_score - current_score) / previous_score) * 100,
                        'severity': severity
                    })
            
            if version_regressions:
                regressions['detected_regressions'].append({
                    'from_version': previous_version['version'],
                    'to_version': current_version['version'],
                    'regressions': version_regressions,
                    'overall_severity': max(r['severity'] for r in version_regressions)
                })
        
        return regressions
    
    def _assess_regression_severity(self, previous_score: float, 
                                  current_score: float, metric: str) -> str:
        """íšŒê·€ ì‹¬ê°ë„ í‰ê°€"""
        regression_amount = previous_score - current_score
        
        if regression_amount > 0.15:
            return 'critical'
        elif regression_amount > 0.10:
            return 'major'
        elif regression_amount > 0.05:
            return 'minor'
        else:
            return 'negligible'
    
    def _identify_improvements(self, sorted_versions: List[Dict]) -> List[Dict]:
        """ì£¼ëª©í•  ë§Œí•œ ê°œì„ ì‚¬í•­ ì‹ë³„"""
        improvements = []
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for i in range(1, len(sorted_versions)):
            current_version = sorted_versions[i]
            previous_version = sorted_versions[i-1]
            
            version_improvements = []
            
            for metric in metrics:
                current_score = current_version['performance'][metric]['mean_score']
                previous_score = previous_version['performance'][metric]['mean_score']
                
                # ì˜ë¯¸ìˆëŠ” ê°œì„  ì„ê³„ê°’: 3% ì´ìƒ
                improvement_threshold = 0.03
                
                if current_score - previous_score > improvement_threshold:
                    improvement_magnitude = self._assess_improvement_magnitude(
                        previous_score, current_score
                    )
                    
                    version_improvements.append({
                        'metric': metric,
                        'previous_score': previous_score,
                        'current_score': current_score,
                        'improvement_amount': current_score - previous_score,
                        'improvement_percentage': ((current_score - previous_score) / previous_score) * 100,
                        'magnitude': improvement_magnitude
                    })
            
            if version_improvements:
                improvements.append({
                    'from_version': previous_version['version'],
                    'to_version': current_version['version'],
                    'improvements': version_improvements,
                    'overall_magnitude': max(i['magnitude'] for i in version_improvements)
                })
        
        return improvements
    
    def _assess_improvement_magnitude(self, previous_score: float, current_score: float) -> str:
        """ê°œì„  ì •ë„ í‰ê°€"""
        improvement_amount = current_score - previous_score
        
        if improvement_amount > 0.15:
            return 'breakthrough'
        elif improvement_amount > 0.10:
            return 'significant'
        elif improvement_amount > 0.05:
            return 'moderate'
        else:
            return 'minor'
    
    def _generate_version_recommendations(self, 
                                        sorted_versions: List[Dict],
                                        analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë²„ì „ë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = {
            'recommended_version': None,
            'version_assessments': {},
            'upgrade_path': [],
            'risk_assessment': {}
        }
        
        # ìµœê³  ì„±ëŠ¥ ë²„ì „ ì‹ë³„
        best_version = max(
            sorted_versions,
            key=lambda x: x['performance']['overall']['weighted_score']
        )
        recommendations['recommended_version'] = best_version['version']
        
        # ê° ë²„ì „ë³„ í‰ê°€
        for version_data in sorted_versions:
            version = version_data['version']
            performance = version_data['performance']
            
            # ì¥ë‹¨ì  ë¶„ì„
            strengths = []
            weaknesses = []
            
            for metric in ['correctness', 'clarity', 'actionability', 'json_correctness']:
                score = performance[metric]['mean_score']
                if score >= 0.8:
                    strengths.append(f"{metric} ìš°ìˆ˜ ({score:.3f})")
                elif score < 0.7:
                    weaknesses.append(f"{metric} ê°œì„  í•„ìš” ({score:.3f})")
            
            recommendations['version_assessments'][version] = {
                'overall_score': performance['overall']['weighted_score'],
                'grade': performance['overall']['grade'],
                'strengths': strengths,
                'weaknesses': weaknesses,
                'stability_rating': self._assess_version_stability(version, analysis)
            }
        
        return recommendations
    
    def _assess_version_stability(self, version: str, analysis: Dict[str, Any]) -> str:
        """ë²„ì „ ì•ˆì •ì„± í‰ê°€"""
        # íšŒê·€ ë¶„ì„ì—ì„œ í•´ë‹¹ ë²„ì „ì´ ê´€ë ¨ëœ ê²½ìš° í™•ì¸
        regressions = analysis.get('regression_analysis', {}).get('detected_regressions', [])
        
        for regression in regressions:
            if regression['to_version'] == version:
                if regression['overall_severity'] in ['critical', 'major']:
                    return 'unstable'
                elif regression['overall_severity'] == 'minor':
                    return 'moderately_stable'
        
        return 'stable'
```

## 6. ì‹œê°í™” ë° ë³´ê³ ì„œ ìƒì„± ì „ëµ

### 6.1 ëŒ€ì‹œë³´ë“œ ì„¤ê³„

```python
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

class VisualizationGenerator:
    """ë¶„ì„ ê²°ê³¼ ì‹œê°í™” ìƒì„±ê¸°"""
    
    def __init__(self):
        self.color_palette = {
            'primary': '#1f77b4',
            'success': '#2ca02c', 
            'warning': '#ff7f0e',
            'danger': '#d62728',
            'info': '#17becf'
        }
        
    def generate_comprehensive_dashboard(self, 
                                       analysis_results: Dict[str, Any],
                                       output_dir: str) -> List[str]:
        """ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        generated_files = []
        
        # 1. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë ˆì´ë” ì°¨íŠ¸
        radar_file = self._create_model_performance_radar(
            analysis_results.get('model_comparison', {}), output_path
        )
        generated_files.append(radar_file)
        
        # 2. ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ íˆíŠ¸ë§µ
        heatmap_file = self._create_performance_heatmap(
            analysis_results.get('model_comparison', {}), output_path
        )
        generated_files.append(heatmap_file)
        
        # 3. ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì°¨íŠ¸
        failure_file = self._create_failure_pattern_charts(
            analysis_results.get('failure_analysis', {}), output_path
        )
        generated_files.append(failure_file)
        
        # 4. ë²„ì „ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ
        trend_file = self._create_version_trend_chart(
            analysis_results.get('version_analysis', {}), output_path
        )
        generated_files.append(trend_file)
        
        # 5. ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¹„êµ
        tech_stack_file = self._create_tech_stack_comparison(
            analysis_results.get('tech_stack_analysis', {}), output_path
        )
        generated_files.append(tech_stack_file)
        
        return generated_files
    
    def _create_model_performance_radar(self, 
                                      model_comparison: Dict[str, Any],
                                      output_path: Path) -> str:
        """ëª¨ë¸ ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸ ìƒì„±"""
        fig = go.Figure()
        
        model_stats = model_comparison.get('model_statistics', {})
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for model_name, stats in model_stats.items():
            scores = [stats[metric]['mean_score'] for metric in metrics]
            
            fig.add_trace(go.Scatterpolar(
                r=scores,
                theta=metrics,
                fill='toself',
                name=model_name,
                line=dict(width=2)
            ))
        
        fig.update_layout(
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1]
                )
            ),
            showlegend=True,
            title="ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ (ë ˆì´ë” ì°¨íŠ¸)",
            width=800,
            height=600
        )
        
        file_path = output_path / "model_performance_radar.html"
        fig.write_html(str(file_path))
        return str(file_path)
    
    def _create_performance_heatmap(self, 
                                  model_comparison: Dict[str, Any],
                                  output_path: Path) -> str:
        """ì„±ëŠ¥ íˆíŠ¸ë§µ ìƒì„±"""
        model_stats = model_comparison.get('model_statistics', {})
        
        # ë°ì´í„° ì¤€ë¹„
        models = list(model_stats.keys())
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        heatmap_data = []
        for model in models:
            row = [model_stats[model][metric]['mean_score'] for metric in metrics]
            heatmap_data.append(row)
        
        fig = go.Figure(data=go.Heatmap(
            z=heatmap_data,
            x=metrics,
            y=models,
            colorscale='RdYlGn',
            zmin=0,
            zmax=1,
            text=[[f"{val:.3f}" for val in row] for row in heatmap_data],
            texttemplate="%{text}",
            textfont={"size": 12},
            colorbar=dict(title="ì„±ëŠ¥ ì ìˆ˜")
        ))
        
        fig.update_layout(
            title="ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ì„±ëŠ¥ íˆíŠ¸ë§µ",
            xaxis_title="ë©”íŠ¸ë¦­",
            yaxis_title="ëª¨ë¸",
            width=800,
            height=500
        )
        
        file_path = output_path / "performance_heatmap.html"
        fig.write_html(str(file_path))
        return str(file_path)
    
    def _create_failure_pattern_charts(self, 
                                     failure_analysis: Dict[str, Any],
                                     output_path: Path) -> str:
        """ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì°¨íŠ¸ ìƒì„±"""
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=('ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„í¬', 'ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬', 
                          'ì‹¬ê°ë„ë³„ ì‹¤íŒ¨ ê±´ìˆ˜', 'ê°œì„  ìš°ì„ ìˆœìœ„'),
            specs=[[{"type": "bar"}, {"type": "pie"}],
                   [{"type": "bar"}, {"type": "bar"}]]
        )
        
        # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„í¬
        if 'by_metric' in failure_analysis:
            metrics = list(failure_analysis['by_metric'].keys())
            failure_counts = [failure_analysis['by_metric'][m]['total_failures'] for m in metrics]
            
            fig.add_trace(
                go.Bar(x=metrics, y=failure_counts, name="ì‹¤íŒ¨ ê±´ìˆ˜"),
                row=1, col=1
            )
        
        # ì‹¤íŒ¨ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬ (íŒŒì´ ì°¨íŠ¸)
        if 'by_category' in failure_analysis:
            categories = list(failure_analysis['by_category'].keys())
            category_counts = list(failure_analysis['by_category'].values())
            
            fig.add_trace(
                go.Pie(labels=categories, values=category_counts, name="ì¹´í…Œê³ ë¦¬"),
                row=1, col=2
            )
        
        fig.update_layout(
            title_text="ì‹¤íŒ¨ íŒ¨í„´ ì¢…í•© ë¶„ì„",
            showlegend=True,
            height=800,
            width=1200
        )
        
        file_path = output_path / "failure_pattern_analysis.html"
        fig.write_html(str(file_path))
        return str(file_path)
    
    def _create_version_trend_chart(self, 
                                  version_analysis: Dict[str, Any],
                                  output_path: Path) -> str:
        """ë²„ì „ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ ì°¨íŠ¸ ìƒì„±"""
        trends = version_analysis.get('performance_trends', {})
        
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=['Correctness íŠ¸ë Œë“œ', 'Clarity íŠ¸ë Œë“œ', 
                          'Actionability íŠ¸ë Œë“œ', 'JSON Correctness íŠ¸ë Œë“œ']
        )
        
        positions = [(1, 1), (1, 2), (2, 1), (2, 2)]
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for i, metric in enumerate(metrics):
            if metric in trends:
                trend_data = trends[metric]
                versions = trend_data['versions']
                scores = trend_data['scores']
                
                row, col = positions[i]
                
                # ì‹¤ì œ ì ìˆ˜
                fig.add_trace(
                    go.Scatter(
                        x=versions, y=scores,
                        mode='lines+markers',
                        name=f"{metric} ì ìˆ˜",
                        line=dict(width=3)
                    ),
                    row=row, col=col
                )
                
                # íŠ¸ë Œë“œ ë¼ì¸
                x_numeric = list(range(len(versions)))
                z = np.polyfit(x_numeric, scores, 1)
                trend_line = np.poly1d(z)
                
                fig.add_trace(
                    go.Scatter(
                        x=versions, y=trend_line(x_numeric),
                        mode='lines',
                        name=f"{metric} íŠ¸ë Œë“œ",
                        line=dict(dash='dash', width=2)
                    ),
                    row=row, col=col
                )
        
        fig.update_layout(
            title_text="Selvage ë²„ì „ë³„ ì„±ëŠ¥ íŠ¸ë Œë“œ",
            height=800,
            width=1200,
            showlegend=True
        )
        
        file_path = output_path / "version_trend_analysis.html"
        fig.write_html(str(file_path))
        return str(file_path)
```

## 7. ì¢…í•© ë¶„ì„ ì—”ì§„

### 7.1 ë©”ì¸ ë¶„ì„ ì—”ì§„ í´ë˜ìŠ¤

```python
"""DeepEval ë¶„ì„ ì—”ì§„

DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µí•©ëœ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from dataclasses import dataclass
import logging

import numpy as np
import pandas as pd

logger = logging.getLogger(__name__)


@dataclass
class ModelPerformance:
    """ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„°"""
    model_name: str
    total_test_cases: int
    correctness_mean: float
    correctness_std: float
    clarity_mean: float
    clarity_std: float
    actionability_mean: float
    actionability_std: float
    json_correctness_mean: float
    json_correctness_std: float
    overall_score: float
    pass_rate: float
    grade: str


@dataclass
class FailurePattern:
    """ì‹¤íŒ¨ íŒ¨í„´ ë°ì´í„°"""
    metric_name: str
    total_failures: int
    missing_issues: int
    incorrect_line_numbers: int
    inappropriate_severity: int
    unclear_descriptions: int
    non_actionable_suggestions: int
    json_format_errors: int
    other: int


class DeepEvalAnalysisEngine:
    """DeepEval ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, output_dir: str = "~/Library/selvage-eval/analyze_results"):
        """ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            output_dir: ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def analyze_session(self, session_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì„¸ì…˜ ë¶„ì„ ì‹¤í–‰
        
        Args:
            session_path: DeepEval ê²°ê³¼ê°€ ìˆëŠ” ì„¸ì…˜ ê²½ë¡œ
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        session_path = Path(session_path).expanduser()
        
        if not session_path.exists():
            raise FileNotFoundError(f"ì„¸ì…˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            session_id = session_path.name
            final_output_dir = self.output_dir / session_id
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        
        # DeepEval ê²°ê³¼ ìˆ˜ì§‘
        deepeval_results = self._collect_deepeval_results(session_path)
        
        if not deepeval_results:
            raise ValueError("DeepEval ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë¶„ì„ ì‹¤í–‰
        analysis_data = self._perform_comprehensive_analysis(deepeval_results)
        
        # í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report = self._generate_markdown_report(analysis_data)
        report_path = final_output_dir / "analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        # JSON ë°ì´í„° ì €ì¥ (í”„ë¡œê·¸ë˜ë° í™œìš©ìš©)
        json_data = self._prepare_json_data(analysis_data)
        json_path = final_output_dir / "analysis_data.json"
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        # ì„ íƒì  ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ìƒì„±
        dashboard_path = None
        if self._should_generate_dashboard(analysis_data):
            dashboard_path = self._generate_interactive_dashboard(analysis_data, final_output_dir)
        
        return {
            "analysis_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "session_path": str(session_path),
                "total_test_cases": analysis_data["data_summary"]["total_test_cases"],
                "models_analyzed": analysis_data["models_analyzed"]
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "interactive_dashboard": dashboard_path
            }
        }
    
    def _collect_deepeval_results(self, session_path: Path) -> List[Dict[str, Any]]:
        """DeepEval ê²°ê³¼ ìˆ˜ì§‘"""
        results = []
        
        # deepeval_results_*.json íŒ¨í„´ì˜ íŒŒì¼ë“¤ ì°¾ê¸°
        for result_file in session_path.glob("deepeval_results_*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
                filename = result_file.stem
                parts = filename.split('_')
                if len(parts) >= 3:
                    model_name = '_'.join(parts[2:])  # deepeval_results_ì´í›„ ë¶€ë¶„
                else:
                    model_name = "unknown"
                
                results.append({
                    "model_name": model_name,
                    "file_path": str(result_file),
                    "data": result_data
                })
                
            except Exception as e:
                logger.warning(f"DeepEval ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {result_file} - {e}")
        
        return results
    
    def _perform_comprehensive_analysis(self, deepeval_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰"""
        models_analyzed = [r["model_name"] for r in deepeval_results]
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„
        model_performances = []
        for result in deepeval_results:
            performance = self._analyze_model_performance(result)
            model_performances.append(performance)
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_patterns = self._analyze_failure_patterns(deepeval_results)
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ (í˜„ì¬ëŠ” ë‹¨ìˆœí™”)
        tech_stack_analysis = self._analyze_tech_stack_performance(model_performances)
        
        # í†µê³„ ë¶„ì„
        statistical_analysis = self._perform_statistical_analysis(model_performances)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_recommendations(model_performances, failure_patterns)
        
        return {
            "models_analyzed": models_analyzed,
            "model_performances": model_performances,
            "failure_patterns": failure_patterns,
            "tech_stack_analysis": tech_stack_analysis,
            "statistical_analysis": statistical_analysis,
            "recommendations": recommendations,
            "data_summary": {
                "total_test_cases": sum(len(r["data"].get("testCases", [])) for r in deepeval_results),
                "successful_evaluations": sum(
                    len([tc for tc in r["data"].get("testCases", []) if tc.get("success", True)])
                    for r in deepeval_results
                ),
                "models_count": len(deepeval_results)
            }
        }
    
    def _analyze_model_performance(self, result: Dict[str, Any]) -> ModelPerformance:
        """ê°œë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„"""
        model_name = result["model_name"]
        test_cases = result["data"].get("testCases", [])
        
        if not test_cases:
            return ModelPerformance(
                model_name=model_name,
                total_test_cases=0,
                correctness_mean=0.0, correctness_std=0.0,
                clarity_mean=0.0, clarity_std=0.0,
                actionability_mean=0.0, actionability_std=0.0,
                json_correctness_mean=0.0, json_correctness_std=0.0,
                overall_score=0.0, pass_rate=0.0, grade="F"
            )
        
        # ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ìˆ˜ì§‘
        metric_scores = {
            "correctness": [],
            "clarity": [],
            "actionability": [],
            "json_correctness": []
        }
        
        for test_case in test_cases:
            metrics_data = test_case.get("metricsData", [])
            for metric in metrics_data:
                metric_name = metric.get("name", "").lower().replace(" ", "_")
                if metric_name in metric_scores:
                    score = metric.get("score", 0.0)
                    if isinstance(score, (int, float)):
                        metric_scores[metric_name].append(float(score))
        
        # í†µê³„ ê³„ì‚°
        def calc_stats(scores):
            if not scores:
                return 0.0, 0.0
            return float(np.mean(scores)), float(np.std(scores))
        
        correctness_mean, correctness_std = calc_stats(metric_scores["correctness"])
        clarity_mean, clarity_std = calc_stats(metric_scores["clarity"])
        actionability_mean, actionability_std = calc_stats(metric_scores["actionability"])
        json_correctness_mean, json_correctness_std = calc_stats(metric_scores["json_correctness"])
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚° (ê°€ì¤‘í‰ê· )
        weights = {"correctness": 0.4, "clarity": 0.25, "actionability": 0.25, "json_correctness": 0.1}
        overall_score = (
            correctness_mean * weights["correctness"] +
            clarity_mean * weights["clarity"] +
            actionability_mean * weights["actionability"] +
            json_correctness_mean * weights["json_correctness"]
        )
        
        # í•©ê²©ë¥  ê³„ì‚° (0.7 ì´ìƒ)
        all_scores = []
        for scores in metric_scores.values():
            all_scores.extend(scores)
        pass_rate = len([s for s in all_scores if s >= 0.7]) / len(all_scores) if all_scores else 0.0
        
        # ë“±ê¸‰ í• ë‹¹
        if overall_score >= 0.9:
            grade = "A"
        elif overall_score >= 0.8:
            grade = "B"
        elif overall_score >= 0.7:
            grade = "C"
        elif overall_score >= 0.6:
            grade = "D"
        else:
            grade = "F"
        
        return ModelPerformance(
            model_name=model_name,
            total_test_cases=len(test_cases),
            correctness_mean=correctness_mean,
            correctness_std=correctness_std,
            clarity_mean=clarity_mean,
            clarity_std=clarity_std,
            actionability_mean=actionability_mean,
            actionability_std=actionability_std,
            json_correctness_mean=json_correctness_mean,
            json_correctness_std=json_correctness_std,
            overall_score=overall_score,
            pass_rate=pass_rate,
            grade=grade
        )
    
    def _analyze_failure_patterns(self, deepeval_results: List[Dict[str, Any]]) -> List[FailurePattern]:
        """ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„"""
        patterns = []
        metrics = ["correctness", "clarity", "actionability", "json_correctness"]
        
        for metric in metrics:
            failure_reasons = []
            
            for result in deepeval_results:
                test_cases = result["data"].get("testCases", [])
                for test_case in test_cases:
                    metrics_data = test_case.get("metricsData", [])
                    for metric_data in metrics_data:
                        if (metric_data.get("name", "").lower().replace(" ", "_") == metric and
                            not metric_data.get("success", True)):
                            reason = metric_data.get("reason", "").lower()
                            failure_reasons.append(reason)
            
            # ì‹¤íŒ¨ ìœ í˜•ë³„ ë¶„ë¥˜
            missing_issues = len([r for r in failure_reasons if "missing" in r or "not identified" in r])
            incorrect_line_numbers = len([r for r in failure_reasons if "line number" in r or "incorrect" in r])
            inappropriate_severity = len([r for r in failure_reasons if "severity" in r or "inappropriate" in r])
            unclear_descriptions = len([r for r in failure_reasons if "unclear" in r or "vague" in r])
            non_actionable_suggestions = len([r for r in failure_reasons if "actionable" in r or "implementable" in r])
            json_format_errors = len([r for r in failure_reasons if "json" in r or "format" in r])
            
            categorized_count = (missing_issues + incorrect_line_numbers + inappropriate_severity +
                               unclear_descriptions + non_actionable_suggestions + json_format_errors)
            other = len(failure_reasons) - categorized_count
            
            patterns.append(FailurePattern(
                metric_name=metric,
                total_failures=len(failure_reasons),
                missing_issues=missing_issues,
                incorrect_line_numbers=incorrect_line_numbers,
                inappropriate_severity=inappropriate_severity,
                unclear_descriptions=unclear_descriptions,
                non_actionable_suggestions=non_actionable_suggestions,
                json_format_errors=json_format_errors,
                other=max(0, other)
            ))
        
        return patterns
    
    def _analyze_tech_stack_performance(self, model_performances: List[ModelPerformance]) -> Dict[str, Any]:
        """ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„ (í˜„ì¬ëŠ” ë‹¨ìˆœí™”)"""
        if not model_performances:
            return {}
        
        # ì „ì²´ í‰ê·  ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„
        overall_scores = [mp.overall_score for mp in model_performances]
        avg_score = np.mean(overall_scores) if overall_scores else 0.0
        
        return {
            "overall_average": float(avg_score),
            "best_performing_model": max(model_performances, key=lambda x: x.overall_score).model_name,
            "most_consistent_model": min(model_performances, key=lambda x: x.correctness_std).model_name
        }
    
    def _perform_statistical_analysis(self, model_performances: List[ModelPerformance]) -> Dict[str, Any]:
        """í†µê³„ ë¶„ì„"""
        if not model_performances:
            return {}
        
        scores = [mp.overall_score for mp in model_performances]
        
        return {
            "mean_performance": float(np.mean(scores)),
            "std_performance": float(np.std(scores)),
            "min_performance": float(np.min(scores)),
            "max_performance": float(np.max(scores)),
            "performance_range": float(np.max(scores) - np.min(scores)),
            "models_above_threshold": len([s for s in scores if s >= 0.7])
        }
    
    def _generate_recommendations(self, model_performances: List[ModelPerformance], 
                                failure_patterns: List[FailurePattern]) -> List[str]:
        """ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if not model_performances:
            return recommendations
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = max(model_performances, key=lambda x: x.overall_score)
        recommendations.append(
            f"ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: {best_model.model_name} (ì¢…í•© ì ìˆ˜: {best_model.overall_score:.3f}, ë“±ê¸‰: {best_model.grade})"
        )
        
        # ë©”íŠ¸ë¦­ë³„ ìµœê³  ì„±ëŠ¥
        best_correctness = max(model_performances, key=lambda x: x.correctness_mean)
        recommendations.append(
            f"ğŸ“Š Correctness ìµœê³ : {best_correctness.model_name} (ì ìˆ˜: {best_correctness.correctness_mean:.3f})"
        )
        
        # ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­
        for pattern in failure_patterns:
            if pattern.total_failures > 5:
                recommendations.append(
                    f"âš ï¸ {pattern.metric_name} ê°œì„  í•„ìš”: {pattern.total_failures}ê°œ ì‹¤íŒ¨ ì‚¬ë¡€ ë°œê²¬"
                )
        
        # ì¼ê´€ì„± ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        most_consistent = min(model_performances, key=lambda x: x.correctness_std)
        recommendations.append(
            f"ğŸ¯ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸: {most_consistent.model_name} (í‘œì¤€í¸ì°¨: {most_consistent.correctness_std:.3f})"
        )
        
        return recommendations
    
    def _generate_markdown_report(self, analysis_data: Dict[str, Any]) -> str:
        """í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±"""
        # ì‹¤ì œ êµ¬í˜„ì€ ë§¤ìš° ê¸¸ë¯€ë¡œ ì£¼ìš” êµ¬ì¡°ë§Œ í‘œì‹œ
        report_lines = [
            "# DeepEval ë¶„ì„ ë³´ê³ ì„œ",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ë¶„ì„ ëŒ€ìƒ ëª¨ë¸**: {', '.join(analysis_data['models_analyzed'])}",
            f"**ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {analysis_data['data_summary']['total_test_cases']}ê°œ",
            "",
            "## ğŸ“‹ í•µì‹¬ ê²°ê³¼ ìš”ì•½",
            ""
        ]
        
        # ê¶Œì¥ì‚¬í•­ ì¶”ê°€
        for recommendation in analysis_data['recommendations']:
            report_lines.append(f"- {recommendation}")
        
        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
        report_lines.extend([
            "",
            "## ğŸ¯ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ",
            "",
            "| ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë“±ê¸‰ | Correctness | Clarity | Actionability | JSON Correctness | í•©ê²©ë¥  |",
            "|--------|----------|------|-------------|---------|---------------|------------------|--------|"
        ])
        
        for perf in analysis_data['model_performances']:
            report_lines.append(
                f"| {perf.model_name} | {perf.overall_score:.3f} | {perf.grade} | "
                f"{perf.correctness_mean:.3f} | {perf.clarity_mean:.3f} | "
                f"{perf.actionability_mean:.3f} | {perf.json_correctness_mean:.3f} | "
                f"{perf.pass_rate:.1%} |"
            )
        
        # ì¶”ê°€ ì„¹ì…˜ë“¤...
        report_lines.extend([
            "",
            "## ğŸ“Š ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„",
            "## ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„", 
            "## ğŸ“ˆ í†µê³„ ë¶„ì„ ê²°ê³¼",
            "## ğŸ’¡ ê¶Œì¥ì‚¬í•­ ë° ê²°ë¡ "
        ])
        
        return "\n".join(report_lines)
    
    def _prepare_json_data(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
        """JSON ë°ì´í„° ì¤€ë¹„"""
        # ModelPerformanceì™€ FailurePattern ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        return {
            "analysis_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "models_analyzed": analysis_data["models_analyzed"]
            },
            "model_performances": [
                {
                    "model_name": perf.model_name,
                    "total_test_cases": perf.total_test_cases,
                    "overall_score": perf.overall_score,
                    "grade": perf.grade,
                    # ê¸°íƒ€ ë©”íŠ¸ë¦­ë“¤...
                }
                for perf in analysis_data["model_performances"]
            ],
            "failure_patterns": [
                {
                    "metric_name": pattern.metric_name,
                    "total_failures": pattern.total_failures,
                    # ê¸°íƒ€ ì‹¤íŒ¨ ìœ í˜•ë“¤...
                }
                for pattern in analysis_data["failure_patterns"]
            ],
            "statistical_analysis": analysis_data["statistical_analysis"],
            "recommendations": analysis_data["recommendations"],
            "data_summary": analysis_data["data_summary"]
        }
    
    def _should_generate_dashboard(self, analysis_data: Dict[str, Any]) -> bool:
        """ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ìƒì„± ì—¬ë¶€ ê²°ì •"""
        # ë³µì¡í•œ ë¶„ì„ì´ë‚˜ ë§ì€ ëª¨ë¸ì´ ìˆì„ ë•Œë§Œ ìƒì„±
        return len(analysis_data["models_analyzed"]) >= 3
    
    def _generate_interactive_dashboard(self, analysis_data: Dict[str, Any], output_dir: Path) -> Optional[str]:
        """ì„ íƒì  ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        try:
            import plotly.graph_objects as go
            import plotly.express as px
            from plotly.subplots import make_subplots
            
            # Plotlyë¥¼ ì‚¬ìš©í•œ ëŒ€ì‹œë³´ë“œ ìƒì„±
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥', 'ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë¹„êµ', 'ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„', 'ì„±ëŠ¥ ë¶„í¬')
            )
            
            # ì°¨íŠ¸ êµ¬ì„±...
            # HTML íŒŒì¼ë¡œ ì €ì¥
            dashboard_path = output_dir / "interactive_dashboard.html"
            fig.write_html(dashboard_path)
            
            return str(dashboard_path)
            
        except ImportError:
            logger.warning("plotlyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None
        except Exception as e:
            logger.error(f"ëŒ€ì‹œë³´ë“œ ìƒì„± ì‹¤íŒ¨: {e}")
            return None

### 7.2 ë¶„ì„ ê²°ê³¼ íŒŒì¼ êµ¬ì¡°

`analyze_session()` ë©”ì„œë“œë¥¼ ì‹¤í–‰í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê°„ì†Œí™”ëœ êµ¬ì¡°ë¡œ ë¶„ì„ ê²°ê³¼ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤:

#### 7.2.1 ê¸°ë³¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
~/Library/selvage-eval/analyze_results/{session_id}/
â”œâ”€â”€ analysis_report.md          # í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ (ë©”ì¸)
â”œâ”€â”€ analysis_data.json          # ì›ì‹œ ë°ì´í„° (í”„ë¡œê·¸ë˜ë° í™œìš©ìš©)
â””â”€â”€ interactive_dashboard.html  # ì„ íƒì  í†µí•© ëŒ€ì‹œë³´ë“œ (ì •ë§ í•„ìš”ì‹œë§Œ)
```

#### 7.2.2 ì‹¤ì œ ì˜ˆì‹œ

ì„¸ì…˜ IDê°€ `eval_20250708_004754_a1b2c3d4`ì¸ ê²½ìš°:

```
~/Library/selvage-eval/analyze_results/eval_20250708_004754_a1b2c3d4/
â”œâ”€â”€ analysis_report.md          # 20-50KB (í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ)
â”œâ”€â”€ analysis_data.json          # 100-500KB (êµ¬ì¡°í™”ëœ ë°ì´í„°)
â””â”€â”€ interactive_dashboard.html  # 200-800KB (ë³µì¡í•œ ë¶„ì„ì‹œì—ë§Œ ìƒì„±)
```

#### 7.2.3 ê° íŒŒì¼ë³„ ìƒì„¸ ì„¤ëª…

**1. `analysis_report.md` - í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ (ë©”ì¸)**

ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì— í†µí•©í•˜ì—¬ ì œê³µí•©ë‹ˆë‹¤.

```markdown
# DeepEval ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ì‹œê°„**: 2025-07-08 10:30:45
**ë¶„ì„ ëŒ€ìƒ ëª¨ë¸**: gemini-2.5-flash, gemini-2.5-pro, o3, o4-mini-high
**ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 150ê°œ

## ğŸ“‹ í•µì‹¬ ê²°ê³¼ ìš”ì•½
- ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: gemini-2.5-pro (ì¢…í•© ì ìˆ˜: 0.892, ë“±ê¸‰: A)
- ğŸ“Š Correctness ìµœê³ : gemini-2.5-pro (ì ìˆ˜: 0.925)
- ğŸ¯ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸: claude-sonnet-4 (í‘œì¤€í¸ì°¨: 0.045)
- âš ï¸ actionability ê°œì„  í•„ìš”: 12ê°œ ì‹¤íŒ¨ ì‚¬ë¡€ ë°œê²¬

## ğŸ¯ ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
| ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë“±ê¸‰ | Correctness | Clarity | Actionability | JSON Correctness | í•©ê²©ë¥  |
|--------|----------|------|-------------|---------|---------------|------------------|--------|
| gemini-2.5-pro | 0.892 | A | 0.925 | 0.875 | 0.845 | 0.980 | 89.2% |
| claude-sonnet-4 | 0.834 | B | 0.815 | 0.890 | 0.820 | 0.960 | 83.1% |
| o3 | 0.798 | B | 0.780 | 0.835 | 0.775 | 0.945 | 79.5% |
| o4-mini-high | 0.745 | C | 0.720 | 0.785 | 0.730 | 0.935 | 72.8% |

## ğŸ“Š ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„
### gemini-2.5-pro
- **Correctness**: 0.925 Â± 0.067
- **Clarity**: 0.875 Â± 0.054
- **Actionability**: 0.845 Â± 0.089
- **JSON Correctness**: 0.980 Â± 0.023
- **ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: 150ê°œ

### [ë‹¤ë¥¸ ëª¨ë¸ë“¤ ìƒì„¸ ë¶„ì„...]

## ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
### Correctness
- **ì´ ì‹¤íŒ¨ ìˆ˜**: 23ê°œ
- **ì´ìŠˆ ëˆ„ë½**: 8ê°œ
- **ì˜ëª»ëœ ë¼ì¸ ë²ˆí˜¸**: 5ê°œ
- **ë¶€ì ì ˆí•œ ì‹¬ê°ë„**: 6ê°œ
- **ê¸°íƒ€**: 4ê°œ

### [ë‹¤ë¥¸ ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ íŒ¨í„´...]

## ğŸ“ˆ í†µê³„ ë¶„ì„ ê²°ê³¼
- **í‰ê·  ì„±ëŠ¥**: 0.817
- **ì„±ëŠ¥ í‘œì¤€í¸ì°¨**: 0.061
- **ìµœê³  ì„±ëŠ¥**: 0.892
- **ìµœì € ì„±ëŠ¥**: 0.745
- **ê¸°ì¤€ì (0.7) ì´ìƒ ëª¨ë¸**: 4ê°œ

## ğŸ’¡ ê¶Œì¥ì‚¬í•­ ë° ê²°ë¡ 
### ì£¼ìš” ê¶Œì¥ì‚¬í•­
1. ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: gemini-2.5-pro (ì¢…í•© ì ìˆ˜: 0.892, ë“±ê¸‰: A)
2. ğŸ“Š Correctness ìµœê³ : gemini-2.5-pro (ì ìˆ˜: 0.925)
3. ğŸ¯ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸: claude-sonnet-4 (í‘œì¤€í¸ì°¨: 0.045)

### ê°œì„  ë°©í–¥
- ì‹¤íŒ¨ íŒ¨í„´ì´ ë§ì€ ë©”íŠ¸ë¦­ì— ëŒ€í•œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
- ì¼ê´€ì„±ì´ ë‚®ì€ ëª¨ë¸ì˜ ì•ˆì •ì„± í–¥ìƒ ë°©ì•ˆ ê²€í† 
- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ ì¥ì ì„ ë‹¤ë¥¸ ëª¨ë¸ì— ì ìš©
```

**2. `analysis_data.json` - êµ¬ì¡°í™”ëœ ì›ì‹œ ë°ì´í„° (í”„ë¡œê·¸ë˜ë° í™œìš©ìš©)**

í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°í™”ëœ JSON ë°ì´í„°ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

```json
{
  "analysis_metadata": {
    "analysis_timestamp": "2025-07-08T10:30:45.123456",
    "models_analyzed": ["gemini-2.5-flash", "gemini-2.5-pro", "o3", "o4-mini-high"]
  },
  "model_performances": [
    {
      "model_name": "gemini-2.5-pro",
      "total_test_cases": 150,
      "correctness_mean": 0.925,
      "correctness_std": 0.067,
      "clarity_mean": 0.875,
      "clarity_std": 0.054,
      "actionability_mean": 0.845,
      "actionability_std": 0.089,
      "json_correctness_mean": 0.980,
      "json_correctness_std": 0.023,
      "overall_score": 0.892,
      "pass_rate": 0.892,
      "grade": "A"
    }
  ],
  "failure_patterns": [
    {
      "metric_name": "correctness",
      "total_failures": 23,
      "missing_issues": 8,
      "incorrect_line_numbers": 5,
      "inappropriate_severity": 6,
      "unclear_descriptions": 2,
      "non_actionable_suggestions": 1,
      "json_format_errors": 1,
      "other": 0
    }
  ],
  "statistical_analysis": {
    "mean_performance": 0.817,
    "std_performance": 0.061,
    "min_performance": 0.745,
    "max_performance": 0.892,
    "performance_range": 0.147,
    "models_above_threshold": 4
  },
  "recommendations": [
    "ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: gemini-2.5-pro (ì¢…í•© ì ìˆ˜: 0.892, ë“±ê¸‰: A)",
    "ğŸ“Š Correctness ìµœê³ : gemini-2.5-pro (ì ìˆ˜: 0.925)"
  ]
}
```

**3. `interactive_dashboard.html` - ì„ íƒì  í†µí•© ëŒ€ì‹œë³´ë“œ**

3ê°œ ì´ìƒì˜ ëª¨ë¸ì´ ë¶„ì„ë  ë•Œë§Œ ìƒì„±ë˜ëŠ” ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œì…ë‹ˆë‹¤.

- **Plotly ê¸°ë°˜ í†µí•© ëŒ€ì‹œë³´ë“œ**
- **ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥, ë©”íŠ¸ë¦­ë³„ ë ˆì´ë” ì°¨íŠ¸, ì‹¤íŒ¨ íŒ¨í„´, ì„±ëŠ¥ ë¶„í¬ë¥¼ í•œ í™”ë©´ì—ì„œ í™•ì¸**
- **ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ì—´ì–´ì„œ í™•ì¸ ê°€ëŠ¥**
- **í™•ëŒ€/ì¶•ì†Œ, í•„í„°ë§, ë°ì´í„° í¬ì¸íŠ¸ ìƒì„¸ ì •ë³´ ì œê³µ**

#### 7.2.4 ì‚¬ìš©ì ê°€ì´ë“œ

**1. ë¹ ë¥¸ í™•ì¸ ìˆœì„œ**
1. `analysis_report.md` - í†µí•© ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œì—ì„œ ëª¨ë“  ë¶„ì„ ê²°ê³¼ í™•ì¸
2. `interactive_dashboard.html` - ë³µì¡í•œ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ì°¸ì¡°

**2. ìƒì„¸ ë¶„ì„**
- `analysis_data.json` - í”„ë¡œê·¸ë˜ë° ë°©ì‹ìœ¼ë¡œ ë°ì´í„° í™œìš©
- `analysis_report.md` - í…ìŠ¤íŠ¸ ê¸°ë°˜ ìƒì„¸ ë¶„ì„ ë° íŒ¨í„´ í™•ì¸

**3. ë³´ê³ ì„œ í™œìš©**
- **ê²½ì˜ì§„ ë³´ê³ **: `analysis_report.md`ì˜ í•µì‹¬ ê²°ê³¼ ìš”ì•½ ì„¹ì…˜ í™œìš©
- **ê¸°ìˆ íŒ€ ë¶„ì„**: `analysis_report.md`ì˜ ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ë¶„ì„ ë° ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ í™œìš©
- **ìë™í™” ì²˜ë¦¬**: `analysis_data.json` í™œìš©

**4. ì¥ì **
- **í†µí•©ì„±**: í•œ íŒŒì¼(`analysis_report.md`)ì—ì„œ ëª¨ë“  ì •ë³´ í™•ì¸ ê°€ëŠ¥
- **ì ‘ê·¼ì„±**: í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰/ë³µì‚¬/ê³µìœ  ìš©ì´
- **íš¨ìœ¨ì„±**: ë¡œë”© ì‹œê°„ ë‹¨ì¶•, í”„ë¦°íŠ¸ ì¹œí™”ì 
- **ì„ íƒì  ì‹œê°í™”**: ì •ë§ í•„ìš”ì‹œì—ë§Œ ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ ì°¸ì¡°

# ì‚¬ìš© ì˜ˆì‹œ

```python
from selvage_eval.analysis import DeepEvalAnalysisEngine

if __name__ == "__main__":
    # ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™” (ê¸°ë³¸ ì¶œë ¥ ë””ë ‰í† ë¦¬: ~/Library/selvage-eval/analyze_results)
    engine = DeepEvalAnalysisEngine()
    
    # ì„¸ì…˜ ë¶„ì„ ì‹¤í–‰ (ì¶œë ¥ ë””ë ‰í† ë¦¬ ìë™ ìƒì„±)
    session_path = "~/Library/selvage-eval/deepeval_results/eval_20250708_004754_a1b2c3d4"
    results = engine.analyze_session(session_path)
    # ê²°ê³¼ëŠ” ~/Library/selvage-eval/analyze_results/eval_20250708_004754_a1b2c3d4/ ì— ì €ì¥ë¨
    
    # ì»¤ìŠ¤í…€ ì¶œë ¥ ë””ë ‰í† ë¦¬ ì§€ì •ë„ ê°€ëŠ¥
    # results = engine.analyze_session(session_path, output_dir="/custom/path")
    
    print("ë¶„ì„ ì™„ë£Œ!")
    print(f"ë¶„ì„ëœ ëª¨ë¸: {results['analysis_metadata']['models_analyzed']}")
    print(f"ìƒì„±ëœ íŒŒì¼:")
    print(f"  - ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ: {results['files_generated']['markdown_report']}")
    print(f"  - JSON ë°ì´í„°: {results['files_generated']['json_data']}")
    if results['files_generated']['interactive_dashboard']:
        print(f"  - ì¸í„°ë™í‹°ë¸Œ ëŒ€ì‹œë³´ë“œ: {results['files_generated']['interactive_dashboard']}")
```