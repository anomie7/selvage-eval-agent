# Selvage í‰ê°€ ì—ì´ì „íŠ¸ - í‰ê°€ ë° ë¶„ì„ (3-4ë‹¨ê³„)

### 3ë‹¨ê³„: DeepEval ë³€í™˜ ë° í‰ê°€

#### ëª©í‘œ
Selvage ê²°ê³¼ë¥¼ DeepEval í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  í‰ê°€

#### ë³€í™˜ ìŠ¤í‚¤ë§ˆ
# DeepEval Test case êµ¬ì¡°
**ì£¼ìš” í•„ë“œ ì„¤ëª…:**
-  input: review_logì˜ promptì™€ ëŒ€ì‘ë¨
-  actual_output: review_logì˜ review_responseì™€ ëŒ€ì‘ë¨
-  expected_output: None(í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)

```python
import json
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional


def scan_review_logs(base_path: str = "~/Library/selvage-eval-agent/review_logs") -> List[Dict[str, Any]]:
    """
    review_logs ë””ë ‰í† ë¦¬ êµ¬ì¡°ë¥¼ íƒìƒ‰í•˜ì—¬ ëª¨ë“  ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì„ ì°¾ëŠ”ë‹¤.
    
    Args:
        base_path: ë¦¬ë·° ë¡œê·¸ ê¸°ë³¸ ê²½ë¡œ
    
    Returns:
        ë¦¬ë·° ë¡œê·¸ íŒŒì¼ ì •ë³´ ë¦¬ìŠ¤íŠ¸ (repo_name, commit_id, model_name, file_path í¬í•¨)
    """
    review_logs = []
    base_path = Path(base_path).expanduser()
    
    if not base_path.exists():
        print(f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        return review_logs
    
    # repo_name í´ë” ìˆœíšŒ
    for repo_dir in base_path.iterdir():
        if not repo_dir.is_dir():
            continue
            
        repo_name = repo_dir.name
        
        # commit_id í´ë” ìˆœíšŒ
        for commit_dir in repo_dir.iterdir():
            if not commit_dir.is_dir():
                continue
                
            commit_id = commit_dir.name
            
            # model_name í´ë” ìˆœíšŒ
            for model_dir in commit_dir.iterdir():
                if not model_dir.is_dir():
                    continue
                    
                model_name = model_dir.name
                
                # ë¦¬ë·° ë¡œê·¸ íŒŒì¼ ì°¾ê¸° (.json íŒŒì¼)
                for log_file in model_dir.glob("*.json"):
                    review_logs.append({
                        "repo_name": repo_name,
                        "commit_id": commit_id,
                        "model_name": model_name,
                        "file_path": log_file,
                        "file_name": log_file.name
                    })
    
    return review_logs


def extract_prompt_and_response(log_file_path: Path) -> Optional[Dict[str, Any]]:
    """
    ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì—ì„œ promptì™€ review_responseë¥¼ ì¶”ì¶œí•œë‹¤.
    
    Args:
        log_file_path: ë¦¬ë·° ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
    
    Returns:
        ì¶”ì¶œëœ promptì™€ review_response ë°ì´í„°
    """
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            log_data = json.load(f)
        
        # promptì™€ review_response í•„ë“œ ì¶”ì¶œ
        prompt = log_data.get("prompt", [])
        review_response = log_data.get("review_response", {})
        
        if not prompt or not review_response:
            print(f"í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤: {log_file_path}")
            return None
        
        return {
            "prompt": prompt,
            "review_response": review_response,
            "original_data": log_data  # ì›ë³¸ ë°ì´í„°ë„ í¬í•¨
        }
        
    except Exception as e:
        print(f"íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {log_file_path}: {e}")
        return None


def convert_to_deepeval_format(repo_name: str, model_name: str, 
                             extracted_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì¶”ì¶œëœ ë°ì´í„°ë¥¼ DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•œë‹¤.
    
    Args:
        repo_name: ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„
        model_name: ëª¨ë¸ ì´ë¦„
        extracted_data: ì¶”ì¶œëœ promptì™€ response ë°ì´í„°
    
    Returns:
        DeepEval í˜•ì‹ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    """
    return {
        "input": json.dumps(extracted_data["prompt"]),
        "actual_output": json.dumps(extracted_data["review_response"])
    }


def save_deepeval_test_cases(repo_name: str, model_name: str, 
                           test_cases: List[Dict[str, Any]], 
                           output_dir: str = "~/Library/selvage-eval-agent/deep_eval_test_case") -> str:
    """
    DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤.
    
    Args:
        repo_name: ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„
        model_name: ëª¨ë¸ ì´ë¦„
        test_cases: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
    
    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ
    """
    output_path = Path(output_dir).expanduser()
    output_path.mkdir(parents=True, exist_ok=True)
    
    # í˜„ì¬ ì‹œê°„ì„ ê¸°ë°˜ìœ¼ë¡œ íŒŒì¼ëª… ìƒì„±
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"test_data_{timestamp}_{repo_name}_{model_name}.json"
    
    file_path = output_path / filename
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(test_cases, f, ensure_ascii=False, indent=2)
    
    print(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì €ì¥ ì™„ë£Œ: {file_path}")
    return str(file_path)


def process_review_logs_to_deepeval():
    """
    ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    print("ë¦¬ë·° ë¡œê·¸ ìŠ¤ìº” ì‹œì‘...")
    review_logs = scan_review_logs()
    
    if not review_logs:
        print("ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"ì´ {len(review_logs)}ê°œì˜ ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")
    
    # repo_nameê³¼ model_nameë³„ë¡œ ê·¸ë£¹í™”
    grouped_logs = {}
    for log_info in review_logs:
        key = (log_info["repo_name"], log_info["model_name"])
        if key not in grouped_logs:
            grouped_logs[key] = []
        grouped_logs[key].append(log_info)
    
    # ê° ê·¸ë£¹ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
    for (repo_name, model_name), logs in grouped_logs.items():
        print(f"\nì²˜ë¦¬ ì¤‘: {repo_name} - {model_name} ({len(logs)}ê°œ íŒŒì¼)")
        
        test_cases = []
        for log_info in logs:
            extracted_data = extract_prompt_and_response(log_info["file_path"])
            if extracted_data:
                test_case = convert_to_deepeval_format(repo_name, model_name, extracted_data)
                test_cases.append(test_case)
        
        if test_cases:
            saved_file = save_deepeval_test_cases(repo_name, model_name, test_cases)
            print(f"  â†’ {len(test_cases)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì €ì¥: {saved_file}")
        else:
            print(f"  â†’ ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")


# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    process_review_logs_to_deepeval()
```

#### í‰ê°€ ë©”íŠ¸ë¦­

DeepEvalì„ ì‚¬ìš©í•œ 4ê°œ í•µì‹¬ ë©”íŠ¸ë¦­ìœ¼ë¡œ Selvage ë¦¬ë·° í’ˆì§ˆì„ ì •ëŸ‰í™”í•©ë‹ˆë‹¤.

**1. Correctness (ì •í™•ì„±) - ì„ê³„ê°’: 0.7**
```python
correctness = GEval(
    name="Correctness",
    model="gemini-2.5-pro-preview-05-06",
    evaluation_steps=[
        "ì…ë ¥ ì½”ë“œì—ì„œ ë°œê²¬ëœ ëª¨ë“  ê´€ë ¨ ì£¼ìš” ì´ìŠˆ(ë²„ê·¸, ë³´ì•ˆ ì·¨ì•½ì , ì„±ëŠ¥ ë¬¸ì œ, ì¤‘ëŒ€í•œ ìŠ¤íƒ€ì¼/ì„¤ê³„ ê²°í•¨)ê°€ 'issues' ë°°ì—´ì— ë³´ê³ ë˜ì—ˆëŠ”ì§€ í™•ì¸",
        "'issues' ë°°ì—´ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°, ì…ë ¥ ì½”ë“œë¥¼ ë¹„íŒì ìœ¼ë¡œ í‰ê°€í•˜ì—¬ íƒì§€ ì‹¤íŒ¨ê°€ ì•„ë‹Œ ì‹¤ì œ ì´ìŠˆ ë¶€ì¬ì¸ì§€ í™•ì¸",
        "ì´ìŠˆê°€ ë³´ê³ ëœ ê²½ìš°, íŒŒì¼ëª…ê³¼ ë¼ì¸ ë²ˆí˜¸ì˜ ì •í™•ì„± í™•ì¸",
        "ì´ìŠˆ ìœ í˜•(ë²„ê·¸, ë³´ì•ˆ, ì„±ëŠ¥, ìŠ¤íƒ€ì¼, ì„¤ê³„)ì´ í•´ë‹¹ ì½”ë“œì— ì ì ˆí•œì§€ í‰ê°€",
        "ì‹¬ê°ë„ ìˆ˜ì¤€(info, warning, error)ì´ ê° ì´ìŠˆì˜ ì‹¤ì œ ì˜í–¥ì— ë”°ë¼ ì ì ˆíˆ í• ë‹¹ë˜ì—ˆëŠ”ì§€ í™•ì¸",
        "ì´ìŠˆ ì„¤ëª…ì´ ì½”ë“œ ë³€ê²½ì˜ ì˜í–¥ì„ ì •í™•í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ë°˜ì˜í•˜ëŠ”ì§€ ê²€í† ",
        "'issues' ë°°ì—´ì´ ì •ë‹¹í•˜ê²Œ ë¹„ì–´ìˆëŠ” ê²½ìš°, 'summary'ê°€ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì´ë“œë¼ì¸ì— ë”°ë¼ ì ì ˆíˆ ëª…ì‹œí•˜ëŠ”ì§€ í™•ì¸"
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],
    threshold=0.7,
)
```

**2. Clarity (ëª…í™•ì„±) - ì„ê³„ê°’: 0.7**
```python
clarity = GEval(
    name="Clarity",
    model="gemini-2.5-pro-preview-05-06",
    evaluation_steps=[
        "ì „ì²´ ì½”ë“œ ë¦¬ë·° ì¶œë ¥(ìš”ì•½, ì´ìŠˆ ì„¤ëª…, ì œì•ˆ, ê¶Œì¥ì‚¬í•­)ì´ ê°„ê²°í•˜ê³  ì§ì ‘ì ì¸ ì–¸ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í‰ê°€",
        "ì´ìŠˆ ì„¤ëª…ê³¼ ì œì•ˆ, ê¶Œì¥ì‚¬í•­ì´ êµ¬ì²´ì ì´ê³  ëª…í™•í•œì§€ í‰ê°€",
        "ì½”ë“œ ë³€ê²½ì˜ ëª©ì ê³¼ ì˜ë„ê°€ ëª…í™•í•˜ê²Œ ì´í•´ ê°€ëŠ¥í•œì§€ ê²€í† ",
        "ê°œì„ ëœ ì½”ë“œ ì˜ˆì‹œê°€ ì œê³µë˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ì§€ í™•ì¸"
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT],
    threshold=0.7,
)
```

**3. Actionability (ì‹¤í–‰ê°€ëŠ¥ì„±) - ì„ê³„ê°’: 0.7**
```python
actionability = GEval(
    name="Actionability",
    model="gemini-2.5-pro-preview-05-06",
    evaluation_steps=[
        "ê° ì´ìŠˆì— ëŒ€í•´ êµ¬ì²´ì ì¸ í•´ê²°ì±…ì´ ì œì‹œë˜ì—ˆëŠ”ì§€ í™•ì¸",
        "ì œì•ˆëœ ê°œì„  ì‚¬í•­ì´ ì‹¤ì œë¡œ êµ¬í˜„ ê°€ëŠ¥í•œì§€ í‰ê°€",
        "ì½”ë“œ ê°œì„  ì˜ˆì‹œê°€ ì‹¤ì œ ì½”ë“œë² ì´ìŠ¤ì— í†µí•©ë  ìˆ˜ ìˆì„ ë§Œí¼ êµ¬ì²´ì ì¸ì§€ ê²€í† ",
        "ì œì•ˆì´ ì½”ë“œ í’ˆì§ˆ, ì„±ëŠ¥, ë³´ì•ˆ ë“±ì˜ ì¸¡ë©´ì—ì„œ ì‹¤ì§ˆì ì¸ ê°œì„ ì„ ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ”ì§€ í‰ê°€",
        "ì „ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­ì´ í”„ë¡œì íŠ¸ ë§¥ë½ì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œì§€ í™•ì¸"
    ],
    evaluation_params=[LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.INPUT],
    threshold=0.7,
)
```

**4. JsonCorrectnessMetric (JSON ìŠ¤í‚¤ë§ˆ ê²€ì¦)**
```python
jsoncorrectness = JsonCorrectnessMetric(
    expected_schema=StructuredReviewResponse(
        issues=[
            StructuredReviewIssue(
                type="",           # ì´ìŠˆ ìœ í˜• (ë²„ê·¸, ë³´ì•ˆ, ì„±ëŠ¥, ìŠ¤íƒ€ì¼, ì„¤ê³„)
                line_number=0,     # ë¼ì¸ ë²ˆí˜¸
                file="",           # íŒŒì¼ ê²½ë¡œ
                description="",    # ì´ìŠˆ ì„¤ëª…
                suggestion="",     # ê°œì„  ì œì•ˆ
                severity=IssueSeverityEnum.INFO,  # ì‹¬ê°ë„ (info, warning, error)
                target_code="",    # ë¬¸ì œê°€ ìˆëŠ” ì›ë³¸ ì½”ë“œ
                suggested_code="", # ê°œì„ ëœ ì½”ë“œ
            )
        ],
        summary="",           # ì „ì²´ ë¦¬ë·° ìš”ì•½
        score=0,             # 0-10 í’ˆì§ˆ ì ìˆ˜
        recommendations=[],   # ì „ë°˜ì ì¸ ê¶Œì¥ì‚¬í•­ ëª©ë¡
    ),
    model="gemini-2.5-pro-preview-05-06",
    include_reason=True,
)
```

**í‰ê°€ ì‹¤í–‰ ì½”ë“œ**
```python
@pytest.mark.parametrize("test_case", dataset)
def test_code_review_evaluation(test_case: LLMTestCase):
    """ì½”ë“œ ë¦¬ë·° í‰ê°€ í…ŒìŠ¤íŠ¸."""
    assert_test(
        test_case,
        metrics=[correctness, clarity, actionability, jsoncorrectness],
    )
```

**ë©”íŠ¸ë¦­ ì ìˆ˜ í•´ì„**
- **0.7 ì´ìƒ**: í†µê³¼ (ì–‘ì§ˆì˜ ë¦¬ë·°)
- **0.5-0.7**: ë³´í†µ (ê°œì„  í•„ìš”)
- **0.5 ë¯¸ë§Œ**: ì‹¤íŒ¨ (ì‹¬ê°í•œ ë¬¸ì œ)

**ê²°ê³¼ ë¶„ì„ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸**
DeepEval ê²°ê³¼ì˜ ì˜ì–´ ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ê³  ê°€ë…ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•´ ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©:

```markdown
# ROLE
ë‹¹ì‹ ì€ ë¬¸ì œ í•´ê²° ì¤‘ì‹¬ì˜ ì •í™•í•˜ê³  ì¶©ì‹¤í•œ í…Œí¬ë‹ˆì»¬ ë¼ì´í„°ì…ë‹ˆë‹¤.

# PROBLEM
deepevalì˜ metricì„ í†µí•´ í‰ê°€í•œ ê²°ê³¼ì—ì„œ fail reasonì´ ì˜ì–´ë¡œ ì í˜€ìˆì–´ í‰ê°€ê°€ ì–´ë µìŠµë‹ˆë‹¤.

## INSTRUCTIONS
1. ê° testCaseì˜ metricsDataë§Œ ì¶”ì¶œ
2. metricsData.reasonë“¤ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­
3. reason ì˜ê²¬ì„ í† ëŒ€ë¡œ input(í”„ë¡¬í”„íŠ¸), actualOutputì—ì„œ ë¬¸ì œ ë¶€ë¶„ë§Œ ì²¨ë¶€
4. reason ê²°ê³¼ë¥¼ ì¢…í•©í•œ ì˜ê²¬ ì²¨ë¶€

ê°€ë…ì„±ì„ ê³ ë ¤í•´ì„œ í¸ì§‘í•´ì„œ ë°˜í™˜í•´ì£¼ì„¸ìš”.
```

### 4ë‹¨ê³„: ê²°ê³¼ ë¶„ì„ ë° ë¹„êµ

#### ëª©í‘œ
DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ë³„ ì„±ëŠ¥ì„ ì •ëŸ‰ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ , ì‹¤ì œ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ actionable insightsë¥¼ ìë™ ë„ì¶œ

#### 4.1 DeepEval ê²°ê³¼ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬

**ê²°ê³¼ íŒŒì¼ êµ¬ì¡° ìŠ¤ìº”**
```python
def collect_deepeval_results(base_path: str = "~/Library/selvage-eval-agent/deep_eval_test_case") -> Dict[str, Any]:
    """
    DeepEval í‰ê°€ ê²°ê³¼ íŒŒì¼ë“¤ì„ ìˆ˜ì§‘í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤.
    
    Returns:
        repo_name, model_nameë³„ë¡œ ë¶„ë¥˜ëœ í‰ê°€ ê²°ê³¼ ë°ì´í„°
    """
    results = {}
    base_path = Path(base_path).expanduser()
    
    for result_file in base_path.glob("deepeval_results_*.json"):
        # íŒŒì¼ëª…ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ: deepeval_results_20250622_143022_cline_gemini-2.5-pro.json
        parts = result_file.stem.split("_")
        if len(parts) >= 6:
            timestamp = f"{parts[2]}_{parts[3]}"
            repo_name = parts[4]
            model_name = "_".join(parts[5:])
            
            with open(result_file, 'r', encoding='utf-8') as f:
                result_data = json.load(f)
                
            key = f"{repo_name}_{model_name}"
            if key not in results:
                results[key] = []
            
            results[key].append({
                "timestamp": timestamp,
                "repo_name": repo_name,
                "model_name": model_name,
                "file_path": result_file,
                "data": result_data
            })
    
    return results
```

**ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì§‘ê³„ ë° í†µê³„ ì²˜ë¦¬**
```python
def analyze_metric_scores(evaluation_results: List[Dict]) -> Dict[str, Any]:
    """
    4ê°œ ë©”íŠ¸ë¦­ë³„ ìƒì„¸ í†µê³„ ë¶„ì„ì„ ìˆ˜í–‰í•œë‹¤.
    
    Returns:
        ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ë¶„í¬, ì‹¤íŒ¨ íŒ¨í„´, í†µê³„ì  íŠ¹ì„±
    """
    metrics_analysis = {
        "correctness": {"scores": [], "failures": []},
        "clarity": {"scores": [], "failures": []},
        "actionability": {"scores": [], "failures": []},
        "json_correctness": {"scores": [], "failures": []}
    }
    
    for result in evaluation_results:
        test_cases = result["data"].get("testCases", [])
        
        for test_case in test_cases:
            if not test_case.get("success", True):
                # ì‹¤íŒ¨í•œ ì¼€ì´ìŠ¤ ë¶„ì„
                metrics_data = test_case.get("metricsData", [])
                
                for metric in metrics_data:
                    metric_name = metric.get("name", "").lower()
                    score = metric.get("score", 0)
                    reason = metric.get("reason", "")
                    
                    if metric_name in metrics_analysis:
                        metrics_analysis[metric_name]["scores"].append(score)
                        
                        if not metric.get("success", True):
                            metrics_analysis[metric_name]["failures"].append({
                                "score": score,
                                "reason": reason,
                                "test_case_id": test_case.get("id", ""),
                                "repo_name": result["repo_name"],
                                "model_name": result["model_name"]
                            })
    
    # í†µê³„ ê³„ì‚°
    for metric_name, data in metrics_analysis.items():
        scores = data["scores"]
        if scores:
            data["statistics"] = {
                "count": len(scores),
                "mean": np.mean(scores),
                "median": np.median(scores),
                "std": np.std(scores),
                "min": np.min(scores),
                "max": np.max(scores),
                "q25": np.percentile(scores, 25),
                "q75": np.percentile(scores, 75),
                "pass_rate": len([s for s in scores if s >= 0.7]) / len(scores)
            }
    
    return metrics_analysis
```

#### 4.2 ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„

**ì¢…í•© ì„±ëŠ¥ ë§¤íŠ¸ë¦­ìŠ¤ ìƒì„±**
```python
def generate_model_performance_matrix(results: Dict[str, Any]) -> pd.DataFrame:
    """
    ëª¨ë¸ë³„ ì„±ëŠ¥ì„ ë‹¤ì°¨ì›ì ìœ¼ë¡œ ë¹„êµí•˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ë¥¼ ìƒì„±í•œë‹¤.
    
    Returns:
        ëª¨ë¸ë³„ ì„±ëŠ¥ ì§€í‘œê°€ í¬í•¨ëœ DataFrame
    """
    performance_data = []
    
    for key, model_results in results.items():
        repo_name, model_name = key.split("_", 1)
        
        # ê° ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ì§‘ê³„
        all_scores = {"correctness": [], "clarity": [], "actionability": [], "json_correctness": []}
        total_cost = 0
        total_time = 0
        total_tests = 0
        
        for result in model_results:
            test_cases = result["data"].get("testCases", [])
            total_tests += len(test_cases)
            
            for test_case in test_cases:
                metrics_data = test_case.get("metricsData", [])
                for metric in metrics_data:
                    metric_name = metric.get("name", "").lower()
                    if metric_name in all_scores:
                        all_scores[metric_name].append(metric.get("score", 0))
        
        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        performance_metrics = {
            "repo_name": repo_name,
            "model_name": model_name,
            "total_test_cases": total_tests,
            "correctness_mean": np.mean(all_scores["correctness"]) if all_scores["correctness"] else 0,
            "correctness_std": np.std(all_scores["correctness"]) if all_scores["correctness"] else 0,
            "clarity_mean": np.mean(all_scores["clarity"]) if all_scores["clarity"] else 0,
            "clarity_std": np.std(all_scores["clarity"]) if all_scores["clarity"] else 0,
            "actionability_mean": np.mean(all_scores["actionability"]) if all_scores["actionability"] else 0,
            "actionability_std": np.std(all_scores["actionability"]) if all_scores["actionability"] else 0,
            "json_correctness_mean": np.mean(all_scores["json_correctness"]) if all_scores["json_correctness"] else 0,
            "overall_pass_rate": calculate_overall_pass_rate(all_scores),
            "consistency_score": calculate_consistency_score(all_scores),  # ë‚®ì€ í‘œì¤€í¸ì°¨ = ë†’ì€ ì¼ê´€ì„±
            "weighted_score": calculate_weighted_performance_score(all_scores)
        }
        
        performance_data.append(performance_metrics)
    
    return pd.DataFrame(performance_data)

def calculate_weighted_performance_score(scores: Dict[str, List[float]]) -> float:
    """
    ë©”íŠ¸ë¦­ë³„ ê°€ì¤‘ì¹˜ë¥¼ ì ìš©í•œ ì¢…í•© ì„±ëŠ¥ ì ìˆ˜ë¥¼ ê³„ì‚°í•œë‹¤.
    
    ê°€ì¤‘ì¹˜: Correctness(40%), Clarity(25%), Actionability(25%), JsonCorrectness(10%)
    """
    weights = {"correctness": 0.4, "clarity": 0.25, "actionability": 0.25, "json_correctness": 0.1}
    weighted_sum = 0
    
    for metric, weight in weights.items():
        if scores[metric]:
            weighted_sum += np.mean(scores[metric]) * weight
    
    return weighted_sum
```

**ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ì°¨ì´ ë¶„ì„**
```python
def analyze_tech_stack_performance(performance_df: pd.DataFrame) -> Dict[str, Any]:
    """
    ì €ì¥ì†Œë³„/ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ì°¨ì´ë¥¼ ë¶„ì„í•œë‹¤.
    """
    tech_stack_mapping = {
        "cline": "typescript",
        "ecommerce-microservices": "java_spring", 
        "kotlin-realworld": "kotlin_jpa",
        "selvage-deprecated": "mixed"
    }
    
    performance_df["tech_stack"] = performance_df["repo_name"].map(tech_stack_mapping)
    
    # ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
    tech_analysis = {}
    
    for tech_stack in performance_df["tech_stack"].unique():
        stack_data = performance_df[performance_df["tech_stack"] == tech_stack]
        
        tech_analysis[tech_stack] = {
            "best_model": {
                "correctness": stack_data.loc[stack_data["correctness_mean"].idxmax(), "model_name"],
                "clarity": stack_data.loc[stack_data["clarity_mean"].idxmax(), "model_name"],
                "actionability": stack_data.loc[stack_data["actionability_mean"].idxmax(), "model_name"],
                "overall": stack_data.loc[stack_data["weighted_score"].idxmax(), "model_name"]
            },
            "performance_gap": {
                "max_correctness": stack_data["correctness_mean"].max(),
                "min_correctness": stack_data["correctness_mean"].min(),
                "gap": stack_data["correctness_mean"].max() - stack_data["correctness_mean"].min()
            },
            "consistency_ranking": stack_data.nsmallest(3, "correctness_std")[["model_name", "correctness_std"]].to_dict("records")
        }
    
    return tech_analysis
```

#### 4.3 ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ë„ì¶œ

**ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ë¥˜ ë° íŒ¨í„´ ë¶„ì„**
```python
def analyze_failure_patterns(metrics_analysis: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤ì„ ë¶„ì„í•˜ì—¬ ê³µí†µ íŒ¨í„´ê³¼ ê°œì„  ë°©í–¥ì„ ë„ì¶œí•œë‹¤.
    """
    failure_patterns = {}
    
    for metric_name, data in metrics_analysis.items():
        failures = data["failures"]
        
        if not failures:
            continue
            
        # ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ë¥˜
        reason_categories = categorize_failure_reasons(failures)
        
        # ëª¨ë¸ë³„ ì‹¤íŒ¨ íŒ¨í„´
        model_failures = {}
        for failure in failures:
            model = failure["model_name"]
            if model not in model_failures:
                model_failures[model] = []
            model_failures[model].append(failure)
        
        # ì €ì¥ì†Œë³„ ì‹¤íŒ¨ íŒ¨í„´  
        repo_failures = {}
        for failure in failures:
            repo = failure["repo_name"]
            if repo not in repo_failures:
                repo_failures[repo] = []
            repo_failures[repo].append(failure)
        
        failure_patterns[metric_name] = {
            "total_failures": len(failures),
            "reason_categories": reason_categories,
            "worst_performing_models": sorted(model_failures.items(), 
                                           key=lambda x: len(x[1]), reverse=True)[:3],
            "problematic_repos": sorted(repo_failures.items(), 
                                      key=lambda x: len(x[1]), reverse=True)[:3],
            "improvement_suggestions": generate_improvement_suggestions(reason_categories, metric_name)
        }
    
    return failure_patterns

def categorize_failure_reasons(failures: List[Dict]) -> Dict[str, int]:
    """ì‹¤íŒ¨ ì‚¬ìœ ë¥¼ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜í•œë‹¤."""
    categories = {
        "missing_issues": 0,          # ì´ìŠˆ ëˆ„ë½
        "incorrect_line_numbers": 0,   # ì˜ëª»ëœ ë¼ì¸ ë²ˆí˜¸
        "inappropriate_severity": 0,   # ë¶€ì ì ˆí•œ ì‹¬ê°ë„
        "unclear_descriptions": 0,     # ë¶ˆëª…í™•í•œ ì„¤ëª…
        "non_actionable_suggestions": 0, # ì‹¤í–‰ ë¶ˆê°€ëŠ¥í•œ ì œì•ˆ
        "json_format_errors": 0,       # JSON í˜•ì‹ ì˜¤ë¥˜
        "other": 0
    }
    
    for failure in failures:
        reason = failure["reason"].lower()
        
        if "missing" in reason or "not identified" in reason:
            categories["missing_issues"] += 1
        elif "line number" in reason or "incorrect" in reason:
            categories["incorrect_line_numbers"] += 1
        elif "severity" in reason or "inappropriate" in reason:
            categories["inappropriate_severity"] += 1
        elif "unclear" in reason or "vague" in reason:
            categories["unclear_descriptions"] += 1
        elif "actionable" in reason or "implementable" in reason:
            categories["non_actionable_suggestions"] += 1
        elif "json" in reason or "format" in reason:
            categories["json_format_errors"] += 1
        else:
            categories["other"] += 1
    
    return categories

def generate_improvement_suggestions(reason_categories: Dict[str, int], metric_name: str) -> List[str]:
    """ì‹¤íŒ¨ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œì„  ì œì•ˆì„ ìƒì„±í•œë‹¤."""
    suggestions = []
    
    if reason_categories["missing_issues"] > 5:
        suggestions.append(f"{metric_name}: ì´ìŠˆ íƒì§€ìœ¨ í–¥ìƒì„ ìœ„í•´ í”„ë¡¬í”„íŠ¸ì— ë” êµ¬ì²´ì ì¸ ê²€í†  ì§€ì¹¨ ì¶”ê°€ í•„ìš”")
    
    if reason_categories["incorrect_line_numbers"] > 3:
        suggestions.append(f"{metric_name}: ë¼ì¸ ë²ˆí˜¸ ì •í™•ì„± í–¥ìƒì„ ìœ„í•´ diff íŒŒì‹± ë¡œì§ ê°œì„  í•„ìš”")
    
    if reason_categories["unclear_descriptions"] > 4:
        suggestions.append(f"{metric_name}: ì„¤ëª… ëª…í™•ì„± í–¥ìƒì„ ìœ„í•´ ì˜ˆì‹œ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ê°œì„  í•„ìš”")
    
    if reason_categories["non_actionable_suggestions"] > 3:
        suggestions.append(f"{metric_name}: ì‹¤í–‰ê°€ëŠ¥í•œ ì œì•ˆì„ ìœ„í•´ ì½”ë“œ ì˜ˆì‹œ í¬í•¨ ì§€ì¹¨ ê°•í™” í•„ìš”")
    
    return suggestions
```

#### 4.4 í”„ë¡¬í”„íŠ¸ ë²„ì „ íš¨ê³¼ì„± ë¶„ì„

**A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„**
```python
def analyze_prompt_version_effectiveness(results: Dict[str, Any]) -> Dict[str, Any]:
    """
    ë™ì¼ ì»¤ë°‹ì— ëŒ€í•œ ë‹¤ë¥¸ í”„ë¡¬í”„íŠ¸ ë²„ì „ ì ìš© ê²°ê³¼ë¥¼ í†µê³„ì ìœ¼ë¡œ ë¹„êµ ë¶„ì„í•œë‹¤.
    """
    prompt_comparison = {}
    
    # í”„ë¡¬í”„íŠ¸ ë²„ì „ë³„ ê²°ê³¼ ê·¸ë£¹í™” (review_logì˜ prompt_version í•„ë“œ ê¸°ì¤€)
    version_results = group_by_prompt_version(results)
    
    for v1, v2 in itertools.combinations(version_results.keys(), 2):
        # ë™ì¼ ì»¤ë°‹ì— ëŒ€í•œ ê²°ê³¼ë§Œ ë¹„êµ
        common_commits = find_common_commits(version_results[v1], version_results[v2])
        
        if len(common_commits) < 5:  # í†µê³„ì  ìœ ì˜ì„±ì„ ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
            continue
            
        comparison_result = perform_statistical_comparison(
            version_results[v1], version_results[v2], common_commits
        )
        
        prompt_comparison[f"{v1}_vs_{v2}"] = comparison_result
    
    return prompt_comparison

def perform_statistical_comparison(v1_results: List, v2_results: List, common_commits: List) -> Dict[str, Any]:
    """ë‘ í”„ë¡¬í”„íŠ¸ ë²„ì „ ê°„ í†µê³„ì  ë¹„êµë¥¼ ìˆ˜í–‰í•œë‹¤."""
    v1_scores = extract_scores_for_commits(v1_results, common_commits)
    v2_scores = extract_scores_for_commits(v2_results, common_commits)
    
    # ëŒ€ì‘í‘œë³¸ t-ê²€ì • (paired t-test)
    from scipy import stats
    
    comparison = {}
    
    for metric in ["correctness", "clarity", "actionability", "json_correctness"]:
        v1_metric_scores = [scores[metric] for scores in v1_scores]
        v2_metric_scores = [scores[metric] for scores in v2_scores]
        
        # í†µê³„ì  ê²€ì •
        t_stat, p_value = stats.ttest_rel(v2_metric_scores, v1_metric_scores)
        effect_size = calculate_cohens_d(v2_metric_scores, v1_metric_scores)
        
        comparison[metric] = {
            "v1_mean": np.mean(v1_metric_scores),
            "v2_mean": np.mean(v2_metric_scores),
            "improvement": np.mean(v2_metric_scores) - np.mean(v1_metric_scores),
            "improvement_percentage": ((np.mean(v2_metric_scores) - np.mean(v1_metric_scores)) / np.mean(v1_metric_scores)) * 100,
            "t_statistic": t_stat,
            "p_value": p_value,
            "is_significant": p_value < 0.05,
            "effect_size": effect_size,
            "effect_magnitude": interpret_effect_size(effect_size)
        }
    
    return comparison

def calculate_cohens_d(group1: List[float], group2: List[float]) -> float:
    """Cohen's d íš¨ê³¼ í¬ê¸°ë¥¼ ê³„ì‚°í•œë‹¤."""
    mean1, mean2 = np.mean(group1), np.mean(group2)
    std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)
    
    pooled_std = np.sqrt(((len(group1) - 1) * std1**2 + (len(group2) - 1) * std2**2) / 
                        (len(group1) + len(group2) - 2))
    
    return (mean2 - mean1) / pooled_std

def interpret_effect_size(d: float) -> str:
    """Cohen's d ê°’ì„ í•´ì„í•œë‹¤."""
    abs_d = abs(d)
    if abs_d < 0.2:
        return "ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” íš¨ê³¼"
    elif abs_d < 0.5:
        return "ì‘ì€ íš¨ê³¼"
    elif abs_d < 0.8:
        return "ì¤‘ê°„ íš¨ê³¼"
    else:
        return "í° íš¨ê³¼"
```

#### 4.5 ìë™í™”ëœ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë° ì˜ì‚¬ê²°ì • ì§€ì›

**ìµœì  ëª¨ë¸ ì¡°í•© ìë™ ì¶”ì²œ**
```python
def recommend_optimal_model_configuration(performance_df: pd.DataFrame, 
                                        cost_data: Dict[str, float],
                                        priority: str = "balanced") -> Dict[str, Any]:
    """
    ì„±ëŠ¥, ë¹„ìš©, ì¼ê´€ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•œ ìµœì  ëª¨ë¸ ì¡°í•©ì„ ì¶”ì²œí•œë‹¤.
    
    Args:
        priority: "performance", "cost", "balanced" ì¤‘ ì„ íƒ
    """
    recommendations = {}
    
    # ìš°ì„ ìˆœìœ„ë³„ ê°€ì¤‘ì¹˜ ì„¤ì •
    weight_configs = {
        "performance": {"performance": 0.7, "cost": 0.1, "consistency": 0.2},
        "cost": {"performance": 0.3, "cost": 0.5, "consistency": 0.2},
        "balanced": {"performance": 0.5, "cost": 0.3, "consistency": 0.2}
    }
    
    weights = weight_configs[priority]
    
    # ì €ì¥ì†Œë³„ ìµœì  ëª¨ë¸ ì¶”ì²œ
    for repo in performance_df["repo_name"].unique():
        repo_data = performance_df[performance_df["repo_name"] == repo]
        
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        repo_data["performance_score_norm"] = normalize_scores(repo_data["weighted_score"])
        repo_data["cost_score_norm"] = normalize_scores([1/cost_data.get(model, 1) for model in repo_data["model_name"]])
        repo_data["consistency_score_norm"] = normalize_scores([1/std for std in repo_data["correctness_std"]])
        
        # ì¢…í•© ì ìˆ˜ ê³„ì‚°
        repo_data["total_score"] = (
            repo_data["performance_score_norm"] * weights["performance"] +
            repo_data["cost_score_norm"] * weights["cost"] +
            repo_data["consistency_score_norm"] * weights["consistency"]
        )
        
        best_model = repo_data.loc[repo_data["total_score"].idxmax()]
        
        recommendations[repo] = {
            "recommended_model": best_model["model_name"],
            "total_score": best_model["total_score"],
            "performance_score": best_model["weighted_score"],
            "estimated_monthly_cost": estimate_monthly_cost(best_model["model_name"], cost_data),
            "confidence_level": calculate_confidence_level(repo_data),
            "alternative_models": get_alternative_models(repo_data, best_model["model_name"])
        }
    
    return recommendations

def generate_actionable_insights(performance_df: pd.DataFrame, 
                               failure_patterns: Dict[str, Any],
                               prompt_comparison: Dict[str, Any]) -> List[str]:
    """ì‹¤ì œ ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ì•¡ì…˜ ì•„ì´í…œì„ ìƒì„±í•œë‹¤."""
    insights = []
    
    # ì„±ëŠ¥ ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
    best_overall_model = performance_df.loc[performance_df["weighted_score"].idxmax(), "model_name"]
    insights.append(f"ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_overall_model} (ì¢…í•© ì ìˆ˜: {performance_df['weighted_score'].max():.3f})")
    
    # ì¼ê´€ì„± ê¸°ë°˜ ì¸ì‚¬ì´íŠ¸
    most_consistent_model = performance_df.loc[performance_df["consistency_score"].idxmax(), "model_name"]
    insights.append(f"ğŸ¯ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ëª¨ë¸: {most_consistent_model}")
    
    # ê°œì„  ìš°ì„ ìˆœìœ„
    for metric, patterns in failure_patterns.items():
        if patterns["total_failures"] > 10:
            worst_model = patterns["worst_performing_models"][0][0]
            insights.append(f"âš ï¸ {metric} ê°œì„  í•„ìš”: {worst_model} ëª¨ë¸ì˜ ì‹¤íŒ¨ìœ¨ì´ ë†’ìŒ")
    
    # í”„ë¡¬í”„íŠ¸ ê°œì„  íš¨ê³¼
    for comparison, result in prompt_comparison.items():
        for metric, data in result.items():
            if data["is_significant"] and data["improvement"] > 0.1:
                insights.append(f"ğŸ“ˆ {comparison} ë¹„êµ: {metric}ì—ì„œ {data['improvement_percentage']:.1f}% ìœ ì˜ë¯¸í•œ ê°œì„ ")
    
    return insights
```

#### 4.6 ìë™í™”ëœ ë³´ê³ ì„œ ìƒì„± ë° ì‹œê°í™”

**ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìŠ¤í‚¤ë§ˆ**
```json
{
  "evaluation_session": {
    "id": "eval_20240622_143022_a1b2c3d",
    "date": "2024-06-22T14:30:22Z",
    "repositories_analyzed": ["cline", "ecommerce-microservices", "kotlin-realworld"],
    "models_compared": ["gemini-2.5-pro", "claude-sonnet-4", "claude-sonnet-4-thinking"],
    "total_test_cases": 450,
    "evaluation_duration_minutes": 127
  },
  "executive_summary": {
    "key_findings": [
      "gemini-2.5-proê°€ TypeScript ì½”ë“œì—ì„œ ìµœê³  ì„±ëŠ¥ (0.847 ì¢…í•© ì ìˆ˜)",
      "claude-sonnet-4ê°€ ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ê²°ê³¼ ì œê³µ (í‘œì¤€í¸ì°¨ 0.045)",
      "í”„ë¡¬í”„íŠ¸ v3ê°€ v2 ëŒ€ë¹„ Correctnessì—ì„œ í‰ê·  15.3% ê°œì„ "
    ],
    "recommended_actions": [
      "TypeScript í”„ë¡œì íŠ¸ì—ëŠ” gemini-2.5-pro ìš°ì„  ì‚¬ìš©",
      "í”„ë¡¬í”„íŠ¸ v3ë¥¼ ëª¨ë“  ëª¨ë¸ì— ì ìš©",
      "Java ì½”ë“œ ë¦¬ë·° í’ˆì§ˆ ê°œì„ ì„ ìœ„í•œ specialized prompt ê°œë°œ"
    ],
    "estimated_cost_impact": {
      "current_monthly_cost": 245.67,
      "optimized_monthly_cost": 189.23,
      "savings_percentage": 23.0
    }
  },
  "detailed_analysis": {
    "model_performance_matrix": {
      "gemini-2.5-pro": {
        "overall_score": 0.847,
        "correctness": {"mean": 0.832, "std": 0.067, "pass_rate": 0.89},
        "clarity": {"mean": 0.798, "std": 0.054, "pass_rate": 0.92},
        "actionability": {"mean": 0.756, "std": 0.089, "pass_rate": 0.85},
        "json_correctness": {"mean": 0.945, "std": 0.023, "pass_rate": 0.98},
        "strengths": ["ë†’ì€ ì •í™•ì„±", "ìš°ìˆ˜í•œ JSON í˜•ì‹ ì¤€ìˆ˜"],
        "weaknesses": ["ì‹¤í–‰ê°€ëŠ¥ì„± ì ìˆ˜ê°€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ"],
        "best_for": ["typescript", "javascript", "python"]
      },
      "claude-sonnet-4": {
        "overall_score": 0.789,
        "correctness": {"mean": 0.778, "std": 0.045, "pass_rate": 0.82},
        "clarity": {"mean": 0.823, "std": 0.041, "pass_rate": 0.94},
        "actionability": {"mean": 0.801, "std": 0.052, "pass_rate": 0.88},
        "json_correctness": {"mean": 0.934, "std": 0.031, "pass_rate": 0.96},
        "strengths": ["ê°€ì¥ ì¼ê´€ì„± ìˆëŠ” ì„±ëŠ¥", "ìš°ìˆ˜í•œ ëª…í™•ì„±"],
        "weaknesses": ["ì •í™•ì„±ì—ì„œ ìƒëŒ€ì  ì—´ì„¸"],
        "best_for": ["java", "kotlin", "enterprise_code"]
      }
    },
    "failure_pattern_analysis": {
      "correctness_failures": {
        "total": 23,
        "categories": {
          "missing_issues": 8,
          "incorrect_line_numbers": 5,
          "inappropriate_severity": 6,
          "other": 4
        },
        "worst_performing_repos": ["ecommerce-microservices", "kotlin-realworld"],
        "improvement_suggestions": [
          "Java/Kotlin ì½”ë“œë¥¼ ìœ„í•œ specialized prompt ê°œë°œ",
          "ë¼ì¸ ë²ˆí˜¸ ë§¤í•‘ ì •í™•ë„ ê°œì„  ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„"
        ]
      }
    },
    "prompt_version_comparison": {
      "v2_vs_v3": {
        "sample_size": 45,
        "correctness": {
          "improvement": 0.153,
          "p_value": 0.003,
          "effect_size": 0.712,
          "significance": "í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ê°œì„ "
        },
        "clarity": {
          "improvement": 0.087,
          "p_value": 0.021,
          "effect_size": 0.423,
          "significance": "ì¤‘ê°„ ì •ë„ì˜ ê°œì„ "
        }
      }
    },
    "optimization_recommendations": {
      "model_assignments": {
        "cline": "gemini-2.5-pro",
        "ecommerce-microservices": "claude-sonnet-4", 
        "kotlin-realworld": "claude-sonnet-4",
        "selvage-deprecated": "gemini-2.5-pro"
      },
      "prompt_versions": {
        "current_best": "v3",
        "next_iteration_focus": ["actionability ê°œì„ ", "Java/Kotlin íŠ¹í™”"]
      },
      "cost_optimization": {
        "high_confidence_cases": "gemini-2.5-flash ì‚¬ìš©ìœ¼ë¡œ 30% ë¹„ìš© ì ˆê° ê°€ëŠ¥",
        "critical_cases": "claude-sonnet-4-thinking ì‚¬ìš©ìœ¼ë¡œ í’ˆì§ˆ ë³´ì¥"
      }
    }
  },
  "monitoring_alerts": [
    {
      "type": "performance_degradation",
      "model": "gemini-2.5-pro",
      "repo": "ecommerce-microservices", 
      "metric": "correctness",
      "threshold": 0.7,
      "current_value": 0.647,
      "recommendation": "í”„ë¡¬í”„íŠ¸ ì¬ì¡°ì • ë˜ëŠ” ëª¨ë¸ êµì²´ ê²€í† "
    }
  ],
  "next_evaluation_recommendations": {
    "focus_areas": ["Java ì½”ë“œ ë¦¬ë·° í’ˆì§ˆ", "ë¹„ìš© ìµœì í™”", "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"],
    "new_test_cases": ["ë³µì¡í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§", "ë³´ì•ˆ ì·¨ì•½ì  íƒì§€"],
    "experiment_ideas": ["ëª¨ë¸ ì•™ìƒë¸”", "dynamic prompt selection"]
  }
}
```

**ì‹œê°í™” ìë™ ìƒì„±**
```python
def generate_performance_visualizations(performance_df: pd.DataFrame, 
                                      output_dir: str) -> List[str]:
    """ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ì°¨íŠ¸ë¡œ ìƒì„±í•œë‹¤."""
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    output_paths = []
    
    # 1. ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ë ˆì´ë” ì°¨íŠ¸
    fig, ax = plt.subplots(figsize=(10, 8), subplot_kw=dict(projection='polar'))
    
    metrics = ['correctness_mean', 'clarity_mean', 'actionability_mean', 'json_correctness_mean']
    for _, row in performance_df.iterrows():
        values = [row[metric] for metric in metrics]
        values += values[:1]  # ì°¨íŠ¸ë¥¼ ë‹«ê¸° ìœ„í•´ ì²« ê°’ì„ ë§ˆì§€ë§‰ì— ì¶”ê°€
        
        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]
        
        ax.plot(angles, values, 'o-', linewidth=2, label=row['model_name'])
        ax.fill(angles, values, alpha=0.25)
    
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(['Correctness', 'Clarity', 'Actionability', 'JSON Correctness'])
    ax.set_ylim(0, 1)
    ax.legend()
    ax.set_title('ëª¨ë¸ë³„ ì¢…í•© ì„±ëŠ¥ ë¹„êµ')
    
    radar_path = f"{output_dir}/model_performance_radar.png"
    plt.savefig(radar_path, dpi=300, bbox_inches='tight')
    output_paths.append(radar_path)
    plt.close()
    
    # 2. ì €ì¥ì†Œë³„ ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ
    pivot_data = performance_df.pivot(index='model_name', columns='repo_name', values='weighted_score')
    
    plt.figure(figsize=(12, 8))
    sns.heatmap(pivot_data, annot=True, cmap='RdYlGn', center=0.7, 
                fmt='.3f', cbar_kws={'label': 'ì¢…í•© ì„±ëŠ¥ ì ìˆ˜'})
    plt.title('ì €ì¥ì†Œë³„ ëª¨ë¸ ì„±ëŠ¥ íˆíŠ¸ë§µ')
    plt.xlabel('ì €ì¥ì†Œ')
    plt.ylabel('ëª¨ë¸')
    
    heatmap_path = f"{output_dir}/repo_model_heatmap.png"
    plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')
    output_paths.append(heatmap_path)
    plt.close()
    
    # 3. ì„±ëŠ¥ vs ì¼ê´€ì„± ì‚°ì ë„
    plt.figure(figsize=(10, 8))
    
    for repo in performance_df['repo_name'].unique():
        repo_data = performance_df[performance_df['repo_name'] == repo]
        plt.scatter(repo_data['weighted_score'], repo_data['consistency_score'], 
                   label=repo, s=100, alpha=0.7)
        
        # ëª¨ë¸ëª… ë¼ë²¨ ì¶”ê°€
        for _, row in repo_data.iterrows():
            plt.annotate(row['model_name'], 
                        (row['weighted_score'], row['consistency_score']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)
    
    plt.xlabel('ì¢…í•© ì„±ëŠ¥ ì ìˆ˜')
    plt.ylabel('ì¼ê´€ì„± ì ìˆ˜')
    plt.title('ì„±ëŠ¥ vs ì¼ê´€ì„± ë¶„ì„')
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    scatter_path = f"{output_dir}/performance_consistency_scatter.png"
    plt.savefig(scatter_path, dpi=300, bbox_inches='tight')
    output_paths.append(scatter_path)
    plt.close()
    
    return output_paths
```

## í‰ê°€ ì§€í‘œ ë° ë¶„ì„

### ì •ëŸ‰ì  ë©”íŠ¸ë¦­
1. **ë¦¬ë·° í’ˆì§ˆ**:
   - ì´ìŠˆ íƒì§€ ì •í™•ë„
   - ì œì•ˆ ì‚¬í•­ì˜ ì‹¤ìš©ì„±
   - ì½”ë“œ ì´í•´ë„

2. **ì„±ëŠ¥ ë©”íŠ¸ë¦­**:
   - í‰ê·  ì‘ë‹µ ì‹œê°„
   - í† í° ì‚¬ìš© íš¨ìœ¨ì„±
   - API í˜¸ì¶œ ë¹„ìš©

3. **ì‹ ë¢°ì„± ë©”íŠ¸ë¦­**:
   - ì„±ê³µë¥ 
   - ì—ëŸ¬ ë¹ˆë„
   - ì¼ê´€ì„±

### ë¹„êµ ë¶„ì„ ë°©ë²•
```python
def compare_models(results: Dict[str, List[EvaluationResult]]) -> ComparisonReport:
    """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„
    
    Args:
        results: ëª¨ë¸ë³„ í‰ê°€ ê²°ê³¼
        
    Returns:
        ë¹„êµ ë¶„ì„ ë³´ê³ ì„œ
    """
    report = ComparisonReport()
    
    for model, evals in results.items():
        metrics = calculate_aggregate_metrics(evals)
        report.add_model_metrics(model, metrics)
    
    report.generate_statistical_comparison()
    return report
```

## Phase 3-4 Tool êµ¬í˜„

### Phase 3 Tools: DeepEval Conversion

#### ReviewLogScannerTool - ë¦¬ë·° ë¡œê·¸ ìŠ¤ìº”
```python
class ReviewLogScannerTool(Tool):
    """ë¦¬ë·° ë¡œê·¸ íŒŒì¼ ìŠ¤ìº” ë° ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    
    @property
    def name(self) -> str:
        return "review_log_scanner"
    
    @property
    def description(self) -> str:
        return "ë¦¬ë·° ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ìŠ¤ìº”í•˜ì—¬ ëª¨ë“  ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì„ ì°¾ê³  ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"
    
    @property
    def parameters_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "base_path": {
                    "type": "string", 
                    "description": "ë¦¬ë·° ë¡œê·¸ ê¸°ë³¸ ê²½ë¡œ",
                    "default": "~/Library/selvage-eval-agent/review_logs"
                }
            }
        }
    
    async def execute(self, **kwargs) -> ToolResult:
        base_path = kwargs.get("base_path", "~/Library/selvage-eval-agent/review_logs")
        
        review_logs = []
        base_path = Path(base_path).expanduser()
        
        try:
            if not base_path.exists():
                return ToolResult(
                    success=False,
                    error_message=f"ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}"
                )
            
            # ë””ë ‰í† ë¦¬ êµ¬ì¡° íƒìƒ‰: repo_name/commit_id/model_name/*.json
            for repo_dir in base_path.iterdir():
                if not repo_dir.is_dir():
                    continue
                    
                for commit_dir in repo_dir.iterdir():
                    if not commit_dir.is_dir():
                        continue
                        
                    for model_dir in commit_dir.iterdir():
                        if not model_dir.is_dir():
                            continue
                            
                        for log_file in model_dir.glob("*.json"):
                            metadata = await self._extract_log_metadata(log_file)
                            review_logs.append({
                                "repo_name": repo_dir.name,
                                "commit_id": commit_dir.name,
                                "model_name": model_dir.name,
                                "file_path": str(log_file),
                                "file_name": log_file.name,
                                "metadata": metadata
                            })
            
            return ToolResult(
                success=True, 
                data={
                    "review_logs": review_logs,
                    "total_count": len(review_logs),
                    "scan_path": str(base_path)
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error_message=f"Failed to scan review logs: {str(e)}"
            )
    
    async def _extract_log_metadata(self, log_file: Path) -> Dict[str, Any]:
        """ë¦¬ë·° ë¡œê·¸ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        try:
            with open(log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            return {
                "log_id": log_data.get("id"),
                "model": log_data.get("model", {}),
                "created_at": log_data.get("created_at"),
                "status": log_data.get("status", "UNKNOWN"),
                "prompt_version": log_data.get("prompt_version"),
                "file_size": log_file.stat().st_size,
                "has_prompt": bool(log_data.get("prompt")),
                "has_response": bool(log_data.get("review_response"))
            }
        except Exception as e:
            return {"error": str(e)}
```

#### DeepEvalConverterTool - DeepEval í˜•ì‹ ë³€í™˜
```python
class DeepEvalConverterTool(Tool):
    """ë¦¬ë·° ë¡œê·¸ë¥¼ DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ë³€í™˜"""
    
    @property
    def name(self) -> str:
        return "deepeval_converter"
    
    @property
    def description(self) -> str:
        return "ë¦¬ë·° ë¡œê·¸ ë°ì´í„°ë¥¼ DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤"
    
    @property
    def parameters_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "review_logs": {
                    "type": "array",
                    "description": "ë³€í™˜í•  ë¦¬ë·° ë¡œê·¸ ì •ë³´ ë¦¬ìŠ¤íŠ¸"
                },
                "output_dir": {
                    "type": "string",
                    "description": "ì¶œë ¥ ë””ë ‰í† ë¦¬",
                    "default": "~/Library/selvage-eval-agent/deep_eval_test_case"
                },
                "group_by": {
                    "type": "string",
                    "description": "ê·¸ë£¹í™” ê¸°ì¤€ (repo_model, repo, model)",
                    "default": "repo_model"
                }
            },
            "required": ["review_logs"]
        }
    
    async def execute(self, **kwargs) -> ToolResult:
        review_logs = kwargs["review_logs"]
        output_dir = kwargs.get("output_dir", "~/Library/selvage-eval-agent/deep_eval_test_case")
        group_by = kwargs.get("group_by", "repo_model")
        
        try:
            output_path = Path(output_dir).expanduser()
            output_path.mkdir(parents=True, exist_ok=True)
            
            # ê·¸ë£¹ë³„ë¡œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
            grouped_logs = self._group_logs(review_logs, group_by)
            converted_files = []
            
            for group_key, logs in grouped_logs.items():
                test_cases = []
                for log_info in logs:
                    test_case = await self._convert_single_log(log_info)
                    if test_case:
                        test_cases.append(test_case)
                
                if test_cases:
                    file_path = await self._save_test_cases(
                        group_key, test_cases, output_path
                    )
                    converted_files.append({
                        "group": group_key,
                        "file_path": file_path,
                        "test_case_count": len(test_cases)
                    })
            
            return ToolResult(
                success=True,
                data={
                    "converted_files": converted_files,
                    "total_test_cases": sum(f["test_case_count"] for f in converted_files),
                    "output_directory": str(output_path)
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error_message=f"Failed to convert logs: {str(e)}"
            )
    
    def _group_logs(self, review_logs: List[Dict], group_by: str) -> Dict[str, List[Dict]]:
        """ë¡œê·¸ë¥¼ ê·¸ë£¹ë³„ë¡œ ë¶„ë¥˜"""
        grouped = {}
        
        for log in review_logs:
            if group_by == "repo_model":
                key = f"{log['repo_name']}_{log['model_name']}"
            elif group_by == "repo":
                key = log['repo_name']
            elif group_by == "model":
                key = log['model_name']
            else:
                key = "all"
            
            if key not in grouped:
                grouped[key] = []
            grouped[key].append(log)
        
        return grouped
    
    async def _convert_single_log(self, log_info: Dict) -> Optional[Dict[str, Any]]:
        """ë‹¨ì¼ ë¡œê·¸ë¥¼ DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ë³€í™˜"""
        try:
            with open(log_info["file_path"], 'r', encoding='utf-8') as f:
                log_data = json.load(f)
            
            prompt = log_data.get("prompt", [])
            review_response = log_data.get("review_response", {})
            
            if not prompt or not review_response:
                return None
            
            return {
                "input": json.dumps(prompt, ensure_ascii=False),
                "actual_output": json.dumps(review_response, ensure_ascii=False),
                "expected_output": None,  # í˜„ì¬ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                "metadata": {
                    "repo_name": log_info["repo_name"],
                    "commit_id": log_info["commit_id"],
                    "model_name": log_info["model_name"],
                    "log_id": log_data.get("id"),
                    "created_at": log_data.get("created_at")
                }
            }
        except Exception as e:
            print(f"Failed to convert log {log_info['file_path']}: {e}")
            return None
    
    async def _save_test_cases(self, group_key: str, test_cases: List[Dict], 
                              output_path: Path) -> str:
        """í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"test_data_{timestamp}_{group_key}.json"
        file_path = output_path / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(test_cases, f, ensure_ascii=False, indent=2)
        
        return str(file_path)
```

#### MetricEvaluatorTool - ë©”íŠ¸ë¦­ í‰ê°€ ì‹¤í–‰
```python
class MetricEvaluatorTool(Tool):
    """DeepEval ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•œ í‰ê°€ ì‹¤í–‰"""
    
    @property
    def name(self) -> str:
        return "metric_evaluator"
    
    @property
    def description(self) -> str:
        return "DeepEval ë©”íŠ¸ë¦­ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ í‰ê°€í•©ë‹ˆë‹¤"
    
    @property
    def parameters_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "test_case_files": {
                    "type": "array",
                    "description": "í‰ê°€í•  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ íŒŒì¼ ëª©ë¡"
                },
                "metrics": {
                    "type": "array",
                    "description": "ì‚¬ìš©í•  ë©”íŠ¸ë¦­ ëª©ë¡",
                    "default": ["correctness", "clarity", "actionability", "json_correctness"]
                },
                "judge_model": {
                    "type": "string",
                    "description": "í‰ê°€ì— ì‚¬ìš©í•  judge ëª¨ë¸",
                    "default": "gpt-4"
                },
                "output_dir": {
                    "type": "string",
                    "description": "ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬",
                    "default": "~/Library/selvage-eval-agent/evaluation_results"
                }
            },
            "required": ["test_case_files"]
        }
    
    async def execute(self, **kwargs) -> ToolResult:
        test_case_files = kwargs["test_case_files"]
        metrics = kwargs.get("metrics", ["correctness", "clarity", "actionability", "json_correctness"])
        judge_model = kwargs.get("judge_model", "gpt-4")
        output_dir = kwargs.get("output_dir", "~/Library/selvage-eval-agent/evaluation_results")
        
        try:
            output_path = Path(output_dir).expanduser()
            output_path.mkdir(parents=True, exist_ok=True)
            
            evaluation_results = []
            
            for file_info in test_case_files:
                file_path = file_info["file_path"]
                group = file_info["group"]
                
                # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œë“œ
                with open(file_path, 'r', encoding='utf-8') as f:
                    test_cases = json.load(f)
                
                # DeepEval í‰ê°€ ì‹¤í–‰
                results = await self._run_deepeval_evaluation(
                    test_cases, metrics, judge_model
                )
                
                # ê²°ê³¼ ì €ì¥
                result_file = await self._save_evaluation_results(
                    group, results, output_path
                )
                
                evaluation_results.append({
                    "group": group,
                    "test_case_file": file_path,
                    "result_file": result_file,
                    "test_case_count": len(test_cases),
                    "evaluation_count": len(results)
                })
            
            return ToolResult(
                success=True,
                data={
                    "evaluation_results": evaluation_results,
                    "total_evaluations": sum(r["evaluation_count"] for r in evaluation_results),
                    "output_directory": str(output_path)
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error_message=f"Failed to evaluate metrics: {str(e)}"
            )
    
    async def _run_deepeval_evaluation(self, test_cases: List[Dict], 
                                     metrics: List[str], judge_model: str) -> List[Dict]:
        """DeepEvalì„ ì‚¬ìš©í•œ ì‹¤ì œ í‰ê°€ ì‹¤í–‰"""
        from deepeval.metrics import (
            AnswerRelevancyMetric, 
            FaithfulnessMetric,
            HallucinationMetric,
            G_Eval
        )
        from deepeval.test_case import LLMTestCase
        
        # ë©”íŠ¸ë¦­ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        metric_instances = {}
        
        if "correctness" in metrics:
            metric_instances["correctness"] = G_Eval(
                name="Correctness",
                criteria="ì½”ë“œ ë¦¬ë·°ì˜ ì •í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
                evaluation_params=[
                    "ì´ìŠˆ ì‹ë³„ì˜ ì •í™•ì„±",
                    "ì œì•ˆ ì‚¬í•­ì˜ ì ì ˆì„±",
                    "ì½”ë“œ ì´í•´ë„"
                ],
                model=judge_model
            )
        
        if "clarity" in metrics:
            metric_instances["clarity"] = G_Eval(
                name="Clarity",
                criteria="ë¦¬ë·° ë‚´ìš©ì˜ ëª…í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
                evaluation_params=[
                    "ì„¤ëª…ì˜ ì´í•´í•˜ê¸° ì‰¬ì›€",
                    "êµ¬ì²´ì ì¸ ì˜ˆì‹œ ì œê³µ",
                    "ì „ë¬¸ ìš©ì–´ ì‚¬ìš©ì˜ ì ì ˆì„±"
                ],
                model=judge_model
            )
        
        if "actionability" in metrics:
            metric_instances["actionability"] = G_Eval(
                name="Actionability",
                criteria="ë¦¬ë·°ì˜ ì‹¤í–‰ ê°€ëŠ¥ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
                evaluation_params=[
                    "êµ¬ì²´ì ì¸ í•´ê²° ë°©ì•ˆ ì œì‹œ",
                    "ì‹¤ì œ ì ìš© ê°€ëŠ¥ì„±",
                    "ìš°ì„ ìˆœìœ„ì˜ ëª…í™•ì„±"
                ],
                model=judge_model
            )
        
        if "json_correctness" in metrics:
            metric_instances["json_correctness"] = G_Eval(
                name="JsonCorrectness",
                criteria="JSON í˜•ì‹ì˜ ì •í™•ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤",
                evaluation_params=[
                    "JSON êµ¬ì¡°ì˜ ìœ íš¨ì„±",
                    "í•„ìˆ˜ í•„ë“œ í¬í•¨ ì—¬ë¶€",
                    "ë°ì´í„° íƒ€ì…ì˜ ì¼ê´€ì„±"
                ],
                model=judge_model
            )
        
        results = []
        
        for i, test_case in enumerate(test_cases):
            try:
                # DeepEval í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìƒì„±
                llm_test_case = LLMTestCase(
                    input=test_case["input"],
                    actual_output=test_case["actual_output"],
                    expected_output=test_case.get("expected_output")
                )
                
                # ê° ë©”íŠ¸ë¦­ë³„ í‰ê°€
                case_results = {
                    "test_case_index": i,
                    "metadata": test_case.get("metadata", {}),
                    "scores": {}
                }
                
                for metric_name, metric_instance in metric_instances.items():
                    try:
                        metric_instance.measure(llm_test_case)
                        case_results["scores"][metric_name] = {
                            "score": metric_instance.score,
                            "reason": getattr(metric_instance, 'reason', None),
                            "success": metric_instance.success
                        }
                    except Exception as e:
                        case_results["scores"][metric_name] = {
                            "score": 0.0,
                            "reason": f"Evaluation failed: {str(e)}",
                            "success": False
                        }
                
                results.append(case_results)
                
            except Exception as e:
                results.append({
                    "test_case_index": i,
                    "metadata": test_case.get("metadata", {}),
                    "scores": {},
                    "error": str(e)
                })
        
        return results
    
    async def _save_evaluation_results(self, group: str, results: List[Dict], 
                                     output_path: Path) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"evaluation_results_{timestamp}_{group}.json"
        file_path = output_path / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        return str(file_path)
```

### Phase 4 Tools: Analysis and Visualization

#### StatisticalAnalysisTool - í†µê³„ ë¶„ì„
```python
class StatisticalAnalysisTool(Tool):
    """DeepEval ê²°ê³¼ í†µê³„ ë¶„ì„"""
    
    @property
    def name(self) -> str:
        return "statistical_analysis"
    
    @property
    def description(self) -> str:
        return "í‰ê°€ ê²°ê³¼ë¥¼ í†µê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤"
    
    @property
    def parameters_schema(self) -> Dict[str, Any]:
        return {
            "type": "object",
            "properties": {
                "evaluation_files": {
                    "type": "array",
                    "description": "ë¶„ì„í•  í‰ê°€ ê²°ê³¼ íŒŒì¼ ëª©ë¡"
                },
                "analysis_type": {
                    "type": "string",
                    "description": "ë¶„ì„ ìœ í˜•",
                    "enum": ["comprehensive", "model_comparison", "failure_pattern", "repo_analysis"],
                    "default": "comprehensive"
                },
                "output_dir": {
                    "type": "string",
                    "description": "ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬",
                    "default": "~/Library/selvage-eval-agent/analysis_results"
                },
                "generate_visualizations": {
                    "type": "boolean",
                    "description": "ì‹œê°í™” ìƒì„± ì—¬ë¶€",
                    "default": True
                }
            },
            "required": ["evaluation_files"]
        }
    
    async def execute(self, **kwargs) -> ToolResult:
        evaluation_files = kwargs["evaluation_files"]
        analysis_type = kwargs.get("analysis_type", "comprehensive")
        output_dir = kwargs.get("output_dir", "~/Library/selvage-eval-agent/analysis_results")
        generate_visualizations = kwargs.get("generate_visualizations", True)
        
        try:
            output_path = Path(output_dir).expanduser()
            output_path.mkdir(parents=True, exist_ok=True)
            
            # í‰ê°€ ê²°ê³¼ ë¡œë“œ ë° í†µí•©
            all_results = []
            for file_info in evaluation_files:
                with open(file_info["result_file"], 'r', encoding='utf-8') as f:
                    results = json.load(f)
                    for result in results:
                        result["group"] = file_info["group"]
                    all_results.extend(results)
            
            # ë¶„ì„ ì‹¤í–‰
            if analysis_type == "comprehensive":
                analysis = await self._comprehensive_analysis(all_results)
            elif analysis_type == "model_comparison":
                analysis = await self._model_comparison_analysis(all_results)
            elif analysis_type == "failure_pattern":
                analysis = await self._failure_pattern_analysis(all_results)
            elif analysis_type == "repo_analysis":
                analysis = await self._repo_analysis(all_results)
            
            # ê²°ê³¼ ì €ì¥
            analysis_file = await self._save_analysis_results(
                analysis_type, analysis, output_path
            )
            
            # ì‹œê°í™” ìƒì„±
            visualization_files = []
            if generate_visualizations:
                visualization_files = await self._generate_visualizations(
                    analysis, output_path
                )
            
            return ToolResult(
                success=True,
                data={
                    "analysis_file": analysis_file,
                    "visualization_files": visualization_files,
                    "analysis_type": analysis_type,
                    "total_test_cases": len(all_results),
                    "output_directory": str(output_path)
                }
            )
            
        except Exception as e:
            return ToolResult(
                success=False,
                error_message=f"Statistical analysis failed: {str(e)}"
            )
    
    async def _comprehensive_analysis(self, results: List[Dict]) -> Dict[str, Any]:
        """ì¢…í•© í†µê³„ ë¶„ì„"""
        import numpy as np
        
        metrics_stats = {}
        all_metrics = ["correctness", "clarity", "actionability", "json_correctness"]
        
        for metric in all_metrics:
            scores = self._extract_metric_scores(results, metric)
            
            if scores:
                metrics_stats[metric] = {
                    "count": len(scores),
                    "mean": float(np.mean(scores)),
                    "median": float(np.median(scores)),
                    "std": float(np.std(scores)),
                    "min": float(np.min(scores)),
                    "max": float(np.max(scores)),
                    "q25": float(np.percentile(scores, 25)),
                    "q75": float(np.percentile(scores, 75)),
                    "pass_rate": len([s for s in scores if s >= 0.7]) / len(scores)
                }
            else:
                metrics_stats[metric] = {"error": "No valid scores found"}
        
        return {
            "analysis_type": "comprehensive",
            "timestamp": datetime.now().isoformat(),
            "metrics_statistics": metrics_stats,
            "overall_performance": self._calculate_overall_performance(metrics_stats),
            "recommendations": self._generate_recommendations(metrics_stats),
            "data_summary": {
                "total_test_cases": len(results),
                "successful_evaluations": len([r for r in results if not r.get("error")]),
                "failed_evaluations": len([r for r in results if r.get("error")])
            }
        }
    
    def _extract_metric_scores(self, results: List[Dict], metric: str) -> List[float]:
        """ë©”íŠ¸ë¦­ë³„ ì ìˆ˜ ì¶”ì¶œ"""
        scores = []
        for result in results:
            if "scores" in result and metric in result["scores"]:
                score_data = result["scores"][metric]
                if score_data.get("success", False) and isinstance(score_data.get("score"), (int, float)):
                    scores.append(float(score_data["score"]))
        return scores
    
    def _calculate_overall_performance(self, metrics_stats: Dict) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ê³„ì‚°"""
        valid_metrics = {k: v for k, v in metrics_stats.items() if "error" not in v}
        
        if not valid_metrics:
            return {"error": "No valid metrics for overall performance calculation"}
        
        overall_mean = sum(m["mean"] for m in valid_metrics.values()) / len(valid_metrics)
        overall_pass_rate = sum(m["pass_rate"] for m in valid_metrics.values()) / len(valid_metrics)
        
        return {
            "weighted_score": overall_mean,
            "overall_pass_rate": overall_pass_rate,
            "consistency": 1.0 - (sum(m["std"] for m in valid_metrics.values()) / len(valid_metrics)),
            "grade": self._assign_grade(overall_mean)
        }
    
    def _assign_grade(self, score: float) -> str:
        """ì ìˆ˜ì— ë”°ë¥¸ ë“±ê¸‰ í• ë‹¹"""
        if score >= 0.9:
            return "A"
        elif score >= 0.8:
            return "B"
        elif score >= 0.7:
            return "C"
        elif score >= 0.6:
            return "D"
        else:
            return "F"
    
    def _generate_recommendations(self, metrics_stats: Dict) -> List[str]:
        """ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        for metric, stats in metrics_stats.items():
            if "error" in stats:
                continue
                
            if stats["mean"] < 0.7:
                recommendations.append(f"{metric} ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤ (í˜„ì¬ í‰ê· : {stats['mean']:.3f})")
            
            if stats["std"] > 0.2:
                recommendations.append(f"{metric}ì˜ ì¼ê´€ì„± í–¥ìƒì´ í•„ìš”í•©ë‹ˆë‹¤ (í‘œì¤€í¸ì°¨: {stats['std']:.3f})")
        
        return recommendations
    
    async def _save_analysis_results(self, analysis_type: str, analysis: Dict, 
                                   output_path: Path) -> str:
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"analysis_{analysis_type}_{timestamp}.json"
        file_path = output_path / filename
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        return str(file_path)
    
    async def _generate_visualizations(self, analysis: Dict, output_path: Path) -> List[str]:
        """ì‹œê°í™” ìƒì„±"""
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        visualization_files = []
        
        try:
            # ë©”íŠ¸ë¦­ë³„ ì„±ëŠ¥ ë°”ì°¨íŠ¸
            metrics_stats = analysis.get("metrics_statistics", {})
            if metrics_stats:
                fig, ax = plt.subplots(figsize=(10, 6))
                
                metrics = list(metrics_stats.keys())
                means = [stats.get("mean", 0) for stats in metrics_stats.values()]
                
                bars = ax.bar(metrics, means)
                ax.set_ylabel('í‰ê·  ì ìˆ˜')
                ax.set_title('ë©”íŠ¸ë¦­ë³„ í‰ê·  ì„±ëŠ¥')
                ax.set_ylim(0, 1)
                
                # ì ìˆ˜ì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
                for bar, mean in zip(bars, means):
                    if mean >= 0.8:
                        bar.set_color('green')
                    elif mean >= 0.7:
                        bar.set_color('yellow')
                    else:
                        bar.set_color('red')
                
                chart_path = output_path / "metrics_performance_chart.png"
                plt.savefig(chart_path, dpi=300, bbox_inches='tight')
                visualization_files.append(str(chart_path))
                plt.close()
            
        except Exception as e:
            print(f"Failed to generate visualizations: {e}")
        
        return visualization_files
```

## êµ¬í˜„ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 3: í‰ê°€ ì‹œìŠ¤í…œ
- [ ] DeepEval ë³€í™˜ ëª¨ë“ˆ
- [ ] ì»¤ìŠ¤í…€ ë©”íŠ¸ë¦­ êµ¬í˜„
- [ ] ì €ì¥ì†Œë³„/ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ ë„êµ¬
- [ ] ëª¨ë¸ ê°„ ë¹„êµ ë¶„ì„ ë„êµ¬
- [ ] ì¢…í•© ë³´ê³ ì„œ ìƒì„± ê¸°ëŠ¥

### Phase 4: ìµœì í™” ë° í™•ì¥
- [ ] ì„±ëŠ¥ í”„ë¡œíŒŒì¼ë§
- [ ] ìºì‹± ì‹œìŠ¤í…œ êµ¬í˜„
- [ ] ì €ì¥ì†Œë³„ ë¡œê·¸ ë¶„ë¦¬
- [ ] ë³´ì•ˆ ì œì•½ ì‚¬í•­ ì²˜ë¦¬ (selvage-deprecated readonly)
- [ ] ì›¹ ëŒ€ì‹œë³´ë“œ (ì„ íƒì‚¬í•­)
- [ ] CI/CD í†µí•© (ì„ íƒì‚¬í•­)