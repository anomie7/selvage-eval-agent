"""ëª¨ë¸ ì„±ëŠ¥ ë¹„êµê¸°

ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ ë¹„êµí•˜ê³  ë¶„ì„í•©ë‹ˆë‹¤.
"""

import numpy as np
from typing import List, Dict, Any, Tuple
import logging
from scipy import stats

from .deepeval_log_parser import TestCaseResult
from .metric_aggregator import MetricAggregator

logger = logging.getLogger(__name__)


class ModelPerformanceComparator:
    """ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.significance_level = 0.05
        self.effect_size_thresholds = {
            'small': 0.2,
            'medium': 0.5,
            'large': 0.8
        }
        self.aggregator = MetricAggregator()
    
    def compare_models(self, model_results: Dict[str, List[TestCaseResult]]) -> Dict[str, Any]:
        """nê°œ ëª¨ë¸ ì¢…í•© ì„±ëŠ¥ ë¹„êµ ë¶„ì„
        
        Args:
            model_results: ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Dict: ëª¨ë¸ í†µê³„, ë¹„êµ í‘œ, ìˆœìœ„, í†µê³„ ë¶„ì„, ê¶Œì¥ì‚¬í•­ì„ í¬í•¨í•œ ì¢…í•© ê²°ê³¼
        """
        if not model_results:
            return {
                'model_count': 0,
                'model_statistics': {},
                'comparison_table': {},
                'rankings': {},
                'statistical_analysis': {},
                'recommendations': []
            }
        
        # ëª¨ë¸ë³„ ê¸°ë³¸ í†µê³„ ê³„ì‚°
        model_statistics = {}
        for model_name, results in model_results.items():
            if results:
                model_statistics[model_name] = self.aggregator.aggregate_model_performance(results)
            else:
                model_statistics[model_name] = self.aggregator.aggregate_model_performance([])
        
        # ìˆœìœ„ ê³„ì‚°
        rankings = self._calculate_model_rankings(model_statistics)
        
        # ë¹„êµ í‘œ ìƒì„±
        comparison_table = self._create_comparison_table(model_statistics, rankings)
        
        # í†µê³„ ë¶„ì„
        statistical_analysis = self._n_model_statistical_analysis(model_results)
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_model_recommendations(model_statistics, rankings)
        
        return {
            'model_count': len(model_results),
            'model_statistics': model_statistics,
            'comparison_table': comparison_table,
            'rankings': rankings,
            'statistical_analysis': statistical_analysis,
            'recommendations': recommendations
        }
    
    def _create_comparison_table(self, model_stats: Dict[str, Dict], 
                               rankings: Dict[str, Any]) -> Dict[str, Any]:
        """nê°œ ëª¨ë¸ ì¢…í•© ë¹„êµ í‘œ ìƒì„±
        
        Args:
            model_stats: ëª¨ë¸ë³„ í†µê³„ ë°ì´í„°
            rankings: ëª¨ë¸ ìˆœìœ„ ì •ë³´
            
        Returns:
            Dict: ë¹„êµ í‘œ ë°ì´í„°
        """
        comparison_data = []
        
        for model_name, stats in model_stats.items():
            if not stats or 'overall' not in stats:
                continue
                
            overall = stats['overall']
            
            # í‹°ì–´ ë¶„ë¥˜
            tier = self._classify_tier(overall['weighted_score'])
            
            model_data = {
                'model_name': model_name,
                'overall_score': round(overall['weighted_score'], 4),
                'overall_rank': rankings['overall_ranking'].get(model_name, len(model_stats)),
                'grade': overall['grade'],
                'tier': tier,
                'total_cases': overall['total_cases'],
                'overall_pass_rate': round(overall['pass_rate'], 4),
                'total_failures': overall['total_failures'],
                'consistency_score': round(overall['consistency'], 4)
            }
            
            # ë©”íŠ¸ë¦­ë³„ ìƒì„¸ ì ìˆ˜ ë° ìˆœìœ„ ì¶”ê°€
            for metric in ['correctness', 'clarity', 'actionability', 'json_correctness']:
                if metric in stats:
                    model_data[f'{metric}_score'] = round(stats[metric]['mean_score'], 4)
                    model_data[f'{metric}_rank'] = rankings['metric_rankings'][metric].get(model_name, len(model_stats))
                    model_data[f'{metric}_pass_rate'] = round(stats[metric]['pass_rate'], 4)
                else:
                    model_data[f'{metric}_score'] = 0.0
                    model_data[f'{metric}_rank'] = len(model_stats)
                    model_data[f'{metric}_pass_rate'] = 0.0
            
            comparison_data.append(model_data)
        
        # ì¢…í•© ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        comparison_data.sort(key=lambda x: x['overall_score'], reverse=True)
        
        return {
            'table_data': comparison_data,
            'metrics': ['correctness', 'clarity', 'actionability', 'json_correctness'],
            'tier_distribution': self._calculate_tier_distribution(comparison_data),
            'summary': self._generate_comparison_summary(comparison_data)
        }
    
    def _calculate_model_rankings(self, model_stats: Dict[str, Dict]) -> Dict[str, Any]:
        """ëª¨ë¸ ìˆœìœ„ ê³„ì‚°
        
        Args:
            model_stats: ëª¨ë¸ë³„ í†µê³„ ë°ì´í„°
            
        Returns:
            Dict: ìˆœìœ„ ì •ë³´ (ë©”íŠ¸ë¦­ë³„, ì¢…í•©)
        """
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        # ë©”íŠ¸ë¦­ë³„ ìˆœìœ„ ê³„ì‚°
        metric_rankings = {}
        for metric in metrics:
            metric_scores = []
            for model_name, stats in model_stats.items():
                if metric in stats:
                    metric_scores.append((model_name, stats[metric]['mean_score']))
                else:
                    metric_scores.append((model_name, 0.0))
            
            # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ì ìˆ˜ê°€ 1ìœ„)
            metric_scores.sort(key=lambda x: x[1], reverse=True)
            
            metric_rankings[metric] = {
                model_name: rank + 1 
                for rank, (model_name, score) in enumerate(metric_scores)
            }
        
        # ì¢…í•© ìˆœìœ„ ê³„ì‚°
        overall_scores = []
        for model_name, stats in model_stats.items():
            if 'overall' in stats:
                overall_scores.append((model_name, stats['overall']['weighted_score']))
            else:
                overall_scores.append((model_name, 0.0))
        
        overall_scores.sort(key=lambda x: x[1], reverse=True)
        overall_ranking = {
            model_name: rank + 1 
            for rank, (model_name, score) in enumerate(overall_scores)
        }
        
        return {
            'metric_rankings': metric_rankings,
            'overall_ranking': overall_ranking,
            'ranking_summary': self._generate_ranking_summary(metric_rankings, overall_ranking)
        }
    
    def _n_model_statistical_analysis(self, model_results: Dict[str, List[TestCaseResult]]) -> Dict[str, Any]:
        """ANOVA ë° Kruskal-Wallis ê²€ì • ìˆ˜í–‰
        
        Args:
            model_results: ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼
            
        Returns:
            Dict: í†µê³„ ë¶„ì„ ê²°ê³¼
        """
        if len(model_results) < 2:
            return {
                'analysis_performed': False,
                'reason': 'Insufficient models for statistical comparison (need at least 2 models)'
            }
        
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        analysis_results = {
            'analysis_performed': True,
            'model_count': len(model_results),
            'metric_analyses': {}
        }
        
        for metric in metrics:
            metric_data = []
            model_names = []
            
            # ê° ëª¨ë¸ë³„ ë©”íŠ¸ë¦­ ì ìˆ˜ ìˆ˜ì§‘
            for model_name, results in model_results.items():
                if results:
                    scores = [getattr(result, metric).score for result in results]
                    metric_data.append(scores)
                    model_names.append(model_name)
            
            if len(metric_data) < 2 or any(len(scores) == 0 for scores in metric_data):
                analysis_results['metric_analyses'][metric] = {
                    'test_performed': False,
                    'reason': 'Insufficient data for analysis'
                }
                continue
            
            # ANOVA ê²€ì •
            try:
                anova_f_stat, anova_p_value = stats.f_oneway(*metric_data)
                anova_significant = anova_p_value < self.significance_level
            except Exception as e:
                logger.warning(f"ANOVA ê²€ì • ì‹¤íŒ¨ ({metric}): {e}")
                anova_f_stat, anova_p_value, anova_significant = None, None, False
            
            # Kruskal-Wallis ê²€ì •
            try:
                kw_h_stat, kw_p_value = stats.kruskal(*metric_data)
                kw_significant = kw_p_value < self.significance_level
            except Exception as e:
                logger.warning(f"Kruskal-Wallis ê²€ì • ì‹¤íŒ¨ ({metric}): {e}")
                kw_h_stat, kw_p_value, kw_significant = None, None, False
            
            analysis_results['metric_analyses'][metric] = {
                'test_performed': True,
                'sample_sizes': [len(scores) for scores in metric_data],
                'model_names': model_names,
                'anova': {
                    'f_statistic': anova_f_stat,
                    'p_value': anova_p_value,
                    'significant': anova_significant,
                    'interpretation': 'Models show significant differences' if anova_significant else 'No significant differences between models'
                },
                'kruskal_wallis': {
                    'h_statistic': kw_h_stat,
                    'p_value': kw_p_value,
                    'significant': kw_significant,
                    'interpretation': 'Models show significant differences' if kw_significant else 'No significant differences between models'
                }
            }
        
        return analysis_results
    
    def _generate_model_recommendations(self, model_stats: Dict[str, Dict], 
                                       rankings: Dict[str, Any]) -> List[str]:
        """ëª¨ë¸ ì„ íƒ ê¶Œì¥ì‚¬í•­ ìƒì„±
        
        Args:
            model_stats: ëª¨ë¸ë³„ í†µê³„ ë°ì´í„°
            rankings: ìˆœìœ„ ì •ë³´
            
        Returns:
            List: ê¶Œì¥ì‚¬í•­ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸
        """
        recommendations = []
        
        if not model_stats or not rankings:
            return ["ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ì–´ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."]
        
        # ì „ì²´ ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        overall_ranking = rankings['overall_ranking']
        if overall_ranking:
            best_model = min(overall_ranking.items(), key=lambda x: x[1])
            best_model_name = best_model[0]
            best_score = model_stats[best_model_name]['overall']['weighted_score']
            
            recommendations.append(
                f"ğŸ† ì „ì²´ ìµœê³  ì„±ëŠ¥: {best_model_name} (ì ìˆ˜: {best_score:.3f}, ë“±ê¸‰: {model_stats[best_model_name]['overall']['grade']})"
            )
        
        # ë©”íŠ¸ë¦­ë³„ íŠ¹í™” ëª¨ë¸
        metric_rankings = rankings['metric_rankings']
        metric_names = {
            'correctness': 'ì •í™•ì„±',
            'clarity': 'ëª…í™•ì„±',
            'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
            'json_correctness': 'JSON ì •í™•ì„±'
        }
        
        for metric, korean_name in metric_names.items():
            if metric in metric_rankings:
                best_metric_model = min(metric_rankings[metric].items(), key=lambda x: x[1])
                best_metric_model_name = best_metric_model[0]
                best_metric_score = model_stats[best_metric_model_name][metric]['mean_score']
                
                recommendations.append(
                    f"ğŸ“Š {korean_name} ìµœê³ : {best_metric_model_name} (ì ìˆ˜: {best_metric_score:.3f})"
                )
        
        # ì¼ê´€ì„± ìµœê³  ëª¨ë¸
        consistency_scores = [
            (model_name, stats['overall']['consistency'])
            for model_name, stats in model_stats.items()
            if 'overall' in stats
        ]
        
        if consistency_scores:
            most_consistent = max(consistency_scores, key=lambda x: x[1])
            recommendations.append(
                f"ğŸ¯ ì¼ê´€ì„± ìµœê³ : {most_consistent[0]} (ì¼ê´€ì„±: {most_consistent[1]:.3f})"
            )
        
        # ì„±ëŠ¥ í‹°ì–´ ë¶„ì„
        tier_counts = {}
        for model_name, stats in model_stats.items():
            if 'overall' in stats:
                tier = self._classify_tier(stats['overall']['weighted_score'])
                tier_counts[tier] = tier_counts.get(tier, 0) + 1
        
        if tier_counts:
            recommendations.append(
                f"ğŸ“ˆ ì„±ëŠ¥ ë¶„í¬: " + ", ".join([f"{tier} {count}ê°œ" for tier, count in tier_counts.items()])
            )
        
        return recommendations
    
    def _classify_tier(self, score: float) -> str:
        """ì ìˆ˜ ê¸°ë°˜ ì„±ëŠ¥ í‹°ì–´ ë¶„ë¥˜
        
        Args:
            score: ì„±ëŠ¥ ì ìˆ˜
            
        Returns:
            str: ì„±ëŠ¥ í‹°ì–´
        """
        if score >= 0.85:
            return "Tier 1 (ìš°ìˆ˜)"
        elif score >= 0.75:
            return "Tier 2 (ì–‘í˜¸)"
        elif score >= 0.65:
            return "Tier 3 (ë³´í†µ)"
        else:
            return "Tier 4 (ê°œì„ í•„ìš”)"
    
    def _calculate_tier_distribution(self, comparison_data: List[Dict]) -> Dict[str, int]:
        """í‹°ì–´ ë¶„í¬ ê³„ì‚°
        
        Args:
            comparison_data: ë¹„êµ í…Œì´ë¸” ë°ì´í„°
            
        Returns:
            Dict: í‹°ì–´ë³„ ëª¨ë¸ ìˆ˜
        """
        tier_distribution = {}
        for model_data in comparison_data:
            tier = model_data['tier']
            tier_distribution[tier] = tier_distribution.get(tier, 0) + 1
        
        return tier_distribution
    
    def _generate_comparison_summary(self, comparison_data: List[Dict]) -> Dict[str, Any]:
        """ë¹„êµ ìš”ì•½ ì •ë³´ ìƒì„±
        
        Args:
            comparison_data: ë¹„êµ í…Œì´ë¸” ë°ì´í„°
            
        Returns:
            Dict: ìš”ì•½ ì •ë³´
        """
        if not comparison_data:
            return {}
        
        scores = [data['overall_score'] for data in comparison_data]
        
        return {
            'total_models': len(comparison_data),
            'score_statistics': {
                'mean': round(np.mean(scores), 4),
                'std': round(np.std(scores), 4),
                'min': round(np.min(scores), 4),
                'max': round(np.max(scores), 4),
                'median': round(np.median(scores), 4)
            },
            'best_model': comparison_data[0]['model_name'],
            'worst_model': comparison_data[-1]['model_name'],
            'score_gap': round(comparison_data[0]['overall_score'] - comparison_data[-1]['overall_score'], 4)
        }
    
    def _generate_ranking_summary(self, metric_rankings: Dict[str, Dict], 
                                 overall_ranking: Dict[str, int]) -> Dict[str, Any]:
        """ìˆœìœ„ ìš”ì•½ ì •ë³´ ìƒì„±
        
        Args:
            metric_rankings: ë©”íŠ¸ë¦­ë³„ ìˆœìœ„
            overall_ranking: ì¢…í•© ìˆœìœ„
            
        Returns:
            Dict: ìˆœìœ„ ìš”ì•½ ì •ë³´
        """
        summary = {
            'total_models': len(overall_ranking),
            'top_model': min(overall_ranking.items(), key=lambda x: x[1])[0] if overall_ranking else None,
            'metric_leaders': {}
        }
        
        # ë©”íŠ¸ë¦­ë³„ 1ìœ„ ëª¨ë¸
        for metric, rankings in metric_rankings.items():
            if rankings:
                leader = min(rankings.items(), key=lambda x: x[1])[0]
                summary['metric_leaders'][metric] = leader
        
        return summary