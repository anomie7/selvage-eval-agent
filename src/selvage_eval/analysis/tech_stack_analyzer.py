"""ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ê¸°

ì €ì¥ì†Œë³„ë¡œ ë‹¤ë¥¸ ê¸°ìˆ ìŠ¤íƒì—ì„œì˜ ëª¨ë¸ ì„±ëŠ¥ì„ ë¶„ì„í•˜ê³  ë¹„êµí•©ë‹ˆë‹¤.
"""

import numpy as np
from typing import List, Dict, Any, Optional
import logging

from .deepeval_log_parser import TestCaseResult
from .metric_aggregator import MetricAggregator
from ..config.settings import get_tech_stack_mapping

logger = logging.getLogger(__name__)


class TechStackAnalyzer:
    """ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„ê¸°"""
    
    def __init__(self, config_path: Optional[str] = None):
        # ì„¤ì • íŒŒì¼ì—ì„œ ê¸°ìˆ ìŠ¤íƒ ë§¤í•‘ ë¡œë“œ
        self.tech_stack_mapping = get_tech_stack_mapping(config_path)
        self.aggregator = MetricAggregator()
    
    def analyze_tech_stack_performance(self, 
                                     repo_results: Dict[str, Dict[str, List[TestCaseResult]]]) -> Dict[str, Any]:
        """ì €ì¥ì†Œ/ê¸°ìˆ ìŠ¤íƒë³„ ëª¨ë¸ ì„±ëŠ¥ ë¶„ì„
        
        Args:
            repo_results: ì €ì¥ì†Œë³„ ëª¨ë¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
                         {repository: {model_name: [TestCaseResult, ...]}}
            
        Returns:
            Dict: ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ê¸°ìˆ ìŠ¤íƒ ì„±ëŠ¥ ë¶„ì„ ì‹œì‘ - {len(repo_results)}ê°œ ì €ì¥ì†Œ ë¶„ì„")
        
        if not repo_results:
            logger.info("ë¶„ì„í•  ì €ì¥ì†Œ ë°ì´í„°ê°€ ì—†ìŒ")
            return {
                'by_tech_stack': {},
                'cross_stack_comparison': {},
                'recommendations': []
            }
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„
        tech_stack_analysis = {}
        
        for repo_idx, (repository, model_results) in enumerate(repo_results.items(), 1):
            logger.info(f"ì €ì¥ì†Œ {repo_idx}/{len(repo_results)} '{repository}' ë¶„ì„ ì¤‘...")
            # ì €ì¥ì†Œëª…ì„ ê¸°ìˆ ìŠ¤íƒìœ¼ë¡œ ë§¤í•‘
            tech_stack = self.tech_stack_mapping.get(repository, repository)
            logger.debug(f"ì €ì¥ì†Œ '{repository}' -> ê¸°ìˆ ìŠ¤íƒ '{tech_stack}' ë§¤í•‘")
            
            if not model_results:
                logger.warning(f"ì €ì¥ì†Œ '{repository}'ì— ëª¨ë¸ ê²°ê³¼ê°€ ì—†ìŒ")
                continue
            
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ê³„ì‚°
            logger.debug(f"ê¸°ìˆ ìŠ¤íƒ '{tech_stack}' - {len(model_results)}ê°œ ëª¨ë¸ ì„±ëŠ¥ ê³„ì‚° ì¤‘...")
            model_performance = {}
            for model_name, test_results in model_results.items():
                logger.debug(f"ëª¨ë¸ '{model_name}' ì„±ëŠ¥ ê³„ì‚° ì¤‘ ({len(test_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤)...")
                if test_results:
                    model_performance[model_name] = self.aggregator.aggregate_model_performance(test_results)
                else:
                    model_performance[model_name] = self.aggregator.aggregate_model_performance([])
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„
            logger.debug(f"ê¸°ìˆ ìŠ¤íƒ '{tech_stack}' ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì‹ë³„ ì¤‘...")
            best_model = self._find_best_model(model_performance)
            
            # ì„±ëŠ¥ ê²©ì°¨ ê³„ì‚°
            logger.debug(f"ê¸°ìˆ ìŠ¤íƒ '{tech_stack}' ì„±ëŠ¥ ê²©ì°¨ ê³„ì‚° ì¤‘...")
            performance_gap = self._calculate_performance_gap(model_performance)
            
            # ê¸°ìˆ ìŠ¤íƒë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±
            logger.debug(f"ê¸°ìˆ ìŠ¤íƒ '{tech_stack}' ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
            recommendations = self._generate_tech_stack_recommendations(tech_stack, {
                'model_performance': model_performance,
                'best_model': best_model,
                'performance_gap': performance_gap
            })
            
            tech_stack_analysis[tech_stack] = {
                'repository': repository,
                'model_performance': model_performance,
                'best_model': best_model,
                'performance_gap': performance_gap,
                'recommendations': recommendations,
                'model_count': len(model_performance)
            }
            
            if best_model and best_model['name']:
                logger.info(f"ì €ì¥ì†Œ '{repository}' ({tech_stack}) ë¶„ì„ ì™„ë£Œ - ìµœê³  ëª¨ë¸: {best_model['name']} (ì ìˆ˜: {best_model['score']:.3f})")
            else:
                logger.info(f"ì €ì¥ì†Œ '{repository}' ({tech_stack}) ë¶„ì„ ì™„ë£Œ - ìœ íš¨í•œ ëª¨ë¸ ì—†ìŒ")
        
        # ê¸°ìˆ ìŠ¤íƒ ê°„ êµì°¨ ë¹„êµ
        logger.info("ê¸°ìˆ ìŠ¤íƒ ê°„ êµì°¨ ë¹„êµ ë¶„ì„ ì¤‘...")
        cross_stack_comparison = self._cross_stack_comparison(tech_stack_analysis)
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­ ìƒì„±
        logger.info("ì „ì²´ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
        overall_recommendations = self._generate_overall_recommendations(tech_stack_analysis, cross_stack_comparison)
        
        logger.info(f"ê¸°ìˆ ìŠ¤íƒ ì„±ëŠ¥ ë¶„ì„ ì™„ë£Œ - {len(tech_stack_analysis)}ê°œ ê¸°ìˆ ìŠ¤íƒ ë¶„ì„ë¨")
        
        return {
            'by_tech_stack': tech_stack_analysis,
            'cross_stack_comparison': cross_stack_comparison,
            'recommendations': overall_recommendations
        }
    
    def _find_best_model(self, model_performance: Dict[str, Dict]) -> Dict[str, Any]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        
        Args:
            model_performance: ëª¨ë¸ë³„ ì„±ëŠ¥ ë°ì´í„°
            
        Returns:
            Dict: ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì •ë³´
        """
        if not model_performance:
            return {
                'name': None,
                'score': 0.0,
                'grade': 'F',
                'metrics': {}
            }
        
        best_model = None
        best_score = -1
        
        for model_name, performance in model_performance.items():
            if 'overall' in performance:
                score = performance['overall']['weighted_score']
                if score > best_score:
                    best_score = score
                    best_model = model_name
        
        if best_model:
            best_performance = model_performance[best_model]
            return {
                'name': best_model,
                'score': best_score,
                'grade': best_performance['overall']['grade'],
                'metrics': {
                    metric: best_performance[metric]['mean_score']
                    for metric in ['correctness', 'clarity', 'actionability', 'json_correctness']
                    if metric in best_performance
                }
            }
        
        return {
            'name': None,
            'score': 0.0,
            'grade': 'F',
            'metrics': {}
        }
    
    def _calculate_performance_gap(self, model_performance: Dict[str, Dict]) -> Dict[str, float]:
        """ì„±ëŠ¥ ê²©ì°¨ ê³„ì‚°
        
        Args:
            model_performance: ëª¨ë¸ë³„ ì„±ëŠ¥ ë°ì´í„°
            
        Returns:
            Dict: ì„±ëŠ¥ ê²©ì°¨ ì •ë³´
        """
        if not model_performance:
            return {
                'max_score': 0.0,
                'min_score': 0.0,
                'gap': 0.0,
                'coefficient_of_variation': 0.0
            }
        
        # ì „ì²´ ì ìˆ˜ ìˆ˜ì§‘
        scores = []
        for model_name, performance in model_performance.items():
            if 'overall' in performance:
                scores.append(performance['overall']['weighted_score'])
        
        if not scores:
            return {
                'max_score': 0.0,
                'min_score': 0.0,
                'gap': 0.0,
                'coefficient_of_variation': 0.0
            }
        
        max_score = max(scores)
        min_score = min(scores)
        gap = max_score - min_score
        
        # ë³€ë™ê³„ìˆ˜ ê³„ì‚°
        mean_score = np.mean(scores)
        std_score = np.std(scores)
        coefficient_of_variation = std_score / mean_score if mean_score > 0 else 0.0
        
        return {
            'max_score': float(max_score),
            'min_score': float(min_score),
            'gap': float(gap),
            'coefficient_of_variation': float(coefficient_of_variation)
        }
    
    def _generate_tech_stack_recommendations(self, tech_stack: str, 
                                           performance: Dict[str, Any]) -> List[str]:
        """ê¸°ìˆ ìŠ¤íƒë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±
        
        Args:
            tech_stack: ê¸°ìˆ ìŠ¤íƒëª…
            performance: ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
            
        Returns:
            List: ê¶Œì¥ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        recommendations = []
        
        best_model = performance.get('best_model')
        performance_gap = performance.get('performance_gap', {})
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì¶”ì²œ
        if best_model and best_model['name']:
            recommendations.append(
                f"ğŸ¥‡ {tech_stack} ìµœì  ëª¨ë¸: {best_model['name']} "
                f"(ì ìˆ˜: {best_model['score']:.3f}, ë“±ê¸‰: {best_model['grade']})"
            )
        
        # ì„±ëŠ¥ ê²©ì°¨ ë¶„ì„
        gap = performance_gap.get('gap', 0.0)
        if gap > 0.2:
            recommendations.append(
                f"âš ï¸ ëª¨ë¸ ê°„ ì„±ëŠ¥ ê²©ì°¨ê°€ í¼ (ê²©ì°¨: {gap:.3f}) - ëª¨ë¸ ì„ íƒì´ ì¤‘ìš”"
            )
        elif gap < 0.1:
            recommendations.append(
                f"âœ… ëª¨ë¸ ê°„ ì„±ëŠ¥ì´ ê· ë“±í•¨ (ê²©ì°¨: {gap:.3f}) - ì•ˆì •ì ì¸ ì„ íƒ ê°€ëŠ¥"
            )
        
        # ë©”íŠ¸ë¦­ë³„ íŠ¹í™” ë¶„ì„
        if best_model and best_model['metrics']:
            metrics = best_model['metrics']
            metric_names = {
                'correctness': 'ì •í™•ì„±',
                'clarity': 'ëª…í™•ì„±',
                'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                'json_correctness': 'JSON ì •í™•ì„±'
            }
            
            # ì•½í•œ ë©”íŠ¸ë¦­ ì‹ë³„
            weak_metrics = [
                korean_name for metric, korean_name in metric_names.items()
                if metric in metrics and metrics[metric] < 0.7
            ]
            
            if weak_metrics:
                recommendations.append(
                    f"ğŸ“ˆ {tech_stack}ì—ì„œ ê°œì„  í•„ìš” ì˜ì—­: {', '.join(weak_metrics)}"
                )
        
        # ë³€ë™ê³„ìˆ˜ ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„
        cv = performance_gap.get('coefficient_of_variation', 0.0)
        if cv > 0.3:
            recommendations.append(
                f"ğŸ“Š ì„±ëŠ¥ ë³€ë™ì„±ì´ ë†’ìŒ (CV: {cv:.3f}) - ëª¨ë¸ ì„ íƒ ì‹œ ì‹ ì¤‘ ê²€í†  í•„ìš”"
            )
        elif cv < 0.1:
            recommendations.append(
                f"ğŸ¯ ì„±ëŠ¥ ì¼ê´€ì„±ì´ ìš°ìˆ˜í•¨ (CV: {cv:.3f}) - ì•ˆì •ì ì¸ ê¸°ìˆ ìŠ¤íƒ"
            )
        
        return recommendations
    
    def _cross_stack_comparison(self, tech_stack_analysis: Dict[str, Dict]) -> Dict[str, Any]:
        """ê¸°ìˆ ìŠ¤íƒ ê°„ êµì°¨ ë¹„êµ
        
        Args:
            tech_stack_analysis: ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ ê²°ê³¼
            
        Returns:
            Dict: êµì°¨ ë¹„êµ ê²°ê³¼
        """
        if len(tech_stack_analysis) < 2:
            return {
                'comparison_performed': False,
                'reason': 'Need at least 2 tech stacks for comparison'
            }
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ìµœê³  ì ìˆ˜ ìˆ˜ì§‘
        stack_scores = {}
        stack_best_models = {}
        
        for tech_stack, analysis in tech_stack_analysis.items():
            best_model = analysis.get('best_model')
            if best_model and best_model['name']:
                stack_scores[tech_stack] = best_model['score']
                stack_best_models[tech_stack] = best_model['name']
        
        if not stack_scores:
            return {
                'comparison_performed': False,
                'reason': 'No valid performance data for comparison'
            }
        
        # ìˆœìœ„ ìƒì„±
        ranked_stacks = sorted(stack_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ë©”íŠ¸ë¦­ë³„ ë¹„êµ
        metric_comparison = self._compare_metrics_across_stacks(tech_stack_analysis)
        
        return {
            'comparison_performed': True,
            'stack_ranking': [
                {
                    'rank': i + 1,
                    'tech_stack': tech_stack,
                    'score': score,
                    'best_model': stack_best_models.get(tech_stack, 'Unknown')
                }
                for i, (tech_stack, score) in enumerate(ranked_stacks)
            ],
            'metric_comparison': metric_comparison,
            'performance_summary': {
                'best_stack': ranked_stacks[0][0] if ranked_stacks else None,
                'worst_stack': ranked_stacks[-1][0] if ranked_stacks else None,
                'score_range': {
                    'max': max(stack_scores.values()) if stack_scores else 0.0,
                    'min': min(stack_scores.values()) if stack_scores else 0.0,
                    'gap': max(stack_scores.values()) - min(stack_scores.values()) if stack_scores else 0.0
                }
            }
        }
    
    def _compare_metrics_across_stacks(self, tech_stack_analysis: Dict[str, Dict]) -> Dict[str, Dict]:
        """ë©”íŠ¸ë¦­ë³„ ê¸°ìˆ ìŠ¤íƒ ê°„ ë¹„êµ
        
        Args:
            tech_stack_analysis: ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ ê²°ê³¼
            
        Returns:
            Dict: ë©”íŠ¸ë¦­ë³„ ë¹„êµ ê²°ê³¼
        """
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        metric_comparison = {}
        
        for metric in metrics:
            stack_metric_scores = {}
            
            for tech_stack, analysis in tech_stack_analysis.items():
                best_model = analysis.get('best_model')
                if best_model and best_model['metrics'] and metric in best_model['metrics']:
                    stack_metric_scores[tech_stack] = best_model['metrics'][metric]
            
            if stack_metric_scores:
                # ë©”íŠ¸ë¦­ë³„ ìˆœìœ„
                ranked_stacks = sorted(stack_metric_scores.items(), key=lambda x: x[1], reverse=True)
                
                metric_comparison[metric] = {
                    'rankings': [
                        {
                            'rank': i + 1,
                            'tech_stack': tech_stack,
                            'score': score
                        }
                        for i, (tech_stack, score) in enumerate(ranked_stacks)
                    ],
                    'best_stack': ranked_stacks[0][0] if ranked_stacks else None,
                    'worst_stack': ranked_stacks[-1][0] if ranked_stacks else None,
                    'score_statistics': {
                        'mean': float(np.mean(list(stack_metric_scores.values()))),
                        'std': float(np.std(list(stack_metric_scores.values()))),
                        'max': float(max(stack_metric_scores.values())),
                        'min': float(min(stack_metric_scores.values()))
                    }
                }
        
        return metric_comparison
    
    def _generate_overall_recommendations(self, tech_stack_analysis: Dict[str, Dict], 
                                        cross_stack_comparison: Dict[str, Any]) -> List[str]:
        """ì „ì²´ ê¶Œì¥ì‚¬í•­ ìƒì„±
        
        Args:
            tech_stack_analysis: ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ ê²°ê³¼
            cross_stack_comparison: êµì°¨ ë¹„êµ ê²°ê³¼
            
        Returns:
            List: ì „ì²´ ê¶Œì¥ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        recommendations = []
        
        if not tech_stack_analysis:
            return ["ë¶„ì„í•  ê¸°ìˆ ìŠ¤íƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."]
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ìš”ì•½
        recommendations.append(f"ğŸ“Š ë¶„ì„ëœ ê¸°ìˆ ìŠ¤íƒ: {len(tech_stack_analysis)}ê°œ")
        
        # êµì°¨ ë¹„êµ ê²°ê³¼
        if cross_stack_comparison.get('comparison_performed'):
            performance_summary = cross_stack_comparison.get('performance_summary', {})
            best_stack = performance_summary.get('best_stack')
            worst_stack = performance_summary.get('worst_stack')
            
            if best_stack and worst_stack:
                recommendations.append(
                    f"ğŸ† ìµœê³  ì„±ëŠ¥ ê¸°ìˆ ìŠ¤íƒ: {best_stack}"
                )
                recommendations.append(
                    f"ğŸ“ˆ ê°œì„  í•„ìš” ê¸°ìˆ ìŠ¤íƒ: {worst_stack}"
                )
                
                score_range = performance_summary.get('score_range', {})
                gap = score_range.get('gap', 0.0)
                if gap > 0.2:
                    recommendations.append(
                        f"âš ï¸ ê¸°ìˆ ìŠ¤íƒ ê°„ ì„±ëŠ¥ ê²©ì°¨ê°€ í¼ (ê²©ì°¨: {gap:.3f})"
                    )
        
        # ë©”íŠ¸ë¦­ë³„ íŠ¹í™” ê¶Œì¥ì‚¬í•­
        if cross_stack_comparison.get('metric_comparison'):
            metric_comparison = cross_stack_comparison['metric_comparison']
            metric_names = {
                'correctness': 'ì •í™•ì„±',
                'clarity': 'ëª…í™•ì„±',
                'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                'json_correctness': 'JSON ì •í™•ì„±'
            }
            
            for metric, korean_name in metric_names.items():
                if metric in metric_comparison:
                    best_stack = metric_comparison[metric].get('best_stack')
                    if best_stack:
                        recommendations.append(
                            f"ğŸ“ˆ {korean_name} ìµœê³  ê¸°ìˆ ìŠ¤íƒ: {best_stack}"
                        )
        
        # ê°œë³„ ê¸°ìˆ ìŠ¤íƒ ê¶Œì¥ì‚¬í•­ ìˆ˜ì§‘
        for tech_stack, analysis in tech_stack_analysis.items():
            stack_recommendations = analysis.get('recommendations', [])
            if stack_recommendations:
                recommendations.append(f"\n[{tech_stack}]")
                recommendations.extend(stack_recommendations)
        
        return recommendations