"""ë²„ì „ ë¹„êµ ë¶„ì„ê¸°

Selvage ë²„ì „ë³„ ì„±ëŠ¥ ë³€í™” ë¶„ì„ ë° ì¶”ì ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
import logging
import numpy as np
from sklearn.linear_model import LinearRegression

from .deepeval_log_parser import DeepEvalLogParser, TestCaseResult
from .metric_aggregator import MetricAggregator

logger = logging.getLogger(__name__)


class VersionComparisonAnalyzer:
    """ë²„ì „ ë¹„êµ ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.version_pattern = re.compile(r'selvage\s+(\d+\.\d+\.\d+)')
        self.parser = DeepEvalLogParser()
        self.aggregator = MetricAggregator()
        
        # ì„ê³„ê°’ ì„¤ì •
        self.regression_threshold = 0.05  # 5% ì„±ëŠ¥ ì €í•˜
        self.improvement_threshold = 0.03  # 3% ì„±ëŠ¥ ê°œì„ 
        self.excellent_threshold = 0.8     # ìš°ìˆ˜ ì„±ëŠ¥ 80%
        self.needs_improvement_threshold = 0.7  # ê°œì„  í•„ìš” 70%
    
    def collect_version_data(self, base_path: str) -> Dict[str, Any]:
        """ë²„ì „ë³„ í‰ê°€ ë°ì´í„° ìˆ˜ì§‘
        
        Args:
            base_path: í‰ê°€ ì„¸ì…˜ë“¤ì´ ì €ì¥ëœ ê¸°ë³¸ ê²½ë¡œ
            
        Returns:
            Dict: ë²„ì „ë³„ ìˆ˜ì§‘ëœ ë°ì´í„°
        """
        logger.info(f"ë²„ì „ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ - ê¸°ë³¸ ê²½ë¡œ: {base_path}")
        
        base_dir = Path(base_path)
        if not base_dir.exists():
            logger.warning(f"Base path does not exist: {base_dir}")
            return {}
        
        version_data: Dict[str, Any] = {}
        session_dirs = list(base_dir.glob('*/'))
        logger.info(f"ìŠ¤ìº”í•  ì„¸ì…˜ ë””ë ‰í† ë¦¬ {len(session_dirs)}ê°œ ë°œê²¬")
        
        # ëª¨ë“  í‰ê°€ ì„¸ì…˜ ìŠ¤ìº”
        for session_idx, session_dir in enumerate(session_dirs, 1):
            logger.debug(f"ì„¸ì…˜ {session_idx}/{len(session_dirs)} ì²˜ë¦¬ ì¤‘: {session_dir.name}")
            if not session_dir.is_dir():
                continue
            
            try:
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
                metadata_path = session_dir / 'metadata.json'
                if not metadata_path.exists():
                    logger.debug(f"No metadata.json in {session_dir}")
                    continue
                
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # ë²„ì „ ì •ë³´ ì¶”ì¶œ
                version = self._extract_version_from_metadata(metadata)
                if not version:
                    logger.debug(f"No version info in {session_dir}")
                    continue
                
                logger.debug(f"ì„¸ì…˜ '{session_dir.name}'ì—ì„œ ë²„ì „ '{version}' ê°ì§€")
                
                # ì„¸ì…˜ ê²°ê³¼ ìˆ˜ì§‘
                session_results = self._collect_session_results(session_dir)
                if not session_results:
                    logger.debug(f"No results in {session_dir}")
                    continue
                
                logger.debug(f"ì„¸ì…˜ '{session_dir.name}'ì—ì„œ {len(session_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜ì§‘")
                
                # ì‹¤í–‰ ë‚ ì§œ ì¶”ì¶œ
                execution_date = self._extract_execution_date(metadata)
                
                if version not in version_data:
                    sessions_list: List[Dict[str, Any]] = []
                    dates_list: List[Optional[datetime]] = []
                    version_data[version] = {
                        'version': version,
                        'sessions': sessions_list,
                        'execution_dates': dates_list,
                        'latest_execution_date': None
                    }
                    logger.debug(f"ìƒˆ ë²„ì „ '{version}' ì¶”ê°€")
                
                version_data[version]['sessions'].append({
                    'session_dir': str(session_dir),
                    'results': session_results,
                    'execution_date': execution_date
                })
                
                version_data[version]['execution_dates'].append(execution_date)
                
            except Exception as e:
                logger.error(f"Error processing session {session_dir}: {e}")
                continue
        
        # ê° ë²„ì „ë³„ë¡œ í‰ê·  ì‹¤í–‰ ë‚ ì§œ ê³„ì‚°
        for version, data in version_data.items():
            dates = [d for d in data['execution_dates'] if d]
            if dates:
                # ê°€ì¥ ìµœê·¼ ì‹¤í–‰ ë‚ ì§œ ì‚¬ìš©
                data['latest_execution_date'] = max(dates)
                logger.debug(f"ë²„ì „ '{version}' ìµœì‹  ì‹¤í–‰ ë‚ ì§œ: {data['latest_execution_date']}")
            else:
                data['latest_execution_date'] = None
                logger.debug(f"ë²„ì „ '{version}' ì‹¤í–‰ ë‚ ì§œ ì •ë³´ ì—†ìŒ")
        
        total_sessions = sum(len(data['sessions']) for data in version_data.values())
        logger.info(f"ë²„ì „ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ - {len(version_data)}ê°œ ë²„ì „, ì´ {total_sessions}ê°œ ì„¸ì…˜")
        
        return version_data
    
    def _extract_version_from_metadata(self, metadata: Dict) -> Optional[str]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ
        
        Args:
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Optional[str]: ì¶”ì¶œëœ ë²„ì „ ì •ë³´
        """
        # ë‹¤ì–‘í•œ ìœ„ì¹˜ì—ì„œ ë²„ì „ ì •ë³´ íƒìƒ‰
        version_fields = [
            'selvage_version',
            'version',
            'tool_version',
            'selvage_tool_version'
        ]
        
        for field in version_fields:
            if field in metadata:
                version_str = str(metadata[field])
                match = self.version_pattern.search(version_str)
                if match:
                    return match.group(1)
        
        # ëª…ë ¹ì–´ë‚˜ ì„¤ì •ì—ì„œ ë²„ì „ ì •ë³´ íƒìƒ‰
        if 'command' in metadata:
            command_str = str(metadata['command'])
            match = self.version_pattern.search(command_str)
            if match:
                return match.group(1)
        
        return None
    
    def _extract_execution_date(self, metadata: Dict) -> Optional[datetime]:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ì‹¤í–‰ ë‚ ì§œ ì¶”ì¶œ
        
        Args:
            metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
            
        Returns:
            Optional[datetime]: ì¶”ì¶œëœ ì‹¤í–‰ ë‚ ì§œ
        """
        date_fields = [
            'execution_date',
            'timestamp',
            'created_at',
            'start_time'
        ]
        
        for field in date_fields:
            if field in metadata:
                try:
                    date_str = str(metadata[field])
                    # ISO í˜•ì‹ ì‹œë„
                    return datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except ValueError:
                    try:
                        # ë‹¤ë¥¸ í˜•ì‹ë“¤ ì‹œë„
                        return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
                    except ValueError:
                        continue
        
        return None
    
    def _collect_session_results(self, session_dir: Path) -> List[TestCaseResult]:
        """ì„¸ì…˜ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
        
        Args:
            session_dir: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            List[TestCaseResult]: ìˆ˜ì§‘ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ê²°ê³¼
        """
        results = []
        
        # ë¡œê·¸ íŒŒì¼ íƒìƒ‰
        for log_file in session_dir.glob('**/*.log'):
            try:
                file_results = list(self.parser.parse_log_file(log_file))
                results.extend(file_results)
            except Exception as e:
                logger.warning(f"Error parsing log file {log_file}: {e}")
                continue
        
        return results
    
    def analyze_version_progression(self, version_data: Dict[str, Any]) -> Dict[str, Any]:
        """ë²„ì „ë³„ ì„±ëŠ¥ ë°œì „ ë¶„ì„
        
        Args:
            version_data: ë²„ì „ë³„ ë°ì´í„°
            
        Returns:
            Dict: ë²„ì „ ë°œì „ ë¶„ì„ ê²°ê³¼
        """
        logger.info(f"ë²„ì „ ë°œì „ ë¶„ì„ ì‹œì‘ - {len(version_data)}ê°œ ë²„ì „ ë¶„ì„")
        
        if not version_data:
            logger.info("ë¶„ì„í•  ë²„ì „ ë°ì´í„°ê°€ ì—†ìŒ")
            return {
                'version_timeline': [],
                'performance_trends': {},
                'regression_analysis': {},
                'improvement_highlights': [],
                'version_recommendations': {}
            }
        
        # ë²„ì „ë³„ ì¢…í•© ì„±ëŠ¥ ê³„ì‚°
        logger.info("ë²„ì „ë³„ ì¢…í•© ì„±ëŠ¥ ê³„ì‚° ì¤‘...")
        version_performance = {}
        for version_idx, (version, data) in enumerate(version_data.items(), 1):
            logger.debug(f"ë²„ì „ {version_idx}/{len(version_data)} '{version}' ì„±ëŠ¥ ê³„ì‚° ì¤‘...")
            
            all_results = []
            for session in data['sessions']:
                all_results.extend(session['results'])
            
            if all_results:
                logger.debug(f"ë²„ì „ '{version}' - {len(all_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ì„±ëŠ¥ ê³„ì‚°")
                performance = self.aggregator.aggregate_model_performance(all_results)
                version_performance[version] = {
                    'version': version,
                    'performance': performance,
                    'latest_execution_date': data['latest_execution_date'],
                    'total_test_cases': len(all_results)
                }
                
                overall_score = performance.get('overall', {}).get('weighted_score', 0.0)
                logger.debug(f"ë²„ì „ '{version}' ì„±ëŠ¥ ê³„ì‚° ì™„ë£Œ - ì „ì²´ ì ìˆ˜: {overall_score:.3f}")
            else:
                logger.warning(f"ë²„ì „ '{version}'ì— ìœ íš¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ê°€ ì—†ìŒ")
        
        # ì‹œê°„ìˆœ ì •ë ¬
        logger.info("ë²„ì „ì„ ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ ì¤‘...")
        sorted_versions = self._sort_versions_by_date(version_performance)
        logger.info(f"ì‹œê°„ìˆœ ì •ë ¬ ì™„ë£Œ - {len(sorted_versions)}ê°œ ë²„ì „")
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        logger.info("ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ ì¤‘...")
        performance_trends = self._analyze_performance_trends(sorted_versions)
        
        # íšŒê·€ ë¶„ì„
        logger.info("ì„±ëŠ¥ íšŒê·€ ë¶„ì„ ì¤‘...")
        regression_analysis = self._detect_regressions(sorted_versions)
        
        # ê°œì„  ì‚¬í•­ ì‹ë³„
        logger.info("ì„±ëŠ¥ ê°œì„ ì‚¬í•­ ì‹ë³„ ì¤‘...")
        improvement_highlights = self._identify_improvements(sorted_versions)
        
        # ë²„ì „ ê¶Œì¥ì‚¬í•­ ìƒì„±
        logger.info("ë²„ì „ ê¶Œì¥ì‚¬í•­ ìƒì„± ì¤‘...")
        version_recommendations = self._generate_version_recommendations(
            sorted_versions, {
                'performance_trends': performance_trends,
                'regression_analysis': regression_analysis,
                'improvement_highlights': improvement_highlights
            }
        )
        
        logger.info(f"ë²„ì „ ë°œì „ ë¶„ì„ ì™„ë£Œ - {len(improvement_highlights)}ê°œ ê°œì„ ì‚¬í•­, {regression_analysis.get('total_regressions', 0)}ê°œ íšŒê·€ ë°œê²¬")
        
        return {
            'version_timeline': sorted_versions,
            'performance_trends': performance_trends,
            'regression_analysis': regression_analysis,
            'improvement_highlights': improvement_highlights,
            'version_recommendations': version_recommendations
        }
    
    def _sort_versions_by_date(self, version_performance: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ë²„ì „ì„ ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬
        
        Args:
            version_performance: ë²„ì „ë³„ ì„±ëŠ¥ ë°ì´í„°
            
        Returns:
            List: ë‚ ì§œìˆœìœ¼ë¡œ ì •ë ¬ëœ ë²„ì „ ë¦¬ìŠ¤íŠ¸
        """
        versions_with_dates = []
        
        for version, data in version_performance.items():
            execution_date = data['latest_execution_date']
            if execution_date:
                versions_with_dates.append((execution_date, data))
            else:
                # ë‚ ì§œê°€ ì—†ëŠ” ê²½ìš° ë²„ì „ ë²ˆí˜¸ë¡œ ì •ë ¬
                version_tuple = tuple(map(int, version.split('.')))
                # ì„ì˜ì˜ ì˜¤ë˜ëœ ë‚ ì§œ í• ë‹¹
                fake_date = datetime(2020, 1, 1)
                versions_with_dates.append((fake_date, data))
        
        # ë‚ ì§œìˆœ ì •ë ¬
        versions_with_dates.sort(key=lambda x: x[0])
        
        return [data for _, data in versions_with_dates]
    
    def _analyze_performance_trends(self, sorted_versions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„
        
        Args:
            sorted_versions: ì‹œê°„ìˆœ ì •ë ¬ëœ ë²„ì „ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼
        """
        if len(sorted_versions) < 2:
            return {
                'analysis_performed': False,
                'reason': 'Need at least 2 versions for trend analysis'
            }
        
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        trends = {}
        
        for metric in metrics:
            scores = []
            for version_data in sorted_versions:
                performance = version_data['performance']
                if metric in performance:
                    scores.append(performance[metric]['mean_score'])
                else:
                    scores.append(0.0)
            
            # ì„ í˜• íšŒê·€ ë¶„ì„
            if len(scores) >= 2:
                X = np.array(range(len(scores))).reshape(-1, 1)
                y = np.array(scores)
                
                model = LinearRegression()
                model.fit(X, y)
                
                slope = model.coef_[0]
                r_squared = model.score(X, y)
                
                # íŠ¸ë Œë“œ ë¶„ë¥˜
                if slope > 0.01:
                    trend_direction = 'improving'
                elif slope < -0.01:
                    trend_direction = 'declining'
                else:
                    trend_direction = 'stable'
                
                trends[metric] = {
                    'slope': float(slope),
                    'r_squared': float(r_squared),
                    'trend_direction': trend_direction,
                    'scores': scores,
                    'improvement_rate': float(slope * len(scores)) if len(scores) > 1 else 0.0
                }
        
        return {
            'analysis_performed': True,
            'metric_trends': trends,
            'overall_trend': self._calculate_overall_trend(trends)
        }
    
    def _calculate_overall_trend(self, trends: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²´ íŠ¸ë Œë“œ ê³„ì‚°
        
        Args:
            trends: ë©”íŠ¸ë¦­ë³„ íŠ¸ë Œë“œ ë°ì´í„°
            
        Returns:
            Dict: ì „ì²´ íŠ¸ë Œë“œ ì •ë³´
        """
        if not trends:
            return {'direction': 'unknown', 'strength': 0.0}
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        from ..constants import METRIC_WEIGHTS
        weights = METRIC_WEIGHTS
        
        weighted_slope = sum(
            trends[metric]['slope'] * weights.get(metric, 0.25)
            for metric in trends
        )
        
        avg_r_squared = np.mean([trends[metric]['r_squared'] for metric in trends])
        
        # ì „ì²´ íŠ¸ë Œë“œ ë°©í–¥ ê²°ì •
        if weighted_slope > 0.01:
            direction = 'improving'
        elif weighted_slope < -0.01:
            direction = 'declining'
        else:
            direction = 'stable'
        
        return {
            'direction': direction,
            'strength': float(abs(weighted_slope)),
            'confidence': float(avg_r_squared)
        }
    
    def _detect_regressions(self, sorted_versions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ì„±ëŠ¥ íšŒê·€ íƒì§€
        
        Args:
            sorted_versions: ì‹œê°„ìˆœ ì •ë ¬ëœ ë²„ì „ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: íšŒê·€ ë¶„ì„ ê²°ê³¼
        """
        if len(sorted_versions) < 2:
            return {
                'regressions_detected': False,
                'reason': 'Need at least 2 versions for regression detection'
            }
        
        regressions = []
        
        for i in range(1, len(sorted_versions)):
            current_version = sorted_versions[i]
            previous_version = sorted_versions[i-1]
            
            current_score = current_version['performance']['overall']['weighted_score']
            previous_score = previous_version['performance']['overall']['weighted_score']
            
            # ì„±ëŠ¥ ì €í•˜ ê²€ì‚¬
            if previous_score > 0:
                regression_ratio = (previous_score - current_score) / previous_score
                
                if regression_ratio > self.regression_threshold:
                    # íšŒê·€ ì‹¬ê°ë„ í‰ê°€
                    if regression_ratio > 0.15:
                        severity = 'critical'
                    elif regression_ratio > 0.10:
                        severity = 'major'
                    elif regression_ratio > 0.05:
                        severity = 'minor'
                    else:
                        severity = 'negligible'
                    
                    regressions.append({
                        'from_version': previous_version['version'],
                        'to_version': current_version['version'],
                        'regression_ratio': float(regression_ratio),
                        'severity': severity,
                        'previous_score': float(previous_score),
                        'current_score': float(current_score),
                        'affected_metrics': self._analyze_affected_metrics(previous_version, current_version)
                    })
        
        return {
            'regressions_detected': len(regressions) > 0,
            'total_regressions': len(regressions),
            'regressions': regressions,
            'stability_assessment': self._assess_stability(regressions)
        }
    
    def _analyze_affected_metrics(self, previous_version: Dict, current_version: Dict) -> List[Dict[str, Any]]:
        """ì˜í–¥ë°›ì€ ë©”íŠ¸ë¦­ ë¶„ì„
        
        Args:
            previous_version: ì´ì „ ë²„ì „ ë°ì´í„°
            current_version: í˜„ì¬ ë²„ì „ ë°ì´í„°
            
        Returns:
            List: ì˜í–¥ë°›ì€ ë©”íŠ¸ë¦­ ì •ë³´
        """
        affected_metrics = []
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for metric in metrics:
            if metric in previous_version['performance'] and metric in current_version['performance']:
                prev_score = previous_version['performance'][metric]['mean_score']
                curr_score = current_version['performance'][metric]['mean_score']
                
                if prev_score > 0:
                    change_ratio = (curr_score - prev_score) / prev_score
                    
                    if abs(change_ratio) > 0.03:  # 3% ì´ìƒ ë³€í™”
                        affected_metrics.append({
                            'metric': metric,
                            'previous_score': float(prev_score),
                            'current_score': float(curr_score),
                            'change_ratio': float(change_ratio),
                            'change_type': 'improvement' if change_ratio > 0 else 'regression'
                        })
        
        return affected_metrics
    
    def _assess_stability(self, regressions: List[Dict]) -> Dict[str, Any]:
        """ì•ˆì •ì„± í‰ê°€
        
        Args:
            regressions: íšŒê·€ ì •ë³´ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ì•ˆì •ì„± í‰ê°€ ê²°ê³¼
        """
        if not regressions:
            return {
                'stability_level': 'stable',
                'description': 'No significant regressions detected'
            }
        
        # ì‹¬ê°ë„ë³„ ì¹´ìš´íŠ¸
        severity_counts: Dict[str, int] = {}
        for regression in regressions:
            severity = regression['severity']
            severity_counts[severity] = severity_counts.get(severity, 0) + 1
        
        # ì•ˆì •ì„± ë“±ê¸‰ ê²°ì •
        if severity_counts.get('critical', 0) > 0:
            stability_level = 'unstable'
            description = f"Critical regressions detected: {severity_counts['critical']}"
        elif severity_counts.get('major', 0) > 1:
            stability_level = 'unstable'
            description = f"Multiple major regressions detected: {severity_counts['major']}"
        elif severity_counts.get('major', 0) > 0 or severity_counts.get('minor', 0) > 2:
            stability_level = 'moderately_stable'
            description = "Some regressions detected but manageable"
        else:
            stability_level = 'stable'
            description = "Minor regressions only"
        
        return {
            'stability_level': stability_level,
            'description': description,
            'severity_counts': severity_counts
        }
    
    def _identify_improvements(self, sorted_versions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì£¼ëª©í•  ë§Œí•œ ê°œì„ ì‚¬í•­ ì‹ë³„
        
        Args:
            sorted_versions: ì‹œê°„ìˆœ ì •ë ¬ëœ ë²„ì „ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List: ê°œì„ ì‚¬í•­ ë¦¬ìŠ¤íŠ¸
        """
        improvements = []
        
        for i in range(1, len(sorted_versions)):
            current_version = sorted_versions[i]
            previous_version = sorted_versions[i-1]
            
            current_score = current_version['performance']['overall']['weighted_score']
            previous_score = previous_version['performance']['overall']['weighted_score']
            
            # ì„±ëŠ¥ ê°œì„  ê²€ì‚¬
            if previous_score > 0:
                improvement_ratio = (current_score - previous_score) / previous_score
                
                if improvement_ratio > self.improvement_threshold:
                    # ê°œì„  ì •ë„ í‰ê°€
                    if improvement_ratio > 0.15:
                        improvement_level = 'breakthrough'
                    elif improvement_ratio > 0.08:
                        improvement_level = 'significant'
                    else:
                        improvement_level = 'moderate'
                    
                    improvements.append({
                        'from_version': previous_version['version'],
                        'to_version': current_version['version'],
                        'improvement_ratio': float(improvement_ratio),
                        'improvement_level': improvement_level,
                        'previous_score': float(previous_score),
                        'current_score': float(current_score),
                        'improved_metrics': self._analyze_improved_metrics(previous_version, current_version)
                    })
        
        return improvements
    
    def _analyze_improved_metrics(self, previous_version: Dict, current_version: Dict) -> List[Dict[str, Any]]:
        """ê°œì„ ëœ ë©”íŠ¸ë¦­ ë¶„ì„
        
        Args:
            previous_version: ì´ì „ ë²„ì „ ë°ì´í„°
            current_version: í˜„ì¬ ë²„ì „ ë°ì´í„°
            
        Returns:
            List: ê°œì„ ëœ ë©”íŠ¸ë¦­ ì •ë³´
        """
        improved_metrics = []
        metrics = ['correctness', 'clarity', 'actionability', 'json_correctness']
        
        for metric in metrics:
            if metric in previous_version['performance'] and metric in current_version['performance']:
                prev_score = previous_version['performance'][metric]['mean_score']
                curr_score = current_version['performance'][metric]['mean_score']
                
                if prev_score > 0:
                    change_ratio = (curr_score - prev_score) / prev_score
                    
                    if change_ratio > 0.03:  # 3% ì´ìƒ ê°œì„ 
                        improved_metrics.append({
                            'metric': metric,
                            'previous_score': float(prev_score),
                            'current_score': float(curr_score),
                            'improvement_ratio': float(change_ratio)
                        })
        
        return improved_metrics
    
    def _generate_version_recommendations(self, sorted_versions: List[Dict[str, Any]], 
                                        analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ë²„ì „ë³„ ê¶Œì¥ì‚¬í•­ ìƒì„±
        
        Args:
            sorted_versions: ì‹œê°„ìˆœ ì •ë ¬ëœ ë²„ì „ ë¦¬ìŠ¤íŠ¸
            analysis: ë¶„ì„ ê²°ê³¼
            
        Returns:
            Dict: ë²„ì „ë³„ ê¶Œì¥ì‚¬í•­
        """
        if not sorted_versions:
            return {
                'recommended_version': None,
                'recommendations': ['ë¶„ì„í•  ë²„ì „ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.']
            }
        
        # ìµœê³  ì„±ëŠ¥ ë²„ì „ ì‹ë³„
        best_version = max(
            sorted_versions, 
            key=lambda x: x['performance']['overall']['weighted_score']
        )
        
        # ìµœì‹  ë²„ì „
        latest_version = sorted_versions[-1]
        
        recommendations = []
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        best_score = best_version['performance']['overall']['weighted_score']
        if best_score >= self.excellent_threshold:
            recommendations.append(
                f"ğŸ† ìµœê³  ì„±ëŠ¥ ë²„ì „: {best_version['version']} "
                f"(ì ìˆ˜: {best_score:.3f}, ë“±ê¸‰: {best_version['performance']['overall']['grade']})"
            )
        
        # ì•ˆì •ì„± ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        regression_analysis = analysis.get('regression_analysis', {})
        stability = regression_analysis.get('stability_assessment', {})
        
        if stability.get('stability_level') == 'stable':
            recommendations.append(f"âœ… ì•ˆì •ì„± ìš°ìˆ˜: ì „ì²´ì ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„±ëŠ¥")
        elif stability.get('stability_level') == 'unstable':
            recommendations.append(f"âš ï¸ ì•ˆì •ì„± ì£¼ì˜: {stability.get('description', '')}")
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        performance_trends = analysis.get('performance_trends', {})
        if performance_trends.get('analysis_performed'):
            overall_trend = performance_trends.get('overall_trend', {})
            trend_direction = overall_trend.get('direction', 'unknown')
            
            if trend_direction == 'improving':
                recommendations.append("ğŸ“ˆ ì„±ëŠ¥ ê°œì„  íŠ¸ë Œë“œ: ì§€ì†ì ì¸ ì„±ëŠ¥ í–¥ìƒ")
            elif trend_direction == 'declining':
                recommendations.append("ğŸ“‰ ì„±ëŠ¥ í•˜ë½ íŠ¸ë Œë“œ: ì„±ëŠ¥ ì €í•˜ ì¶”ì„¸")
            else:
                recommendations.append("ğŸ“Š ì„±ëŠ¥ ì•ˆì • íŠ¸ë Œë“œ: ì¼ì •í•œ ì„±ëŠ¥ ìœ ì§€")
        
        # ë²„ì „ ì„ íƒ ê¶Œì¥ì‚¬í•­
        if best_version['version'] == latest_version['version']:
            recommended_version = latest_version['version']
            recommendations.append(f"ğŸ¯ ê¶Œì¥ ë²„ì „: {recommended_version} (ìµœê³  ì„±ëŠ¥ + ìµœì‹  ë²„ì „)")
        else:
            latest_score = latest_version['performance']['overall']['weighted_score']
            score_diff = best_score - latest_score
            
            if score_diff > 0.05:  # 5% ì´ìƒ ì°¨ì´
                recommended_version = best_version['version']
                recommendations.append(
                    f"ğŸ¯ ê¶Œì¥ ë²„ì „: {recommended_version} (ìµœê³  ì„±ëŠ¥ ìš°ì„ )"
                )
                recommendations.append(
                    f"ğŸ’¡ ìµœì‹  ë²„ì „ {latest_version['version']}ì€ ì„±ëŠ¥ì´ {score_diff:.3f}ì  ë‚®ìŒ"
                )
            else:
                recommended_version = latest_version['version']
                recommendations.append(
                    f"ğŸ¯ ê¶Œì¥ ë²„ì „: {recommended_version} (ìµœì‹  ë²„ì „, ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸)"
                )
        
        # ê°œì„ ì‚¬í•­ í•˜ì´ë¼ì´íŠ¸
        improvements = analysis.get('improvement_highlights', [])
        if improvements:
            major_improvements = [
                imp for imp in improvements 
                if imp['improvement_level'] in ['breakthrough', 'significant']
            ]
            
            if major_improvements:
                recommendations.append(
                    f"ğŸš€ ì£¼ìš” ê°œì„ ì‚¬í•­: {len(major_improvements)}ê°œì˜ ì˜ë¯¸ìˆëŠ” ì„±ëŠ¥ ê°œì„ "
                )
        
        return {
            'recommended_version': recommended_version,
            'best_performance_version': best_version['version'],
            'latest_version': latest_version['version'],
            'recommendations': recommendations
        }