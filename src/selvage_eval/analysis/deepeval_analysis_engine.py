"""DeepEval ë¶„ì„ ì—”ì§„ V2

ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µí•©ëœ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.
ìƒˆë¡œìš´ ë¶„ì„ í´ë˜ìŠ¤ë“¤ì„ í†µí•©í•˜ì—¬ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
import logging
import os

from selvage_eval.llm.gemini_client import GeminiClient

from .deepeval_log_parser import DeepEvalLogParser
from .metric_aggregator import MetricAggregator
from .failure_pattern_analyzer import FailurePatternAnalyzer
from .model_performance_comparator import ModelPerformanceComparator
from .tech_stack_analyzer import TechStackAnalyzer
from .version_comparison_analyzer import VersionComparisonAnalyzer
from .visualization_generator import VisualizationGenerator

logger = logging.getLogger(__name__)


class DeepEvalAnalysisEngine:
    """DeepEval ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, output_dir: str = "~/Library/selvage-eval/analyze_results"):
        """ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            output_dir: ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.log_parser = DeepEvalLogParser()
        self.metric_aggregator = MetricAggregator()
        self.failure_analyzer = FailurePatternAnalyzer()
        self.model_comparator = ModelPerformanceComparator()
        self.tech_stack_analyzer = TechStackAnalyzer()
        self.version_analyzer = VersionComparisonAnalyzer()
        self.visualizer = VisualizationGenerator()
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._init_gemini_client()
    
    def analyze_session(self, session_id: str, 
                       deepeval_results_path: Optional[str] = None,
                       output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì„¸ì…˜ ë¶„ì„ ì‹¤í–‰
        
        Args:
            session_id: ì„¸ì…˜ ID
            deepeval_results_path: DeepEval ê²°ê³¼ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ session_idë¡œ ìë™ ê²€ìƒ‰)
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        start_time = time.time()
        logger.info(f"=== DeepEval ì„¸ì…˜ ë¶„ì„ ì‹œì‘: {session_id} ===")
        
        # 1ë‹¨ê³„: DeepEval ê²°ê³¼ ê²½ë¡œ í™•ì¸
        logger.info("1ë‹¨ê³„: ì„¸ì…˜ ê²½ë¡œ í™•ì¸ ë° ì„¤ì • ì¤‘...")
        if deepeval_results_path:
            session_path_obj = Path(deepeval_results_path).expanduser()
        else:
            default_base_path = Path("/Users/demin_coder/Library/selvage-eval/deepeval_results")
            session_path_obj = default_base_path / session_id
            
        if not session_path_obj.exists():
            raise FileNotFoundError(f"ì„¸ì…˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path_obj}")
        
        logger.info(f"ì„¸ì…˜ ê²½ë¡œ í™•ì¸ ì™„ë£Œ: {session_path_obj}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            session_id = session_path_obj.name
            final_output_dir = self.output_dir / session_id
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ: {final_output_dir}")
        
        # 2ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ ê²°ê³¼ ìˆ˜ì§‘
        logger.info("2ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")
        step_start = time.time()
        log_results = self._collect_log_results(session_path_obj)
        
        if not log_results:
            raise ValueError("DeepEval ë¡œê·¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        total_cases = sum(len(results) for results in log_results.values())
        logger.info(f"ë¡œê·¸ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ - ëª¨ë¸ {len(log_results)}ê°œ, ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {total_cases}ê°œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 3ë‹¨ê³„: ì¢…í•© ë¶„ì„ ì‹¤í–‰
        logger.info("3ë‹¨ê³„: ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        step_start = time.time()
        analysis_results = self._perform_comprehensive_analysis(log_results)
        logger.info(f"ì¢…í•© ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 4ë‹¨ê³„: ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„° ì¶”ê°€ (Gemini ë²ˆì—­ í¬í•¨)
        logger.info("4ë‹¨ê³„: ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë° Gemini ë²ˆì—­ ìˆ˜í–‰ ì¤‘...")
        step_start = time.time()
        model_failure_analysis = self._generate_model_failure_analysis(log_results)
        analysis_results["model_failure_analysis"] = model_failure_analysis
        logger.info(f"ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 5ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        logger.info("5ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        step_start = time.time()
        markdown_report = self._generate_markdown_report(analysis_results)
        report_path = final_output_dir / "analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        logger.info(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path} (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 6ë‹¨ê³„: JSON ë°ì´í„° ì €ì¥
        logger.info("6ë‹¨ê³„: JSON ë°ì´í„° ì €ì¥ ì¤‘...")
        step_start = time.time()
        json_path = final_output_dir / "analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"JSON ë°ì´í„° ì €ì¥ ì™„ë£Œ: {json_path} (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 7ë‹¨ê³„: ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        logger.info("7ë‹¨ê³„: ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        step_start = time.time()
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                analysis_results, 
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            summary_report = self.visualizer.create_summary_report(
                analysis_results,
                final_output_dir / "summary_report.html"
            )
            if summary_report:
                visualization_files.append(summary_report)
            
            logger.info(f"ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ - {len(visualization_files)}ê°œ íŒŒì¼ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
                
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        total_time = time.time() - start_time
        logger.info(f"=== DeepEval ì„¸ì…˜ ë¶„ì„ ì™„ë£Œ: {session_id} (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ) ===")
        
        return {
            "analysis_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "session_path": str(session_path_obj),
                "total_test_cases": analysis_results.get("data_summary", {}).get("total_test_cases", 0),
                "models_analyzed": list(analysis_results.get("model_comparison", {}).get("model_statistics", {}).keys()),
                "total_processing_time": total_time
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def analyze_multiple_sessions(self, base_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì„¸ì…˜ í†µí•© ë¶„ì„ (ë²„ì „ ë¹„êµ í¬í•¨)
        
        Args:
            base_path: ì—¬ëŸ¬ ì„¸ì…˜ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        base_path_obj = Path(base_path).expanduser()
        
        if not base_path_obj.exists():
            raise FileNotFoundError(f"ê¸°ë³¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            final_output_dir = self.output_dir / "multi_session_analysis"
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë²„ì „ë³„ ë°ì´í„° ìˆ˜ì§‘
        version_data = self.version_analyzer.collect_version_data(str(base_path_obj))
        
        if not version_data:
            raise ValueError("ë¶„ì„í•  ë²„ì „ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë²„ì „ ë¹„êµ ë¶„ì„
        version_analysis = self.version_analyzer.analyze_version_progression(version_data)
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        repo_results = self._prepare_repo_results_from_versions(version_data)
        tech_stack_analysis = self.tech_stack_analyzer.analyze_tech_stack_performance(repo_results)
        
        # í†µí•© ë¶„ì„ ê²°ê³¼
        integrated_results = {
            "analysis_type": "multi_session",
            "version_analysis": version_analysis,
            "tech_stack_analysis": tech_stack_analysis,
            "data_summary": {
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values()),
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report = self._generate_multi_session_markdown_report(integrated_results)
        report_path = final_output_dir / "multi_session_analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        # JSON ë°ì´í„° ì €ì¥
        json_path = final_output_dir / "multi_session_analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(integrated_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                integrated_results,
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
        except Exception as e:
            logger.error(f"ë‹¤ì¤‘ ì„¸ì…˜ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "analysis_metadata": {
                "analysis_type": "multi_session",
                "analysis_timestamp": datetime.now().isoformat(),
                "base_path": str(base_path_obj),
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values())
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def _collect_log_results(self, session_path: Path) -> Dict[str, List]:
        """ë¡œê·¸ íŒŒì¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        log_files = list(session_path.glob("**/*.log"))
        logger.info(f"ë¡œê·¸ íŒŒì¼ {len(log_files)}ê°œ ë°œê²¬, íŒŒì‹± ì‹œì‘...")
        
        # .log íŒŒì¼ë“¤ ì°¾ê¸°
        for i, log_file in enumerate(log_files, 1):
            try:
                logger.debug(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘ ({i}/{len(log_files)}): {log_file.name}")
                
                # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
                model_name = self._extract_model_name_from_path(log_file)
                
                # ë¡œê·¸ íŒŒì‹±
                test_results = list(self.log_parser.parse_log_file(log_file))
                
                if test_results:
                    if model_name not in results:
                        results[model_name] = []
                    results[model_name].extend(test_results)
                    logger.debug(f"ëª¨ë¸ '{model_name}': {len(test_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€")
                else:
                    logger.warning(f"ë¡œê·¸ íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {log_file}")
                    
            except Exception as e:
                logger.warning(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {log_file} - {e}")
        
        logger.info(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì™„ë£Œ - ì´ {len(results)}ê°œ ëª¨ë¸ì—ì„œ {sum(len(r) for r in results.values())}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜ì§‘")
        return results
    
    def _extract_model_name_from_path(self, log_file: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
        
        Args:
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ëª¨ë¸ëª…
        """
        # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ë””ë ‰í† ë¦¬ êµ¬ì¡° ê¸°ë°˜)
        parent_dir = log_file.parent.name
        
        # ìœ íš¨í•œ ëª¨ë¸ëª…ì¸ì§€ í™•ì¸
        if parent_dir and parent_dir != 'deepeval_results':
            return parent_dir
        
        # fallback: íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
        file_name = log_file.stem
        
        # deepeval_results_model_name.log íŒ¨í„´
        if file_name.startswith('deepeval_results_'):
            parts = file_name.split('_')
            if len(parts) >= 3:
                return '_'.join(parts[2:])
        
        # model_name.log íŒ¨í„´
        if '_' in file_name:
            return file_name.split('_')[0]
        
        # ê¸°ë³¸ê°’
        return file_name or "unknown_model"
    
    def _perform_comprehensive_analysis(self, log_results: Dict[str, List]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        
        Args:
            log_results: ëª¨ë¸ë³„ ë¡œê·¸ ê²°ê³¼
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        if not log_results:
            return {"error": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
            logger.info("ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘...")
            step_start = time.time()
            model_comparison = self.model_comparator.compare_models(log_results)
            logger.info(f"ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ - ëª¨ë“  ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘
            logger.info("ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            step_start = time.time()
            all_failed_cases = []
            for model_name, model_results in log_results.items():
                model_failures = 0
                for test_case in model_results:
                    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¡œ ê°„ì£¼
                    has_failure = (
                        not test_case.correctness.passed or
                        not test_case.clarity.passed or
                        not test_case.actionability.passed or
                        not test_case.json_correctness.passed
                    )
                    if has_failure:
                        all_failed_cases.append(test_case)
                        model_failures += 1
                
                logger.debug(f"ëª¨ë¸ '{model_name}': {model_failures}/{len(model_results)} ì¼€ì´ìŠ¤ ì‹¤íŒ¨")
            
            logger.info(f"ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(all_failed_cases)}ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
            logger.info("ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
            step_start = time.time()
            failure_analysis = self.failure_analyzer.analyze_failure_patterns(all_failed_cases)
            logger.info(f"ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ë°ì´í„° ìš”ì•½
            total_test_cases = sum(len(results) for results in log_results.values())
            successful_cases = total_test_cases - len(all_failed_cases)
            
            logger.info(f"ì¢…í•© ë¶„ì„ ê²°ê³¼ - ì´ {total_test_cases}ê°œ ì¼€ì´ìŠ¤ ì¤‘ ì„±ê³µ {successful_cases}ê°œ, ì‹¤íŒ¨ {len(all_failed_cases)}ê°œ")
            
            return {
                "analysis_type": "single_session", 
                "model_comparison": model_comparison,
                "failure_analysis": failure_analysis,
                "data_summary": {
                    "total_test_cases": total_test_cases,
                    "successful_evaluations": successful_cases,
                    "failed_evaluations": len(all_failed_cases),
                    "models_count": len(log_results),
                    "analysis_timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}
    
    def _prepare_repo_results_from_versions(self, version_data: Dict[str, Any]) -> Dict[str, Dict[str, List]]:
        """ë²„ì „ ë°ì´í„°ì—ì„œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„
        
        Args:
            version_data: ë²„ì „ë³„ ë°ì´í„°
            
        Returns:
            ì €ì¥ì†Œë³„ ëª¨ë¸ ê²°ê³¼
        """
        repo_results = {}
        
        for version, data in version_data.items():
            for session in data['sessions']:
                session_dir = Path(session['session_dir'])
                
                # ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                repo_name = self._extract_repo_name_from_session_path(session_dir)
                
                if repo_name not in repo_results:
                    repo_results[repo_name] = {}
                
                # ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í•‘
                session_results = session.get('results', [])
                if session_results:
                    model_name = f"version_{version}"
                    if model_name not in repo_results[repo_name]:
                        repo_results[repo_name][model_name] = []
                    repo_results[repo_name][model_name].extend(session_results)
        
        return repo_results
    
    def _extract_repo_name_from_session_path(self, session_path: Path) -> str:
        """ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ì €ì¥ì†Œëª…
        """
        # ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª…ìœ¼ë¡œ ë³´ì´ëŠ” ë¶€ë¶„ ì¶”ì¶œ
        path_parts = session_path.parts
        
        # ì¼ë°˜ì ì¸ ì €ì¥ì†Œëª… íŒ¨í„´ ì°¾ê¸°
        for part in reversed(path_parts):
            if any(keyword in part.lower() for keyword in ['cline', 'ecommerce', 'kotlin', 'selvage']):
                return part
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ë§ˆì§€ë§‰ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        return session_path.name or "unknown_repo"
    
    def _generate_markdown_report(self, analysis_results: Dict[str, Any]) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            analysis_results: ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        if "error" in analysis_results:
            return f"# ë¶„ì„ ì˜¤ë¥˜\n\n{analysis_results['error']}"
        
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# DeepEval ë¶„ì„ ë³´ê³ ì„œ (V2)",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ë¶„ì„ ìœ í˜•**: {analysis_results.get('analysis_type', 'single_session')}",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = analysis_results.get("data_summary", {})
        if data_summary:
            lines.extend([
                "## ğŸ“Š ë°ì´í„° ìš”ì•½",
                "",
                f"- **ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {data_summary.get('total_test_cases', 0)}ê°œ",
                f"- **ì„±ê³µí•œ í‰ê°€**: {data_summary.get('successful_evaluations', 0)}ê°œ",
                f"- **ì‹¤íŒ¨í•œ í‰ê°€**: {data_summary.get('failed_evaluations', 0)}ê°œ",
                f"- **ë¶„ì„ëœ ëª¨ë¸**: {data_summary.get('models_count', 0)}ê°œ",
                ""
            ])
        
        # ëª¨ë¸ ë¹„êµ ê²°ê³¼
        model_comparison = analysis_results.get("model_comparison", {})
        if model_comparison and model_comparison.get("recommendations"):
            lines.extend([
                "## ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ",
                "",
                "### ì£¼ìš” ê¶Œì¥ì‚¬í•­",
                ""
            ])
            
            for rec in model_comparison["recommendations"][:10]:  # ìƒìœ„ 10ê°œ
                lines.append(f"- {rec}")
            
            lines.append("")
            
            # ë¹„êµ í‘œ
            comparison_table = model_comparison.get("comparison_table", {})
            table_data = comparison_table.get("table_data", [])
            
            if table_data:
                lines.extend([
                    "### ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½",
                    "",
                    "| ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë“±ê¸‰ | ìˆœìœ„ | ì •í™•ì„± | ëª…í™•ì„± | ì‹¤í–‰ê°€ëŠ¥ì„± | JSON ì •í™•ì„± |",
                    "|--------|----------|------|------|--------|--------|------------|-------------|"
                ])
                
                for model_data in table_data:
                    lines.append(
                        f"| {model_data.get('model_name', 'N/A')} | "
                        f"{model_data.get('overall_score', 0):.3f} | "
                        f"{model_data.get('grade', 'N/A')} | "
                        f"{model_data.get('overall_rank', 'N/A')} | "
                        f"{model_data.get('correctness_score', 0):.3f} | "
                        f"{model_data.get('clarity_score', 0):.3f} | "
                        f"{model_data.get('actionability_score', 0):.3f} | "
                        f"{model_data.get('json_correctness_score', 0):.3f} |"
                    )
                
                lines.append("")
        
        # ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„
        model_failure_analysis = analysis_results.get("model_failure_analysis", {})
        if model_failure_analysis:
            lines.extend([
                "## ğŸš¨ ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„",
                "",
                "ê° ëª¨ë¸ë³„ë¡œ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ë¶„ì„ì…ë‹ˆë‹¤.",
                ""
            ])
            
            # ëª¨ë¸ë³„ ì‹¤íŒ¨ ì •ë³´ í…Œì´ë¸”
            lines.extend([
                "### ëª¨ë¸ë³„ ì‹¤íŒ¨ í˜„í™©",
                "",
                "| ëª¨ë¸ëª… | ì´ í…ŒìŠ¤íŠ¸ | ì‹¤íŒ¨ ê±´ìˆ˜ | ì‹¤íŒ¨ìœ¨ | ì£¼ìš” ì‹¤íŒ¨ ë©”íŠ¸ë¦­ |",
                "|--------|-----------|-----------|--------|------------------|"
            ])
            
            for model_name, failure_data in model_failure_analysis.items():
                total_tests = failure_data.get('total_tests', 0)
                total_failures = failure_data.get('total_failures', 0)
                failure_rate = failure_data.get('failure_rate', 0)
                failed_metrics = failure_data.get('failed_metrics', {})
                
                # ì£¼ìš” ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                main_failed_metrics = [metric for metric in failed_metrics.keys() if failed_metrics[metric]]
                main_metrics_str = ", ".join(main_failed_metrics) if main_failed_metrics else "ì—†ìŒ"
                
                lines.append(
                    f"| {model_name} | {total_tests} | {total_failures} | "
                    f"{failure_rate:.1%} | {main_metrics_str} |"
                )
            
            lines.append("")
            
            # ê° ëª¨ë¸ë³„ ìƒì„¸ ì‹¤íŒ¨ ë¶„ì„
            for model_name, failure_data in model_failure_analysis.items():
                if failure_data.get('total_failures', 0) > 0:
                    lines.extend([
                        f"#### {model_name} ëª¨ë¸ ì‹¤íŒ¨ ë¶„ì„",
                        ""
                    ])
                    
                    # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ì •ë³´
                    failed_metrics = failure_data.get('failed_metrics', {})
                    if failed_metrics:
                        lines.extend([
                            "**ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­:**",
                            ""
                        ])
                        
                        for metric_name, metric_data in failed_metrics.items():
                            metric_display = {
                                'correctness': 'ì •í™•ì„±',
                                'clarity': 'ëª…í™•ì„±', 
                                'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                                'json_correctness': 'JSON ì •í™•ì„±'
                            }.get(metric_name, metric_name)
                            
                            lines.append(
                                f"- **{metric_display}**: {metric_data['failure_count']}ê±´ ì‹¤íŒ¨ "
                                f"(í‰ê·  ì‹ ë¢°ë„: {metric_data['avg_confidence']:.3f})"
                            )
                    
                    # ë©”íŠ¸ë¦­ë³„ ë²ˆì—­ëœ ì‹¤íŒ¨ ì´ìœ 
                    failed_metrics = failure_data.get('failed_metrics', {})
                    if failed_metrics:
                        lines.extend([
                            "",
                            "**ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ì´ìœ :**",
                            ""
                        ])
                        
                        for metric_name, metric_data in failed_metrics.items():
                            metric_display = {
                                'correctness': 'ì •í™•ì„±',
                                'clarity': 'ëª…í™•ì„±', 
                                'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                                'json_correctness': 'JSON ì •í™•ì„±'
                            }.get(metric_name, metric_name)
                            
                            translated_reasons = metric_data.get('translated_reasons', [])
                            if translated_reasons:
                                lines.append(f"**{metric_display}:**")
                                for reason in translated_reasons:
                                    lines.append(f"- {reason}")
                                lines.append("")
                    
                    ai_analysis = failure_data['ai_analyzed_failure_summary']
                    if ai_analysis:
                        lines.extend([
                            "",
                            "## ğŸ¤– AI ê¸°ë°˜ ì‹¤íŒ¨ ì‚¬ìœ  ë¶„ì„",
                            "",
                            f"**ë¶„ì„ ëŒ€ìƒ**: {', '.join(ai_analysis['analyzed_metrics'])} ë©”íŠ¸ë¦­",
                            f"**ì´ ë¶„ì„ ì‹¤íŒ¨ ê±´ìˆ˜**: {ai_analysis['total_failures_analyzed']}ê±´", 
                            "",
                            ai_analysis['analysis_content'],
                            ""
                        ])
                    
                    lines.append("")
            
            lines.append("")
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_analysis = analysis_results.get("failure_analysis", {})
        if failure_analysis and failure_analysis.get("total_failures", 0) > 0:
            lines.extend([
                "## ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„",
                "",
                f"**ì´ ì‹¤íŒ¨ ê±´ìˆ˜**: {failure_analysis['total_failures']}ê°œ",
                ""
            ])
            
            # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„ì„
            by_metric = failure_analysis.get("by_metric", {})
            if by_metric:
                lines.extend([
                    "### ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ í˜„í™©",
                    "",
                    "| ë©”íŠ¸ë¦­ | ì‹¤íŒ¨ ê±´ìˆ˜ | ì‹¤íŒ¨ìœ¨ | í‰ê·  ì‹ ë¢°ë„ |",
                    "|--------|-----------|--------|-------------|"
                ])
                
                for metric, data in by_metric.items():
                    lines.append(
                        f"| {metric.replace('_', ' ').title()} | "
                        f"{data.get('total_failures', 0)} | "
                        f"{data.get('failure_rate', 0):.1%} | "
                        f"{data.get('avg_confidence', 0):.3f} |"
                    )
                
                lines.append("")
            
            # ì¤‘ìš”í•œ íŒ¨í„´
            critical_patterns = failure_analysis.get("critical_patterns", [])
            if critical_patterns:
                lines.extend([
                    "### ì¤‘ìš”í•œ ì‹¤íŒ¨ íŒ¨í„´",
                    ""
                ])
                
                for pattern in critical_patterns[:5]:  # ìƒìœ„ 5ê°œ
                    lines.append(
                        f"- **{pattern.get('category', 'Unknown')}**: "
                        f"{pattern.get('count', 0)}ê±´ ({pattern.get('percentage', 0):.1f}%) "
                        f"- {pattern.get('reason', '')}"
                    )
                
                lines.append("")
        
        # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
        lines.extend([
            "## ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­",
            "",
            "### ì£¼ìš” ë°œê²¬ì‚¬í•­",
            ""
        ])
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­ ì¢…í•©
        all_recommendations = []
        if model_comparison.get("recommendations"):
            all_recommendations.extend(model_comparison["recommendations"][:3])
        
        if failure_analysis.get("critical_patterns"):
            all_recommendations.append(
                f"ì‹¤íŒ¨ íŒ¨í„´ ê°œì„  í•„ìš”: {len(failure_analysis['critical_patterns'])}ê°œì˜ ì¤‘ìš”í•œ íŒ¨í„´ ë°œê²¬"
            )
        
        for rec in all_recommendations:
            lines.append(f"1. {rec}")
        
        # AI ê¸°ë°˜ ì¢…í•© ì‹¤íŒ¨ ë¶„ì„
        ai_summary = self._generate_ai_failure_summary(model_failure_analysis)
        if ai_summary:
            lines.extend([
                "",
                "### AI ê¸°ë°˜ ì¢…í•© ì‹¤íŒ¨ ë¶„ì„",
                "",
                ai_summary,
                ""
            ])
        
        return "\n".join(lines)
    
    def _generate_multi_session_markdown_report(self, integrated_results: Dict[str, Any]) -> str:
        """ë‹¤ì¤‘ ì„¸ì…˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            integrated_results: í†µí•© ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# ë‹¤ì¤‘ ì„¸ì…˜ ë¶„ì„ ë³´ê³ ì„œ",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "**ë¶„ì„ ìœ í˜•**: ë²„ì „ ë¹„êµ ë° ê¸°ìˆ ìŠ¤íƒ ë¶„ì„",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = integrated_results.get("data_summary", {})
        lines.extend([
            "## ğŸ“Š ë¶„ì„ ê°œìš”",
            "",
            f"- **ë¶„ì„ëœ ë²„ì „**: {data_summary.get('total_versions', 0)}ê°œ",
            f"- **ì´ ì„¸ì…˜**: {data_summary.get('total_sessions', 0)}ê°œ",
            ""
        ])
        
        # ë²„ì „ ë¶„ì„
        version_analysis = integrated_results.get("version_analysis", {})
        if version_analysis:
            lines.extend([
                "## ğŸ“ˆ ë²„ì „ë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            # ë²„ì „ ê¶Œì¥ì‚¬í•­
            version_recommendations = version_analysis.get("version_recommendations", {})
            if version_recommendations:
                recommended_version = version_recommendations.get("recommended_version")
                if recommended_version:
                    lines.append(f"**ê¶Œì¥ ë²„ì „**: {recommended_version}")
                    lines.append("")
                
                recommendations = version_recommendations.get("recommendations", [])
                if recommendations:
                    lines.append("### ë²„ì „ ê´€ë ¨ ê¶Œì¥ì‚¬í•­")
                    lines.append("")
                    for rec in recommendations:
                        lines.append(f"- {rec}")
                    lines.append("")
        
        # ê¸°ìˆ ìŠ¤íƒ ë¶„ì„
        tech_stack_analysis = integrated_results.get("tech_stack_analysis", {})
        if tech_stack_analysis:
            lines.extend([
                "## ğŸ›  ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            tech_recommendations = tech_stack_analysis.get("recommendations", [])
            if tech_recommendations:
                for rec in tech_recommendations[:10]:  # ìƒìœ„ 10ê°œ
                    lines.append(f"- {rec}")
                lines.append("")
        
        lines.extend([
            "## ğŸ’¡ í†µí•© ê¶Œì¥ì‚¬í•­",
            "",
            "### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ",
            "- ìµœì‹  ì•ˆì • ë²„ì „ìœ¼ë¡œì˜ ì—…ê·¸ë ˆì´ë“œ ê²€í† ",
            "- ì„±ëŠ¥ íšŒê·€ê°€ ìˆëŠ” ë²„ì „ ì‚¬ìš© ì¤‘ë‹¨",
            "",
            "### ê¸°ìˆ ìŠ¤íƒ ìµœì í™”", 
            "- ê° ê¸°ìˆ ìŠ¤íƒì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ",
            "- ì„±ëŠ¥ì´ ë‚®ì€ ê¸°ìˆ ìŠ¤íƒì˜ í”„ë¡¬í”„íŠ¸ ê°œì„ ",
            "",
            "### ì§€ì†ì  ëª¨ë‹ˆí„°ë§",
            "- ì •ê¸°ì ì¸ ë²„ì „ë³„ ì„±ëŠ¥ ì¶”ì ",
            "- ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹",
            ""
        ])
        
        return "\n".join(lines)
    
    def _init_gemini_client(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ì´ìœ  ë²ˆì—­ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                self.gemini_client = None
                self.gemini_pro_client = None
                return
            
            # ë²ˆì—­ìš© Flash í´ë¼ì´ì–¸íŠ¸
            self.gemini_client = GeminiClient(api_key=api_key, model_name="gemini-2.5-flash")
            # AI ë¶„ì„ìš© Pro í´ë¼ì´ì–¸íŠ¸
            self.gemini_pro_client = GeminiClient(api_key=api_key, model_name="gemini-2.5-pro")
            logger.info("Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (Flash + Pro)")
            
        except Exception as e:
            logger.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.gemini_client = None
            self.gemini_pro_client = None
    
    def _translate_failure_reason_with_gemini(self, failure_reason: str) -> str:
        """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­
        
        Args:
            failure_reason: ì˜ì–´ ì‹¤íŒ¨ ì´ìœ 
            
        Returns:
            í•œê¸€ ë²ˆì—­ëœ ì‹¤íŒ¨ ì´ìœ 
        """
        if not self.gemini_client or not failure_reason:
            return failure_reason
        
        try:
            
            system_instruction = "ë‹¤ìŒ DeepEval í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì´ìœ ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ê¸°ìˆ ì  ìš©ì–´ëŠ” ì ì ˆíˆ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë˜, ì˜ë¯¸ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ë„ë¡ í•´ì£¼ì„¸ìš”."
            
            messages = [{
                "role": "user", 
                "content": f"ì›ë¬¸: {failure_reason}\n\në²ˆì—­:"
            }]
            
            response = self.gemini_client.query(
                messages=messages,
                system_instruction=system_instruction
            )
            
            translated = response.strip()
            
            # ë²ˆì—­ ê²°ê³¼ ê²€ì¦
            if translated and len(translated) > 0:
                return translated
            else:
                logger.warning(f"ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë°˜í™˜: {failure_reason}")
                return failure_reason
                
        except Exception as e:
            logger.error(f"Gemini ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return failure_reason
    
    def _batch_translate_failure_reasons(self, failure_reasons: List[str]) -> List[str]:
        """ì—¬ëŸ¬ ì‹¤íŒ¨ ì´ìœ ë¥¼ ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë²ˆì—­ (ì§„ì •í•œ ë°°ì¹˜ ì²˜ë¦¬)
        
        Args:
            failure_reasons: ì˜ì–´ ì‹¤íŒ¨ ì´ìœ  ëª©ë¡
            
        Returns:
            í•œê¸€ ë²ˆì—­ëœ ì‹¤íŒ¨ ì´ìœ  ëª©ë¡
        """
        if not self.gemini_client or not failure_reasons:
            return failure_reasons
        
        logger.info(f"ì‹¤íŒ¨ ì´ìœ  {len(failure_reasons)}ê°œ ë³‘ë ¬ ë²ˆì—­ ì‹œì‘")
        
        # ë°°ì¹˜ ìš”ì²­ ë°ì´í„° ì¤€ë¹„
        batch_requests = []
        system_instruction = "ë‹¤ìŒ DeepEval í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì´ìœ ë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ë²ˆì—­ëœ ê²°ê³¼ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš”."
        
        for reason in failure_reasons:
            request = {
                'messages': [{
                    "role": "user", 
                    "content": f"ì›ë¬¸: {reason}\n\në²ˆì—­:"
                }]
            }
            batch_requests.append(request)
        
        try:
            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•œ ë°°ì¹˜ ë²ˆì—­
            results = self.gemini_client.batch_query(
                batch_requests=batch_requests,
                system_instruction=system_instruction,
                max_workers=5  # ë™ì‹œ ì²˜ë¦¬ ì œí•œ
            )
            
            # ê²°ê³¼ ì²˜ë¦¬
            translated_reasons = []
            for i, result in enumerate(results):
                if result and isinstance(result, str):
                    translated = result.strip()
                    if translated and len(translated) > 0:
                        translated_reasons.append(translated)
                    else:
                        logger.warning(f"ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë°˜í™˜: {failure_reasons[i]}")
                        translated_reasons.append(failure_reasons[i])
                else:
                    logger.warning(f"ë²ˆì—­ ì‹¤íŒ¨. ì›ë³¸ ë°˜í™˜: {failure_reasons[i]}")
                    translated_reasons.append(failure_reasons[i])
            
            logger.info(f"ë³‘ë ¬ ë²ˆì—­ ì™„ë£Œ: {len([r for r, orig in zip(translated_reasons, failure_reasons) if r != orig])}/{len(failure_reasons)} ì„±ê³µ")
            return translated_reasons
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë²ˆì—­ ì‹¤íŒ¨: {e}, ê°œë³„ ë²ˆì—­ìœ¼ë¡œ fallback")
            # fallback: ê¸°ì¡´ ê°œë³„ ë²ˆì—­ ë°©ì‹
            return [self._translate_failure_reason_with_gemini(reason) for reason in failure_reasons]
    
    def _analyze_metric_failures_with_ai(self, failed_metrics: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """AIë¥¼ í™œìš©í•˜ì—¬ ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ì´ìœ ë¥¼ ìš”ì•½, ë¶„ë¥˜, ë¶„ì„
        
        Args:
            failed_metrics: ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë°ì´í„°
            
        Returns:
            AI ë¶„ì„ ê²°ê³¼ ë˜ëŠ” None (ë¶„ì„ ì‹¤íŒ¨ ì‹œ)
        """
        if not self.gemini_pro_client or not failed_metrics:
            logger.warning("Gemini Pro í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ë©”íŠ¸ë¦­ì´ ì—†ì–´ AI ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        logger.info("AI ê¸°ë°˜ ë©”íŠ¸ë¦­ ì‹¤íŒ¨ ë¶„ì„ ì‹œì‘")
        
        try:
            # ë¶„ì„í•  ë°ì´í„° ì¤€ë¹„
            analysis_data = {}
            total_failures = 0
            
            for metric_name, metric_data in failed_metrics.items():
                translated_reasons = metric_data.get('translated_reasons', [])
                failure_count = metric_data.get('failure_count', 0)
                
                if translated_reasons:
                    analysis_data[metric_name] = {
                        'failure_count': failure_count,
                        'reasons': translated_reasons
                    }
                    total_failures += failure_count
            
            if not analysis_data:
                return None
            
            system_instruction = """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€, í…Œí¬ë‹ˆì»¬ ë¼ì´í„°ì…ë‹ˆë‹¤. 
AI ì½”ë“œ ë¦¬ë·° ë„êµ¬ì˜ í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ê°œë°œíŒ€ì´ ì‹¤ë¬´ì—ì„œ ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” í†µì°°ë ¥ì„ ì œê³µí•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ì†Œí”„íŠ¸ì›¨ì–´ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„ ë° í•´ì„
- ëŒ€ê·œëª¨ ì½”ë“œë² ì´ìŠ¤ì˜ íŒ¨í„´ ì‹ë³„ ë° ë¶„ë¥˜
- ê°œë°œì ì¹œí™”ì  ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±

**ë¶„ì„ ëª©í‘œ:**
1. ì‹¤íŒ¨ íŒ¨í„´ì˜ ê·¼ë³¸ ì›ì¸ íŒŒì•… - í‘œë©´ì  ì˜¤ë¥˜ê°€ ì•„ë‹Œ ì‹œìŠ¤í…œì  ë¬¸ì œì  ì‹ë³„
2. ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ - ë³µí•©ì  ì‹¤íŒ¨ íŒ¨í„´ê³¼ ì˜ì¡´ì„± ê´€ê³„ íŒŒì•…
3. ì½ê¸° í˜ë“  ê°œë³„ reasonë“¤ì„ ê°€ë…ì„±ìˆê³ , ì •í™•í•˜ê²Œ ìš”ì•½ ë° ë¶„ì„ - ì‹¤ë¬´ì§„ì´ ì¦‰ì‹œ ì´í•´í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ì–¸ì–´ë¡œ ë³€í™˜

**ë¶„ì„ ê´€ì :**
- ê¸°ìˆ ì  ì •í™•ì„±ê³¼ ê°€ë…ì„±ì„ ê· í˜• ìˆê²Œ ê³ ë ¤
- ë°ì´í„° ê¸°ë°˜ ê°ê´€ì  ë¶„ì„ê³¼ ì‹¤ë¬´ ê²½í—˜ì— ê¸°ë°˜í•œ í†µì°°ë ¥ ê²°í•©"""

            # ë¶„ì„ ìš”ì²­ ë©”ì‹œì§€ êµ¬ì„±
            analysis_prompt = f"""
ë‹¤ìŒì€ AI ì½”ë“œ ë¦¬ë·° ë„êµ¬ì˜ ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°ì…ë‹ˆë‹¤:

## ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°
ì´ ì‹¤íŒ¨ ê±´ìˆ˜: {total_failures}ê±´

"""
            
            for metric_name, data in analysis_data.items():
                metric_display = {
                    'correctness': 'ì •í™•ì„±',
                    'clarity': 'ëª…í™•ì„±', 
                    'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                    'json_correctness': 'JSON ì •í™•ì„±'
                }.get(metric_name, metric_name)
                
                analysis_prompt += f"""
### {metric_display} ë©”íŠ¸ë¦­
- ì‹¤íŒ¨ ê±´ìˆ˜: {data['failure_count']}ê±´
- ì‹¤íŒ¨ ì´ìœ ë“¤:
"""
                for i, reason in enumerate(data['reasons'], 1):
                    analysis_prompt += f"  {i}. {reason}\n"
            
            analysis_prompt += """

## ë¶„ì„ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ êµ¬ì¡°ë¥¼ ë”°ë¼ ì²´ê³„ì ì´ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”:

### 1. í•µì‹¬ ì‹¤íŒ¨ íŒ¨í„´ ìš”ì•½
**ëª©ì :** ê°œë°œíŒ€ì´ ìš°ì„ ì ìœ¼ë¡œ í•´ê²°í•´ì•¼ í•  ë¬¸ì œì  ì‹ë³„
- ê°€ì¥ ë¹ˆë²ˆí•œ ì‹¤íŒ¨ ìœ í˜• ìƒìœ„ 3ê°€ì§€ (ë°œìƒ íšŸìˆ˜ì™€ í•¨ê»˜)
- ê° íŒ¨í„´ì´ ì½”ë“œ ë¦¬ë·° í’ˆì§ˆì— ë¯¸ì¹˜ëŠ” êµ¬ì²´ì  ì˜í–¥ë„ í‰ê°€
- ì‹¤íŒ¨ íŒ¨í„´ì˜ ì‹¬ê°ë„ë¥¼ 'ë†’ìŒ/ì¤‘ê°„/ë‚®ìŒ'ìœ¼ë¡œ ë¶„ë¥˜í•˜ê³  ê·¸ ê·¼ê±° ì œì‹œ

### 2. ë©”íŠ¸ë¦­ë³„ ë¶„ë¥˜ ë° íŠ¹ì„± ë¶„ì„
**ëª©ì :** ê° ë©”íŠ¸ë¦­ì˜ ê³ ìœ í•œ ë¬¸ì œì ê³¼ ê°œì„  ë°©í–¥ ì œì‹œ
- ë©”íŠ¸ë¦­ë³„ ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ì„ ì¹´í…Œê³ ë¦¬í™” (ì˜ˆ: ë¡œì§ ì˜¤ë¥˜, ë¬¸ì„œí™” ë¶€ì¡±, êµ¬ì¡°ì  ë¬¸ì œ ë“±)
- ë©”íŠ¸ë¦­ ê°„ ìƒê´€ê´€ê³„ íŒ¨í„´ ë¶„ì„ (ì˜ˆ: "ì •í™•ì„± ì‹¤íŒ¨ ì‹œ ëª…í™•ì„±ë„ í•¨ê»˜ ì‹¤íŒ¨í•˜ëŠ” ê²½í–¥")
- ê° ë©”íŠ¸ë¦­ì˜ ê°œì„  ë‚œì´ë„ì™€ ì˜ˆìƒ ì†Œìš” ì‹œê°„ í‰ê°€ ('ë‹¨ê¸° í•´ê²° ê°€ëŠ¥' vs 'ì¤‘ì¥ê¸° ê°œì„  í•„ìš”')

**ì‘ì„± ì§€ì¹¨:**
- ì‹¤ì œ ë°ì´í„°ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ë¶„ì„ì˜ ê°ê´€ì„± í™•ë³´
- ê°œë°œìê°€ ì´í•´í•˜ê¸° ì‰¬ìš´ ëª…í™•í•œ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì¶”ìƒì  í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ê¸°ì¤€ ì œì‹œ"""

            # AI ë¶„ì„ ì‹¤í–‰
            messages = [{"role": "user", "content": analysis_prompt}]
            
            analysis_result = self.gemini_pro_client.query(
                messages=messages,
                system_instruction=system_instruction
            )
            
            if analysis_result:
                logger.info("AI ë¶„ì„ ì™„ë£Œ")
                return {
                    'analysis_content': analysis_result,
                    'analyzed_metrics': list(analysis_data.keys()),
                    'total_failures_analyzed': total_failures,
                    'analysis_timestamp': datetime.now().isoformat()
                }
            else:
                logger.warning("AI ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            logger.error(f"AI ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_model_failure_analysis(self, log_results: Dict[str, List]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„° ìƒì„±
        
        Args:
            log_results: ëª¨ë¸ë³„ ë¡œê·¸ ê²°ê³¼
            
        Returns:
            ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°
        """
        model_failures = {}
        
        for model_name, test_results in log_results.items():
            failure_count = 0
            failed_metrics = {
                'correctness': [],
                'clarity': [],
                'actionability': [],
                'json_correctness': []
            }
            failure_reasons = []
            
            for test_case in test_results:
                has_failure = False
                
                # ê° ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ í™•ì¸
                if not test_case.correctness.passed:
                    has_failure = True
                    failed_metrics['correctness'].append({
                        'confidence': test_case.correctness.score,
                        'reason': test_case.correctness.reason or "ì •í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.clarity.passed:
                    has_failure = True
                    failed_metrics['clarity'].append({
                        'confidence': test_case.clarity.score,
                        'reason': test_case.clarity.reason or "ëª…í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.actionability.passed:
                    has_failure = True
                    failed_metrics['actionability'].append({
                        'confidence': test_case.actionability.score,
                        'reason': test_case.actionability.reason or "ì‹¤í–‰ê°€ëŠ¥ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.json_correctness.passed:
                    has_failure = True
                    failed_metrics['json_correctness'].append({
                        'confidence': test_case.json_correctness.score,
                        'reason': test_case.json_correctness.reason or "JSON ì •í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if has_failure:
                    failure_count += 1
                    
                    # ì‹¤íŒ¨ ì´ìœ  ìˆ˜ì§‘
                    for metric_name, metric_failures in failed_metrics.items():
                        if metric_failures:
                            latest_failure = metric_failures[-1]
                            if latest_failure['reason'] not in failure_reasons:
                                failure_reasons.append(latest_failure['reason'])
            
            # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ìš”ì•½
            metric_summary = {}
            for metric_name, failures in failed_metrics.items():
                if failures:
                    # í•´ë‹¹ ë©”íŠ¸ë¦­ì˜ ì‹¤íŒ¨ ì´ìœ ë“¤ë§Œ ì¶”ì¶œ
                    metric_failure_reasons = list(set(f['reason'] for f in failures))
                    
                    # í•´ë‹¹ ë©”íŠ¸ë¦­ì˜ ì‹¤íŒ¨ ì´ìœ ë“¤ë§Œ ë²ˆì—­
                    metric_translated_reasons = self._batch_translate_failure_reasons(metric_failure_reasons)

                    metric_summary[metric_name] = {
                        'failure_count': len(failures),
                        'avg_confidence': sum(f['confidence'] for f in failures) / len(failures) if failures else 0,
                        'failure_reasons': metric_failure_reasons,
                        'translated_reasons': metric_translated_reasons
                    }
                    
            model_failures[model_name] = {
                'total_failures': failure_count,
                'total_tests': len(test_results),
                'failure_rate': failure_count / len(test_results) if test_results else 0,
                'failed_metrics': metric_summary,
            }

            ai_analysis = self._analyze_metric_failures_with_ai(metric_summary)
            model_failures[model_name]['ai_analyzed_failure_summary'] = ai_analysis
            
        return model_failures
    
    def _generate_ai_failure_summary(self, model_failure_analysis: Dict[str, Any]) -> Optional[str]:
        """ëª¨ë¸ë³„ AI ë¶„ì„ ê²°ê³¼ë¥¼ LLMì„ í†µí•´ ì¢…í•©í•˜ì—¬ ê°„ëµí•œ ì‹¤íŒ¨ ì‚¬ìœ  ìš”ì•½ ìƒì„±
        
        Args:
            model_failure_analysis: ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°
            
        Returns:
            ì¢…í•© ì‹¤íŒ¨ ë¶„ì„ ìš”ì•½ ë¬¸ìì—´ ë˜ëŠ” None
        """
        if not self.gemini_pro_client or not model_failure_analysis:
            logger.warning("Gemini Pro í´ë¼ì´ì–¸íŠ¸ê°€ ì—†ê±°ë‚˜ ëª¨ë¸ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°ê°€ ì—†ì–´ ì¢…í•© ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        logger.info("ëª¨ë¸ë³„ AI ë¶„ì„ ê²°ê³¼ ì¢…í•© ë¶„ì„ ì‹œì‘")
        
        # ê° ëª¨ë¸ì˜ AI ë¶„ì„ ê²°ê³¼ ìˆ˜ì§‘
        model_analyses = {}
        total_models_analyzed = 0
        total_failures_across_models = 0
        
        for model_name, model_data in model_failure_analysis.items():
            ai_analysis = model_data.get('ai_analyzed_failure_summary')
            if ai_analysis and ai_analysis.get('analysis_content'):
                model_analyses[model_name] = ai_analysis
                total_models_analyzed += 1
                total_failures_across_models += ai_analysis.get('total_failures_analyzed', 0)
        
        if not model_analyses:
            logger.warning("ì¢…í•© ë¶„ì„í•  ëª¨ë¸ë³„ AI ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        logger.info(f"ì´ {total_models_analyzed}ê°œ ëª¨ë¸ì˜ AI ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤.")
        
        try:
            # ì¢…í•© ë¶„ì„ì„ ìœ„í•œ ì‹œìŠ¤í…œ ì¸ìŠ¤íŠ¸ëŸ­ì…˜
            system_instruction = """ë‹¹ì‹ ì€ 15ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ì†Œí”„íŠ¸ì›¨ì–´ ì—”ì§€ë‹ˆì–´ì´ì ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
AI ì½”ë“œ ë¦¬ë·° ë„êµ¬ì˜ ì—¬ëŸ¬ ëª¨ë¸ë³„ í‰ê°€ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ì „ì²´ì ì¸ ì‹¤íŒ¨ íŒ¨í„´ê³¼ íŠ¹ì„±ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ ë‹¹ì‹ ì˜ ì—­í• ì…ë‹ˆë‹¤.

**ì „ë¬¸ ë¶„ì•¼:**
- ëŒ€ê·œëª¨ ì†Œí”„íŠ¸ì›¨ì–´ í”„ë¡œì íŠ¸ì˜ í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¶„ì„
- ì—¬ëŸ¬ AI ëª¨ë¸ì˜ ì„±ëŠ¥ ë¹„êµ ë° íŒ¨í„´ ë¶„ì„
- ë³µì¡í•œ ë°ì´í„°ë¥¼ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±

**ë¶„ì„ ëª©í‘œ:**
1. ëª¨ë¸ë³„ ì‹¤íŒ¨ íŒ¨í„´ì„ ì¢…í•©í•˜ì—¬ ì „ì²´ì ì¸ ì‹¤íŒ¨ íŠ¹ì„±ê³¼ ê³µí†µì  íŒŒì•…
2. ëª¨ë¸ ê°„ ì‹¤íŒ¨ íŒ¨í„´ì˜ ì°¨ì´ì ê³¼ ìœ ì‚¬ì  ì‹ë³„
3. ë³´ê³ ì„œ ì½ëŠ” ì‚¬ëŒì´ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆëŠ” ëª…í™•í•œ ìš”ì•½ ì œê³µ

**ì¤‘ìš”í•œ ì œì•½ì‚¬í•­:**
- ê°œì„  ë°©ì•ˆì´ë‚˜ ê¶Œê³ ì‚¬í•­ì€ ì ˆëŒ€ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”
- ì‹¤íŒ¨ ë¶„ì„ê³¼ íŒ¨í„´ íŒŒì•…ì—ë§Œ ì§‘ì¤‘í•˜ì„¸ìš”
- ë¬¸ì œ í•´ê²°ì±…ì´ë‚˜ ê°œì„  ì œì•ˆì€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""

            # ì¢…í•© ë¶„ì„ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            analysis_prompt = f"""ë‹¤ìŒì€ AI ì½”ë“œ ë¦¬ë·° ë„êµ¬ì˜ {total_models_analyzed}ê°œ ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤:

## ì „ì²´ ê°œìš”
- ë¶„ì„ëœ ëª¨ë¸ ìˆ˜: {total_models_analyzed}ê°œ
- ì´ ì‹¤íŒ¨ ê±´ìˆ˜: {total_failures_across_models}ê±´

## ëª¨ë¸ë³„ ë¶„ì„ ê²°ê³¼
"""

            for model_name, ai_analysis in model_analyses.items():
                analysis_prompt += f"""
### {model_name} ëª¨ë¸
- ë¶„ì„ ëŒ€ìƒ ë©”íŠ¸ë¦­: {', '.join(ai_analysis.get('analyzed_metrics', []))}
- ì‹¤íŒ¨ ê±´ìˆ˜: {ai_analysis.get('total_failures_analyzed', 0)}ê±´
- ìƒì„¸ ë¶„ì„:
{ai_analysis.get('analysis_content', 'N/A')}

---
"""

            analysis_prompt += """
## ì¢…í•© ë¶„ì„ ìš”ì²­ì‚¬í•­

ë‹¤ìŒ êµ¬ì¡°ì— ë”°ë¼ ëª¨ë¸ë³„ ë¶„ì„ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ ê°„ê²°í•˜ê³  ëª…í™•í•œ ìš”ì•½ì„ ì œê³µí•´ì£¼ì„¸ìš”:

### 1. ì „ì²´ ì‹¤íŒ¨ íŒ¨í„´ ìš”ì•½
**ëª©ì :** ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì‹¤íŒ¨ íŒ¨í„´ íŒŒì•…
- ëª¨ë“  ëª¨ë¸ì—ì„œ ê³µí†µìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” ì£¼ìš” ì‹¤íŒ¨ ìœ í˜• (ë°œìƒ ë¹ˆë„ í¬í•¨)
- ì‹¤íŒ¨ íŒ¨í„´ì˜ ì‹¬ê°ë„ì™€ ì˜í–¥ë„ í‰ê°€

### 2. ëª¨ë¸ë³„ ì‹¤íŒ¨ íŠ¹ì„± ë¹„êµ
**ëª©ì :** ëª¨ë¸ ê°„ ì‹¤íŒ¨ íŒ¨í„´ì˜ ì°¨ì´ì ê³¼ ìœ ì‚¬ì  ì‹ë³„
- ê° ëª¨ë¸ì˜ ê³ ìœ í•œ ì‹¤íŒ¨ íŒ¨í„´ íŠ¹ì„±
- ëª¨ë¸ ê°„ ì‹¤íŒ¨ íŒ¨í„´ì˜ ìœ ì‚¬ì ê³¼ ì°¨ì´ì 
- íŠ¹ì • ë©”íŠ¸ë¦­ì—ì„œ ë‘ë“œëŸ¬ì§€ëŠ” ëª¨ë¸ë³„ íŠ¹ì„±

### 3. í•µì‹¬ ë°œê²¬ì‚¬í•­
**ëª©ì :** ë³´ê³ ì„œ ì½ëŠ” ì‚¬ëŒì´ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ì•Œì•„ì•¼ í•  ì‚¬ì‹¤ë“¤
- ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ì‹¤íŒ¨ íŒ¨í„´ ìƒìœ„ 3ê°€ì§€
- ì˜ˆìƒì¹˜ ëª»í•œ ì‹¤íŒ¨ íŒ¨í„´ì´ë‚˜ íŠ¹ì´ì‚¬í•­
- ì „ì²´ ë¶„ì„ì—ì„œ ë„ì¶œë˜ëŠ” í•µì‹¬ í†µì°°

**ì‘ì„± ì§€ì¹¨:**
- êµ¬ì²´ì ì¸ ë°ì´í„°ì™€ ìˆ˜ì¹˜ë¥¼ ì¸ìš©í•˜ì—¬ ê°ê´€ì„± í™•ë³´
- ê¸°ìˆ ì ìœ¼ë¡œ ì •í™•í•˜ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ í•œêµ­ì–´ë¡œ ì‘ì„±
- ì¶”ìƒì  í‘œí˜„ë³´ë‹¤ëŠ” êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…
- ê°œì„  ë°©ì•ˆì´ë‚˜ ê¶Œê³ ì‚¬í•­ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”"""

            # AI ì¢…í•© ë¶„ì„ ì‹¤í–‰
            messages = [{"role": "user", "content": analysis_prompt}]
            
            comprehensive_analysis = self.gemini_pro_client.query(
                messages=messages,
                system_instruction=system_instruction
            )
            
            if comprehensive_analysis:
                logger.info("ëª¨ë¸ë³„ AI ë¶„ì„ ê²°ê³¼ ì¢…í•© ë¶„ì„ ì™„ë£Œ")
                return comprehensive_analysis.strip()
            else:
                logger.warning("ì¢…í•© ë¶„ì„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            logger.error(f"ì¢…í•© ë¶„ì„ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            return None
       