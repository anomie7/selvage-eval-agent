"""DeepEval ë¶„ì„ ì—”ì§„ V2

ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µí•©ëœ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.
ìƒˆë¡œìš´ ë¶„ì„ í´ë˜ìŠ¤ë“¤ì„ í†µí•©í•˜ì—¬ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
import logging

from .deepeval_log_parser import DeepEvalLogParser
from .metric_aggregator import MetricAggregator
from .failure_pattern_analyzer import FailurePatternAnalyzer
from .model_performance_comparator import ModelPerformanceComparator
from .tech_stack_analyzer import TechStackAnalyzer
from .version_comparison_analyzer import VersionComparisonAnalyzer
from .visualization_generator import VisualizationGenerator

logger = logging.getLogger(__name__)


class DeepEvalAnalysisEngine:
    """DeepEval ë¶„ì„ ì—”ì§„ V2 - ë¡œê·¸ íŒŒì¼ ê¸°ë°˜"""
    
    def __init__(self, output_dir: str = "~/Library/selvage-eval/analyze_results"):
        """ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            output_dir: ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.log_parser = DeepEvalLogParser()
        self.metric_aggregator = MetricAggregator()
        self.failure_analyzer = FailurePatternAnalyzer()
        self.model_comparator = ModelPerformanceComparator()
        self.tech_stack_analyzer = TechStackAnalyzer()
        self.version_analyzer = VersionComparisonAnalyzer()
        self.visualizer = VisualizationGenerator()
    
    def analyze_session(self, session_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì„¸ì…˜ ë¶„ì„ ì‹¤í–‰
        
        Args:
            session_path: DeepEval ë¡œê·¸ íŒŒì¼ì´ ìˆëŠ” ì„¸ì…˜ ê²½ë¡œ
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        session_path_obj = Path(session_path).expanduser()
        
        if not session_path_obj.exists():
            raise FileNotFoundError(f"ì„¸ì…˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            session_id = session_path_obj.name
            final_output_dir = self.output_dir / session_id
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ ê²°ê³¼ ìˆ˜ì§‘
        log_results = self._collect_log_results(session_path_obj)
        
        if not log_results:
            raise ValueError("DeepEval ë¡œê·¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ì¢…í•© ë¶„ì„ ì‹¤í–‰
        analysis_results = self._perform_comprehensive_analysis(log_results)
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report = self._generate_markdown_report(analysis_results)
        report_path = final_output_dir / "analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        # JSON ë°ì´í„° ì €ì¥
        json_path = final_output_dir / "analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                analysis_results, 
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            summary_report = self.visualizer.create_summary_report(
                analysis_results,
                final_output_dir / "summary_report.html"
            )
            if summary_report:
                visualization_files.append(summary_report)
                
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "analysis_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "session_path": str(session_path_obj),
                "total_test_cases": analysis_results.get("data_summary", {}).get("total_test_cases", 0),
                "models_analyzed": list(analysis_results.get("model_comparison", {}).get("model_statistics", {}).keys())
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def analyze_multiple_sessions(self, base_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì„¸ì…˜ í†µí•© ë¶„ì„ (ë²„ì „ ë¹„êµ í¬í•¨)
        
        Args:
            base_path: ì—¬ëŸ¬ ì„¸ì…˜ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        base_path_obj = Path(base_path).expanduser()
        
        if not base_path_obj.exists():
            raise FileNotFoundError(f"ê¸°ë³¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            final_output_dir = self.output_dir / "multi_session_analysis"
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë²„ì „ë³„ ë°ì´í„° ìˆ˜ì§‘
        version_data = self.version_analyzer.collect_version_data(str(base_path_obj))
        
        if not version_data:
            raise ValueError("ë¶„ì„í•  ë²„ì „ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë²„ì „ ë¹„êµ ë¶„ì„
        version_analysis = self.version_analyzer.analyze_version_progression(version_data)
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        repo_results = self._prepare_repo_results_from_versions(version_data)
        tech_stack_analysis = self.tech_stack_analyzer.analyze_tech_stack_performance(repo_results)
        
        # í†µí•© ë¶„ì„ ê²°ê³¼
        integrated_results = {
            "analysis_type": "multi_session",
            "version_analysis": version_analysis,
            "tech_stack_analysis": tech_stack_analysis,
            "data_summary": {
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values()),
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report = self._generate_multi_session_markdown_report(integrated_results)
        report_path = final_output_dir / "multi_session_analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        # JSON ë°ì´í„° ì €ì¥
        json_path = final_output_dir / "multi_session_analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(integrated_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                integrated_results,
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
        except Exception as e:
            logger.error(f"ë‹¤ì¤‘ ì„¸ì…˜ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "analysis_metadata": {
                "analysis_type": "multi_session",
                "analysis_timestamp": datetime.now().isoformat(),
                "base_path": str(base_path_obj),
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values())
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def _collect_log_results(self, session_path: Path) -> Dict[str, List]:
        """ë¡œê·¸ íŒŒì¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        
        # .log íŒŒì¼ë“¤ ì°¾ê¸°
        for log_file in session_path.glob("**/*.log"):
            try:
                # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
                model_name = self._extract_model_name_from_path(log_file)
                
                # ë¡œê·¸ íŒŒì‹±
                test_results = list(self.log_parser.parse_log_file(log_file))
                
                if test_results:
                    if model_name not in results:
                        results[model_name] = []
                    results[model_name].extend(test_results)
                    
            except Exception as e:
                logger.warning(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {log_file} - {e}")
        
        return results
    
    def _extract_model_name_from_path(self, log_file: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
        
        Args:
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ëª¨ë¸ëª…
        """
        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ëª… ì¶”ì¶œ ì‹œë„
        file_name = log_file.stem
        
        # deepeval_results_model_name.log íŒ¨í„´
        if file_name.startswith('deepeval_results_'):
            parts = file_name.split('_')
            if len(parts) >= 3:
                return '_'.join(parts[2:])
        
        # model_name.log íŒ¨í„´
        if '_' in file_name:
            return file_name.split('_')[0]
        
        # ê¸°ë³¸ê°’
        return file_name or "unknown_model"
    
    def _perform_comprehensive_analysis(self, log_results: Dict[str, List]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        
        Args:
            log_results: ëª¨ë¸ë³„ ë¡œê·¸ ê²°ê³¼
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        if not log_results:
            return {"error": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
            model_comparison = self.model_comparator.compare_models(log_results)
            
            # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ - ëª¨ë“  ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘
            all_failed_cases = []
            for model_results in log_results.values():
                for test_case in model_results:
                    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¡œ ê°„ì£¼
                    has_failure = (
                        not test_case.correctness.passed or
                        not test_case.clarity.passed or
                        not test_case.actionability.passed or
                        not test_case.json_correctness.passed
                    )
                    if has_failure:
                        all_failed_cases.append(test_case)
            
            failure_analysis = self.failure_analyzer.analyze_failure_patterns(all_failed_cases)
            
            # ë°ì´í„° ìš”ì•½
            total_test_cases = sum(len(results) for results in log_results.values())
            successful_cases = total_test_cases - len(all_failed_cases)
            
            return {
                "analysis_type": "single_session", 
                "model_comparison": model_comparison,
                "failure_analysis": failure_analysis,
                "data_summary": {
                    "total_test_cases": total_test_cases,
                    "successful_evaluations": successful_cases,
                    "failed_evaluations": len(all_failed_cases),
                    "models_count": len(log_results),
                    "analysis_timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}
    
    def _prepare_repo_results_from_versions(self, version_data: Dict[str, Any]) -> Dict[str, Dict[str, List]]:
        """ë²„ì „ ë°ì´í„°ì—ì„œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„
        
        Args:
            version_data: ë²„ì „ë³„ ë°ì´í„°
            
        Returns:
            ì €ì¥ì†Œë³„ ëª¨ë¸ ê²°ê³¼
        """
        repo_results = {}
        
        for version, data in version_data.items():
            for session in data['sessions']:
                session_dir = Path(session['session_dir'])
                
                # ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                repo_name = self._extract_repo_name_from_session_path(session_dir)
                
                if repo_name not in repo_results:
                    repo_results[repo_name] = {}
                
                # ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í•‘
                session_results = session.get('results', [])
                if session_results:
                    model_name = f"version_{version}"
                    if model_name not in repo_results[repo_name]:
                        repo_results[repo_name][model_name] = []
                    repo_results[repo_name][model_name].extend(session_results)
        
        return repo_results
    
    def _extract_repo_name_from_session_path(self, session_path: Path) -> str:
        """ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ì €ì¥ì†Œëª…
        """
        # ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª…ìœ¼ë¡œ ë³´ì´ëŠ” ë¶€ë¶„ ì¶”ì¶œ
        path_parts = session_path.parts
        
        # ì¼ë°˜ì ì¸ ì €ì¥ì†Œëª… íŒ¨í„´ ì°¾ê¸°
        for part in reversed(path_parts):
            if any(keyword in part.lower() for keyword in ['cline', 'ecommerce', 'kotlin', 'selvage']):
                return part
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ë§ˆì§€ë§‰ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        return session_path.name or "unknown_repo"
    
    def _generate_markdown_report(self, analysis_results: Dict[str, Any]) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            analysis_results: ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        if "error" in analysis_results:
            return f"# ë¶„ì„ ì˜¤ë¥˜\n\n{analysis_results['error']}"
        
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# DeepEval ë¶„ì„ ë³´ê³ ì„œ (V2)",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ë¶„ì„ ìœ í˜•**: {analysis_results.get('analysis_type', 'single_session')}",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = analysis_results.get("data_summary", {})
        if data_summary:
            lines.extend([
                "## ğŸ“Š ë°ì´í„° ìš”ì•½",
                "",
                f"- **ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {data_summary.get('total_test_cases', 0)}ê°œ",
                f"- **ì„±ê³µí•œ í‰ê°€**: {data_summary.get('successful_evaluations', 0)}ê°œ",
                f"- **ì‹¤íŒ¨í•œ í‰ê°€**: {data_summary.get('failed_evaluations', 0)}ê°œ",
                f"- **ë¶„ì„ëœ ëª¨ë¸**: {data_summary.get('models_count', 0)}ê°œ",
                ""
            ])
        
        # ëª¨ë¸ ë¹„êµ ê²°ê³¼
        model_comparison = analysis_results.get("model_comparison", {})
        if model_comparison and model_comparison.get("recommendations"):
            lines.extend([
                "## ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ",
                "",
                "### ì£¼ìš” ê¶Œì¥ì‚¬í•­",
                ""
            ])
            
            for rec in model_comparison["recommendations"][:10]:  # ìƒìœ„ 10ê°œ
                lines.append(f"- {rec}")
            
            lines.append("")
            
            # ë¹„êµ í‘œ
            comparison_table = model_comparison.get("comparison_table", {})
            table_data = comparison_table.get("table_data", [])
            
            if table_data:
                lines.extend([
                    "### ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½",
                    "",
                    "| ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë“±ê¸‰ | ìˆœìœ„ | ì •í™•ì„± | ëª…í™•ì„± | ì‹¤í–‰ê°€ëŠ¥ì„± | JSON ì •í™•ì„± |",
                    "|--------|----------|------|------|--------|--------|------------|-------------|"
                ])
                
                for model_data in table_data:
                    lines.append(
                        f"| {model_data.get('model_name', 'N/A')} | "
                        f"{model_data.get('overall_score', 0):.3f} | "
                        f"{model_data.get('grade', 'N/A')} | "
                        f"{model_data.get('overall_rank', 'N/A')} | "
                        f"{model_data.get('correctness_score', 0):.3f} | "
                        f"{model_data.get('clarity_score', 0):.3f} | "
                        f"{model_data.get('actionability_score', 0):.3f} | "
                        f"{model_data.get('json_correctness_score', 0):.3f} |"
                    )
                
                lines.append("")
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_analysis = analysis_results.get("failure_analysis", {})
        if failure_analysis and failure_analysis.get("total_failures", 0) > 0:
            lines.extend([
                "## ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„",
                "",
                f"**ì´ ì‹¤íŒ¨ ê±´ìˆ˜**: {failure_analysis['total_failures']}ê°œ",
                ""
            ])
            
            # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„ì„
            by_metric = failure_analysis.get("by_metric", {})
            if by_metric:
                lines.extend([
                    "### ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ í˜„í™©",
                    "",
                    "| ë©”íŠ¸ë¦­ | ì‹¤íŒ¨ ê±´ìˆ˜ | ì‹¤íŒ¨ìœ¨ | í‰ê·  ì‹ ë¢°ë„ |",
                    "|--------|-----------|--------|-------------|"
                ])
                
                for metric, data in by_metric.items():
                    lines.append(
                        f"| {metric.replace('_', ' ').title()} | "
                        f"{data.get('total_failures', 0)} | "
                        f"{data.get('failure_rate', 0):.1%} | "
                        f"{data.get('avg_confidence', 0):.3f} |"
                    )
                
                lines.append("")
            
            # ì¤‘ìš”í•œ íŒ¨í„´
            critical_patterns = failure_analysis.get("critical_patterns", [])
            if critical_patterns:
                lines.extend([
                    "### ì¤‘ìš”í•œ ì‹¤íŒ¨ íŒ¨í„´",
                    ""
                ])
                
                for pattern in critical_patterns[:5]:  # ìƒìœ„ 5ê°œ
                    lines.append(
                        f"- **{pattern.get('category', 'Unknown')}**: "
                        f"{pattern.get('count', 0)}ê±´ ({pattern.get('percentage', 0):.1f}%) "
                        f"- {pattern.get('reason', '')}"
                    )
                
                lines.append("")
        
        # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
        lines.extend([
            "## ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­",
            "",
            "### ì£¼ìš” ë°œê²¬ì‚¬í•­",
            ""
        ])
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­ ì¢…í•©
        all_recommendations = []
        if model_comparison.get("recommendations"):
            all_recommendations.extend(model_comparison["recommendations"][:3])
        
        if failure_analysis.get("critical_patterns"):
            all_recommendations.append(
                f"ì‹¤íŒ¨ íŒ¨í„´ ê°œì„  í•„ìš”: {len(failure_analysis['critical_patterns'])}ê°œì˜ ì¤‘ìš”í•œ íŒ¨í„´ ë°œê²¬"
            )
        
        for rec in all_recommendations:
            lines.append(f"1. {rec}")
        
        lines.extend([
            "",
            "### ë‹¤ìŒ ë‹¨ê³„",
            "",
            "- ì„±ëŠ¥ì´ ë‚®ì€ ë©”íŠ¸ë¦­ì— ëŒ€í•œ ëª¨ë¸ ê°œì„ ",
            "- ì‹¤íŒ¨ íŒ¨í„´ì´ ë§ì€ ì˜ì—­ì˜ í”„ë¡¬í”„íŠ¸ ìµœì í™”",
            "- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ íŠ¹ì„±ì„ ë‹¤ë¥¸ ëª¨ë¸ì— ì ìš©",
            "- ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¶”ì ",
            ""
        ])
        
        return "\n".join(lines)
    
    def _generate_multi_session_markdown_report(self, integrated_results: Dict[str, Any]) -> str:
        """ë‹¤ì¤‘ ì„¸ì…˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            integrated_results: í†µí•© ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# ë‹¤ì¤‘ ì„¸ì…˜ ë¶„ì„ ë³´ê³ ì„œ",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "**ë¶„ì„ ìœ í˜•**: ë²„ì „ ë¹„êµ ë° ê¸°ìˆ ìŠ¤íƒ ë¶„ì„",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = integrated_results.get("data_summary", {})
        lines.extend([
            "## ğŸ“Š ë¶„ì„ ê°œìš”",
            "",
            f"- **ë¶„ì„ëœ ë²„ì „**: {data_summary.get('total_versions', 0)}ê°œ",
            f"- **ì´ ì„¸ì…˜**: {data_summary.get('total_sessions', 0)}ê°œ",
            ""
        ])
        
        # ë²„ì „ ë¶„ì„
        version_analysis = integrated_results.get("version_analysis", {})
        if version_analysis:
            lines.extend([
                "## ğŸ“ˆ ë²„ì „ë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            # ë²„ì „ ê¶Œì¥ì‚¬í•­
            version_recommendations = version_analysis.get("version_recommendations", {})
            if version_recommendations:
                recommended_version = version_recommendations.get("recommended_version")
                if recommended_version:
                    lines.append(f"**ê¶Œì¥ ë²„ì „**: {recommended_version}")
                    lines.append("")
                
                recommendations = version_recommendations.get("recommendations", [])
                if recommendations:
                    lines.append("### ë²„ì „ ê´€ë ¨ ê¶Œì¥ì‚¬í•­")
                    lines.append("")
                    for rec in recommendations:
                        lines.append(f"- {rec}")
                    lines.append("")
        
        # ê¸°ìˆ ìŠ¤íƒ ë¶„ì„
        tech_stack_analysis = integrated_results.get("tech_stack_analysis", {})
        if tech_stack_analysis:
            lines.extend([
                "## ğŸ›  ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            tech_recommendations = tech_stack_analysis.get("recommendations", [])
            if tech_recommendations:
                for rec in tech_recommendations[:10]:  # ìƒìœ„ 10ê°œ
                    lines.append(f"- {rec}")
                lines.append("")
        
        lines.extend([
            "## ğŸ’¡ í†µí•© ê¶Œì¥ì‚¬í•­",
            "",
            "### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ",
            "- ìµœì‹  ì•ˆì • ë²„ì „ìœ¼ë¡œì˜ ì—…ê·¸ë ˆì´ë“œ ê²€í† ",
            "- ì„±ëŠ¥ íšŒê·€ê°€ ìˆëŠ” ë²„ì „ ì‚¬ìš© ì¤‘ë‹¨",
            "",
            "### ê¸°ìˆ ìŠ¤íƒ ìµœì í™”", 
            "- ê° ê¸°ìˆ ìŠ¤íƒì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ",
            "- ì„±ëŠ¥ì´ ë‚®ì€ ê¸°ìˆ ìŠ¤íƒì˜ í”„ë¡¬í”„íŠ¸ ê°œì„ ",
            "",
            "### ì§€ì†ì  ëª¨ë‹ˆí„°ë§",
            "- ì •ê¸°ì ì¸ ë²„ì „ë³„ ì„±ëŠ¥ ì¶”ì ",
            "- ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹",
            ""
        ])
        
        return "\n".join(lines)