"""DeepEval ë¶„ì„ ì—”ì§„ V2

ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ DeepEval í‰ê°€ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í†µí•©ëœ ë³´ê³ ì„œë¥¼ ìƒì„±í•˜ëŠ” ì—”ì§„ì…ë‹ˆë‹¤.
ìƒˆë¡œìš´ ë¶„ì„ í´ë˜ìŠ¤ë“¤ì„ í†µí•©í•˜ì—¬ í¬ê´„ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import json
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
import logging
import os

from selvage_eval.llm.gemini_client import GeminiClient

from .deepeval_log_parser import DeepEvalLogParser
from .metric_aggregator import MetricAggregator
from .failure_pattern_analyzer import FailurePatternAnalyzer
from .model_performance_comparator import ModelPerformanceComparator
from .tech_stack_analyzer import TechStackAnalyzer
from .version_comparison_analyzer import VersionComparisonAnalyzer
from .visualization_generator import VisualizationGenerator

logger = logging.getLogger(__name__)


class DeepEvalAnalysisEngine:
    """DeepEval ë¶„ì„ ì—”ì§„"""
    
    def __init__(self, output_dir: str = "~/Library/selvage-eval/analyze_results"):
        """ë¶„ì„ ì—”ì§„ ì´ˆê¸°í™”
        
        Args:
            output_dir: ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = Path(output_dir).expanduser()
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¶„ì„ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.log_parser = DeepEvalLogParser()
        self.metric_aggregator = MetricAggregator()
        self.failure_analyzer = FailurePatternAnalyzer()
        self.model_comparator = ModelPerformanceComparator()
        self.tech_stack_analyzer = TechStackAnalyzer()
        self.version_analyzer = VersionComparisonAnalyzer()
        self.visualizer = VisualizationGenerator()
        
        # Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._init_gemini_client()
    
    def analyze_session(self, session_id: str, 
                       deepeval_results_path: Optional[str] = None,
                       output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì„¸ì…˜ ë¶„ì„ ì‹¤í–‰
        
        Args:
            session_id: ì„¸ì…˜ ID
            deepeval_results_path: DeepEval ê²°ê³¼ ê²½ë¡œ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ session_idë¡œ ìë™ ê²€ìƒ‰)
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„ íƒì‚¬í•­)
            
        Returns:
            ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        start_time = time.time()
        logger.info(f"=== DeepEval ì„¸ì…˜ ë¶„ì„ ì‹œì‘: {session_id} ===")
        
        # 1ë‹¨ê³„: DeepEval ê²°ê³¼ ê²½ë¡œ í™•ì¸
        logger.info("1ë‹¨ê³„: ì„¸ì…˜ ê²½ë¡œ í™•ì¸ ë° ì„¤ì • ì¤‘...")
        if deepeval_results_path:
            session_path_obj = Path(deepeval_results_path).expanduser()
        else:
            default_base_path = Path("/Users/demin_coder/Library/selvage-eval/deepeval_results")
            session_path_obj = default_base_path / session_id
            
        if not session_path_obj.exists():
            raise FileNotFoundError(f"ì„¸ì…˜ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {session_path_obj}")
        
        logger.info(f"ì„¸ì…˜ ê²½ë¡œ í™•ì¸ ì™„ë£Œ: {session_path_obj}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            session_id = session_path_obj.name
            final_output_dir = self.output_dir / session_id
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì • ì™„ë£Œ: {final_output_dir}")
        
        # 2ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ ê¸°ë°˜ ê²°ê³¼ ìˆ˜ì§‘
        logger.info("2ë‹¨ê³„: ë¡œê·¸ íŒŒì¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘ ì¤‘...")
        step_start = time.time()
        log_results = self._collect_log_results(session_path_obj)
        
        if not log_results:
            raise ValueError("DeepEval ë¡œê·¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        total_cases = sum(len(results) for results in log_results.values())
        logger.info(f"ë¡œê·¸ ê²°ê³¼ ìˆ˜ì§‘ ì™„ë£Œ - ëª¨ë¸ {len(log_results)}ê°œ, ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {total_cases}ê°œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 3ë‹¨ê³„: ì¢…í•© ë¶„ì„ ì‹¤í–‰
        logger.info("3ë‹¨ê³„: ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
        step_start = time.time()
        analysis_results = self._perform_comprehensive_analysis(log_results)
        logger.info(f"ì¢…í•© ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 4ë‹¨ê³„: ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„° ì¶”ê°€ (Gemini ë²ˆì—­ í¬í•¨)
        logger.info("4ë‹¨ê³„: ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë° Gemini ë²ˆì—­ ìˆ˜í–‰ ì¤‘...")
        step_start = time.time()
        model_failure_analysis = self._generate_model_failure_analysis(log_results)
        analysis_results["model_failure_analysis"] = model_failure_analysis
        logger.info(f"ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 5ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        logger.info("5ë‹¨ê³„: ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì¤‘...")
        step_start = time.time()
        markdown_report = self._generate_markdown_report(analysis_results)
        report_path = final_output_dir / "analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        logger.info(f"ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± ì™„ë£Œ: {report_path} (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 6ë‹¨ê³„: JSON ë°ì´í„° ì €ì¥
        logger.info("6ë‹¨ê³„: JSON ë°ì´í„° ì €ì¥ ì¤‘...")
        step_start = time.time()
        json_path = final_output_dir / "analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(analysis_results, f, ensure_ascii=False, indent=2, default=str)
        logger.info(f"JSON ë°ì´í„° ì €ì¥ ì™„ë£Œ: {json_path} (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
        
        # 7ë‹¨ê³„: ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        logger.info("7ë‹¨ê³„: ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„± ì¤‘...")
        step_start = time.time()
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                analysis_results, 
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
            # ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
            summary_report = self.visualizer.create_summary_report(
                analysis_results,
                final_output_dir / "summary_report.html"
            )
            if summary_report:
                visualization_files.append(summary_report)
            
            logger.info(f"ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„± ì™„ë£Œ - {len(visualization_files)}ê°œ íŒŒì¼ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
                
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        total_time = time.time() - start_time
        logger.info(f"=== DeepEval ì„¸ì…˜ ë¶„ì„ ì™„ë£Œ: {session_id} (ì´ ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ) ===")
        
        return {
            "analysis_metadata": {
                "analysis_timestamp": datetime.now().isoformat(),
                "session_path": str(session_path_obj),
                "total_test_cases": analysis_results.get("data_summary", {}).get("total_test_cases", 0),
                "models_analyzed": list(analysis_results.get("model_comparison", {}).get("model_statistics", {}).keys()),
                "total_processing_time": total_time
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def analyze_multiple_sessions(self, base_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """ì—¬ëŸ¬ ì„¸ì…˜ í†µí•© ë¶„ì„ (ë²„ì „ ë¹„êµ í¬í•¨)
        
        Args:
            base_path: ì—¬ëŸ¬ ì„¸ì…˜ì´ ìˆëŠ” ê¸°ë³¸ ê²½ë¡œ
            output_dir: ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            í†µí•© ë¶„ì„ ê²°ê³¼ ë©”íƒ€ë°ì´í„°
        """
        base_path_obj = Path(base_path).expanduser()
        
        if not base_path_obj.exists():
            raise FileNotFoundError(f"ê¸°ë³¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {base_path}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        if output_dir:
            final_output_dir = Path(output_dir).expanduser()
        else:
            final_output_dir = self.output_dir / "multi_session_analysis"
        
        final_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ë²„ì „ë³„ ë°ì´í„° ìˆ˜ì§‘
        version_data = self.version_analyzer.collect_version_data(str(base_path_obj))
        
        if not version_data:
            raise ValueError("ë¶„ì„í•  ë²„ì „ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë²„ì „ ë¹„êµ ë¶„ì„
        version_analysis = self.version_analyzer.analyze_version_progression(version_data)
        
        # ê¸°ìˆ ìŠ¤íƒë³„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„
        repo_results = self._prepare_repo_results_from_versions(version_data)
        tech_stack_analysis = self.tech_stack_analyzer.analyze_tech_stack_performance(repo_results)
        
        # í†µí•© ë¶„ì„ ê²°ê³¼
        integrated_results = {
            "analysis_type": "multi_session",
            "version_analysis": version_analysis,
            "tech_stack_analysis": tech_stack_analysis,
            "data_summary": {
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values()),
                "analysis_timestamp": datetime.now().isoformat()
            }
        }
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        markdown_report = self._generate_multi_session_markdown_report(integrated_results)
        report_path = final_output_dir / "multi_session_analysis_report.md"
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        # JSON ë°ì´í„° ì €ì¥
        json_path = final_output_dir / "multi_session_analysis_data.json"
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(integrated_results, f, ensure_ascii=False, indent=2, default=str)
        
        # ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±
        visualization_files = []
        try:
            viz_files = self.visualizer.generate_comprehensive_dashboard(
                integrated_results,
                str(final_output_dir / "visualizations")
            )
            visualization_files.extend(viz_files)
            
        except Exception as e:
            logger.error(f"ë‹¤ì¤‘ ì„¸ì…˜ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        
        return {
            "analysis_metadata": {
                "analysis_type": "multi_session",
                "analysis_timestamp": datetime.now().isoformat(),
                "base_path": str(base_path_obj),
                "total_versions": len(version_data),
                "total_sessions": sum(len(data['sessions']) for data in version_data.values())
            },
            "files_generated": {
                "markdown_report": str(report_path),
                "json_data": str(json_path),
                "visualization_files": visualization_files
            }
        }
    
    def _collect_log_results(self, session_path: Path) -> Dict[str, List]:
        """ë¡œê·¸ íŒŒì¼ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ëª¨ë¸ë³„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        results = {}
        log_files = list(session_path.glob("**/*.log"))
        logger.info(f"ë¡œê·¸ íŒŒì¼ {len(log_files)}ê°œ ë°œê²¬, íŒŒì‹± ì‹œì‘...")
        
        # .log íŒŒì¼ë“¤ ì°¾ê¸°
        for i, log_file in enumerate(log_files, 1):
            try:
                logger.debug(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘ ({i}/{len(log_files)}): {log_file.name}")
                
                # íŒŒì¼ëª…ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
                model_name = self._extract_model_name_from_path(log_file)
                
                # ë¡œê·¸ íŒŒì‹±
                test_results = list(self.log_parser.parse_log_file(log_file))
                
                if test_results:
                    if model_name not in results:
                        results[model_name] = []
                    results[model_name].extend(test_results)
                    logger.debug(f"ëª¨ë¸ '{model_name}': {len(test_results)}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì¶”ê°€")
                else:
                    logger.warning(f"ë¡œê·¸ íŒŒì¼ì—ì„œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {log_file}")
                    
            except Exception as e:
                logger.warning(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {log_file} - {e}")
        
        logger.info(f"ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì™„ë£Œ - ì´ {len(results)}ê°œ ëª¨ë¸ì—ì„œ {sum(len(r) for r in results.values())}ê°œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ìˆ˜ì§‘")
        return results
    
    def _extract_model_name_from_path(self, log_file: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ
        
        Args:
            log_file: ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ëª¨ë¸ëª…
        """
        # ìƒìœ„ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë¸ëª… ì¶”ì¶œ (ë””ë ‰í† ë¦¬ êµ¬ì¡° ê¸°ë°˜)
        parent_dir = log_file.parent.name
        
        # ìœ íš¨í•œ ëª¨ë¸ëª…ì¸ì§€ í™•ì¸
        if parent_dir and parent_dir != 'deepeval_results':
            return parent_dir
        
        # fallback: íŒŒì¼ëª…ì—ì„œ ì¶”ì¶œ ì‹œë„
        file_name = log_file.stem
        
        # deepeval_results_model_name.log íŒ¨í„´
        if file_name.startswith('deepeval_results_'):
            parts = file_name.split('_')
            if len(parts) >= 3:
                return '_'.join(parts[2:])
        
        # model_name.log íŒ¨í„´
        if '_' in file_name:
            return file_name.split('_')[0]
        
        # ê¸°ë³¸ê°’
        return file_name or "unknown_model"
    
    def _perform_comprehensive_analysis(self, log_results: Dict[str, List]) -> Dict[str, Any]:
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        
        Args:
            log_results: ëª¨ë¸ë³„ ë¡œê·¸ ê²°ê³¼
            
        Returns:
            ì¢…í•© ë¶„ì„ ê²°ê³¼
        """
        if not log_results:
            return {"error": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        try:
            # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
            logger.info("ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì‹œì‘...")
            step_start = time.time()
            model_comparison = self.model_comparator.compare_models(log_results)
            logger.info(f"ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ - ëª¨ë“  ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘
            logger.info("ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
            step_start = time.time()
            all_failed_cases = []
            for model_name, model_results in log_results.items():
                model_failures = 0
                for test_case in model_results:
                    # í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­ì´ ìˆìœ¼ë©´ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¡œ ê°„ì£¼
                    has_failure = (
                        not test_case.correctness.passed or
                        not test_case.clarity.passed or
                        not test_case.actionability.passed or
                        not test_case.json_correctness.passed
                    )
                    if has_failure:
                        all_failed_cases.append(test_case)
                        model_failures += 1
                
                logger.debug(f"ëª¨ë¸ '{model_name}': {model_failures}/{len(model_results)} ì¼€ì´ìŠ¤ ì‹¤íŒ¨")
            
            logger.info(f"ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ - ì´ {len(all_failed_cases)}ê°œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
            logger.info("ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì‹œì‘...")
            step_start = time.time()
            failure_analysis = self.failure_analyzer.analyze_failure_patterns(all_failed_cases)
            logger.info(f"ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {time.time() - step_start:.2f}ì´ˆ)")
            
            # ë°ì´í„° ìš”ì•½
            total_test_cases = sum(len(results) for results in log_results.values())
            successful_cases = total_test_cases - len(all_failed_cases)
            
            logger.info(f"ì¢…í•© ë¶„ì„ ê²°ê³¼ - ì´ {total_test_cases}ê°œ ì¼€ì´ìŠ¤ ì¤‘ ì„±ê³µ {successful_cases}ê°œ, ì‹¤íŒ¨ {len(all_failed_cases)}ê°œ")
            
            return {
                "analysis_type": "single_session", 
                "model_comparison": model_comparison,
                "failure_analysis": failure_analysis,
                "data_summary": {
                    "total_test_cases": total_test_cases,
                    "successful_evaluations": successful_cases,
                    "failed_evaluations": len(all_failed_cases),
                    "models_count": len(log_results),
                    "analysis_timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            logger.error(f"ì¢…í•© ë¶„ì„ ìˆ˜í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            return {"error": f"ë¶„ì„ ì‹¤íŒ¨: {str(e)}"}
    
    def _prepare_repo_results_from_versions(self, version_data: Dict[str, Any]) -> Dict[str, Dict[str, List]]:
        """ë²„ì „ ë°ì´í„°ì—ì„œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„
        
        Args:
            version_data: ë²„ì „ë³„ ë°ì´í„°
            
        Returns:
            ì €ì¥ì†Œë³„ ëª¨ë¸ ê²°ê³¼
        """
        repo_results = {}
        
        for version, data in version_data.items():
            for session in data['sessions']:
                session_dir = Path(session['session_dir'])
                
                # ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
                repo_name = self._extract_repo_name_from_session_path(session_dir)
                
                if repo_name not in repo_results:
                    repo_results[repo_name] = {}
                
                # ì„¸ì…˜ì˜ ê²°ê³¼ë¥¼ ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í•‘
                session_results = session.get('results', [])
                if session_results:
                    model_name = f"version_{version}"
                    if model_name not in repo_results[repo_name]:
                        repo_results[repo_name][model_name] = []
                    repo_results[repo_name][model_name].extend(session_results)
        
        return repo_results
    
    def _extract_repo_name_from_session_path(self, session_path: Path) -> str:
        """ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ
        
        Args:
            session_path: ì„¸ì…˜ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            
        Returns:
            ì¶”ì¶œëœ ì €ì¥ì†Œëª…
        """
        # ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª…ìœ¼ë¡œ ë³´ì´ëŠ” ë¶€ë¶„ ì¶”ì¶œ
        path_parts = session_path.parts
        
        # ì¼ë°˜ì ì¸ ì €ì¥ì†Œëª… íŒ¨í„´ ì°¾ê¸°
        for part in reversed(path_parts):
            if any(keyword in part.lower() for keyword in ['cline', 'ecommerce', 'kotlin', 'selvage']):
                return part
        
        # ê¸°ë³¸ê°’ìœ¼ë¡œ ë§ˆì§€ë§‰ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        return session_path.name or "unknown_repo"
    
    def _generate_markdown_report(self, analysis_results: Dict[str, Any]) -> str:
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            analysis_results: ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        if "error" in analysis_results:
            return f"# ë¶„ì„ ì˜¤ë¥˜\n\n{analysis_results['error']}"
        
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# DeepEval ë¶„ì„ ë³´ê³ ì„œ (V2)",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"**ë¶„ì„ ìœ í˜•**: {analysis_results.get('analysis_type', 'single_session')}",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = analysis_results.get("data_summary", {})
        if data_summary:
            lines.extend([
                "## ğŸ“Š ë°ì´í„° ìš”ì•½",
                "",
                f"- **ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤**: {data_summary.get('total_test_cases', 0)}ê°œ",
                f"- **ì„±ê³µí•œ í‰ê°€**: {data_summary.get('successful_evaluations', 0)}ê°œ",
                f"- **ì‹¤íŒ¨í•œ í‰ê°€**: {data_summary.get('failed_evaluations', 0)}ê°œ",
                f"- **ë¶„ì„ëœ ëª¨ë¸**: {data_summary.get('models_count', 0)}ê°œ",
                ""
            ])
        
        # ëª¨ë¸ ë¹„êµ ê²°ê³¼
        model_comparison = analysis_results.get("model_comparison", {})
        if model_comparison and model_comparison.get("recommendations"):
            lines.extend([
                "## ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ",
                "",
                "### ì£¼ìš” ê¶Œì¥ì‚¬í•­",
                ""
            ])
            
            for rec in model_comparison["recommendations"][:10]:  # ìƒìœ„ 10ê°œ
                lines.append(f"- {rec}")
            
            lines.append("")
            
            # ë¹„êµ í‘œ
            comparison_table = model_comparison.get("comparison_table", {})
            table_data = comparison_table.get("table_data", [])
            
            if table_data:
                lines.extend([
                    "### ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½",
                    "",
                    "| ëª¨ë¸ëª… | ì¢…í•©ì ìˆ˜ | ë“±ê¸‰ | ìˆœìœ„ | ì •í™•ì„± | ëª…í™•ì„± | ì‹¤í–‰ê°€ëŠ¥ì„± | JSON ì •í™•ì„± |",
                    "|--------|----------|------|------|--------|--------|------------|-------------|"
                ])
                
                for model_data in table_data:
                    lines.append(
                        f"| {model_data.get('model_name', 'N/A')} | "
                        f"{model_data.get('overall_score', 0):.3f} | "
                        f"{model_data.get('grade', 'N/A')} | "
                        f"{model_data.get('overall_rank', 'N/A')} | "
                        f"{model_data.get('correctness_score', 0):.3f} | "
                        f"{model_data.get('clarity_score', 0):.3f} | "
                        f"{model_data.get('actionability_score', 0):.3f} | "
                        f"{model_data.get('json_correctness_score', 0):.3f} |"
                    )
                
                lines.append("")
        
        # ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„
        model_failure_analysis = analysis_results.get("model_failure_analysis", {})
        if model_failure_analysis:
            lines.extend([
                "## ğŸš¨ ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„",
                "",
                "ê° ëª¨ë¸ë³„ë¡œ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ì˜ ìƒì„¸ ë¶„ì„ì…ë‹ˆë‹¤.",
                ""
            ])
            
            # ëª¨ë¸ë³„ ì‹¤íŒ¨ ì •ë³´ í…Œì´ë¸”
            lines.extend([
                "### ëª¨ë¸ë³„ ì‹¤íŒ¨ í˜„í™©",
                "",
                "| ëª¨ë¸ëª… | ì´ í…ŒìŠ¤íŠ¸ | ì‹¤íŒ¨ ê±´ìˆ˜ | ì‹¤íŒ¨ìœ¨ | ì£¼ìš” ì‹¤íŒ¨ ë©”íŠ¸ë¦­ |",
                "|--------|-----------|-----------|--------|------------------|"
            ])
            
            for model_name, failure_data in model_failure_analysis.items():
                total_tests = failure_data.get('total_tests', 0)
                total_failures = failure_data.get('total_failures', 0)
                failure_rate = failure_data.get('failure_rate', 0)
                failed_metrics = failure_data.get('failed_metrics', {})
                
                # ì£¼ìš” ì‹¤íŒ¨ ë©”íŠ¸ë¦­ ì¶”ì¶œ
                main_failed_metrics = [metric for metric in failed_metrics.keys() if failed_metrics[metric]]
                main_metrics_str = ", ".join(main_failed_metrics) if main_failed_metrics else "ì—†ìŒ"
                
                lines.append(
                    f"| {model_name} | {total_tests} | {total_failures} | "
                    f"{failure_rate:.1%} | {main_metrics_str} |"
                )
            
            lines.append("")
            
            # ê° ëª¨ë¸ë³„ ìƒì„¸ ì‹¤íŒ¨ ë¶„ì„
            for model_name, failure_data in model_failure_analysis.items():
                if failure_data.get('total_failures', 0) > 0:
                    lines.extend([
                        f"#### {model_name} ëª¨ë¸ ì‹¤íŒ¨ ë¶„ì„",
                        ""
                    ])
                    
                    # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ì •ë³´
                    failed_metrics = failure_data.get('failed_metrics', {})
                    if failed_metrics:
                        lines.extend([
                            "**ì‹¤íŒ¨í•œ ë©”íŠ¸ë¦­:**",
                            ""
                        ])
                        
                        for metric_name, metric_data in failed_metrics.items():
                            metric_display = {
                                'correctness': 'ì •í™•ì„±',
                                'clarity': 'ëª…í™•ì„±', 
                                'actionability': 'ì‹¤í–‰ê°€ëŠ¥ì„±',
                                'json_correctness': 'JSON ì •í™•ì„±'
                            }.get(metric_name, metric_name)
                            
                            lines.append(
                                f"- **{metric_display}**: {metric_data['failure_count']}ê±´ ì‹¤íŒ¨ "
                                f"(í‰ê·  ì‹ ë¢°ë„: {metric_data['avg_confidence']:.3f})"
                            )
                    
                    # ì£¼ìš” ì‹¤íŒ¨ ì´ìœ  (ë²ˆì—­ëœ ê²ƒ)
                    top_reasons = failure_data.get('failure_reasons', [])
                    if top_reasons:
                        lines.extend([
                            "",
                            "**ì‹¤íŒ¨ ì´ìœ :**",
                            ""
                        ])
                        
                        for i, reason in enumerate(top_reasons, 1):
                            lines.append(f"{i}. {reason}")
                    
                    lines.append("")
            
            lines.append("")
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_analysis = analysis_results.get("failure_analysis", {})
        if failure_analysis and failure_analysis.get("total_failures", 0) > 0:
            lines.extend([
                "## ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„",
                "",
                f"**ì´ ì‹¤íŒ¨ ê±´ìˆ˜**: {failure_analysis['total_failures']}ê°œ",
                ""
            ])
            
            # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ë¶„ì„
            by_metric = failure_analysis.get("by_metric", {})
            if by_metric:
                lines.extend([
                    "### ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ í˜„í™©",
                    "",
                    "| ë©”íŠ¸ë¦­ | ì‹¤íŒ¨ ê±´ìˆ˜ | ì‹¤íŒ¨ìœ¨ | í‰ê·  ì‹ ë¢°ë„ |",
                    "|--------|-----------|--------|-------------|"
                ])
                
                for metric, data in by_metric.items():
                    lines.append(
                        f"| {metric.replace('_', ' ').title()} | "
                        f"{data.get('total_failures', 0)} | "
                        f"{data.get('failure_rate', 0):.1%} | "
                        f"{data.get('avg_confidence', 0):.3f} |"
                    )
                
                lines.append("")
            
            # ì¤‘ìš”í•œ íŒ¨í„´
            critical_patterns = failure_analysis.get("critical_patterns", [])
            if critical_patterns:
                lines.extend([
                    "### ì¤‘ìš”í•œ ì‹¤íŒ¨ íŒ¨í„´",
                    ""
                ])
                
                for pattern in critical_patterns[:5]:  # ìƒìœ„ 5ê°œ
                    lines.append(
                        f"- **{pattern.get('category', 'Unknown')}**: "
                        f"{pattern.get('count', 0)}ê±´ ({pattern.get('percentage', 0):.1f}%) "
                        f"- {pattern.get('reason', '')}"
                    )
                
                lines.append("")
        
        # ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­
        lines.extend([
            "## ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­",
            "",
            "### ì£¼ìš” ë°œê²¬ì‚¬í•­",
            ""
        ])
        
        # ì „ì²´ ê¶Œì¥ì‚¬í•­ ì¢…í•©
        all_recommendations = []
        if model_comparison.get("recommendations"):
            all_recommendations.extend(model_comparison["recommendations"][:3])
        
        if failure_analysis.get("critical_patterns"):
            all_recommendations.append(
                f"ì‹¤íŒ¨ íŒ¨í„´ ê°œì„  í•„ìš”: {len(failure_analysis['critical_patterns'])}ê°œì˜ ì¤‘ìš”í•œ íŒ¨í„´ ë°œê²¬"
            )
        
        for rec in all_recommendations:
            lines.append(f"1. {rec}")
        
        lines.extend([
            "",
            "### ë‹¤ìŒ ë‹¨ê³„",
            "",
            "- ì„±ëŠ¥ì´ ë‚®ì€ ë©”íŠ¸ë¦­ì— ëŒ€í•œ ëª¨ë¸ ê°œì„ ",
            "- ì‹¤íŒ¨ íŒ¨í„´ì´ ë§ì€ ì˜ì—­ì˜ í”„ë¡¬í”„íŠ¸ ìµœì í™”",
            "- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì˜ íŠ¹ì„±ì„ ë‹¤ë¥¸ ëª¨ë¸ì— ì ìš©",
            "- ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° ì¶”ì ",
            ""
        ])
        
        return "\n".join(lines)
    
    def _generate_multi_session_markdown_report(self, integrated_results: Dict[str, Any]) -> str:
        """ë‹¤ì¤‘ ì„¸ì…˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            integrated_results: í†µí•© ë¶„ì„ ê²°ê³¼
            
        Returns:
            ë§ˆí¬ë‹¤ìš´ ì½˜í…ì¸ 
        """
        lines = []
        
        # í—¤ë”
        lines.extend([
            "# ë‹¤ì¤‘ ì„¸ì…˜ ë¶„ì„ ë³´ê³ ì„œ",
            "",
            f"**ë¶„ì„ ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "**ë¶„ì„ ìœ í˜•**: ë²„ì „ ë¹„êµ ë° ê¸°ìˆ ìŠ¤íƒ ë¶„ì„",
            ""
        ])
        
        # ë°ì´í„° ìš”ì•½
        data_summary = integrated_results.get("data_summary", {})
        lines.extend([
            "## ğŸ“Š ë¶„ì„ ê°œìš”",
            "",
            f"- **ë¶„ì„ëœ ë²„ì „**: {data_summary.get('total_versions', 0)}ê°œ",
            f"- **ì´ ì„¸ì…˜**: {data_summary.get('total_sessions', 0)}ê°œ",
            ""
        ])
        
        # ë²„ì „ ë¶„ì„
        version_analysis = integrated_results.get("version_analysis", {})
        if version_analysis:
            lines.extend([
                "## ğŸ“ˆ ë²„ì „ë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            # ë²„ì „ ê¶Œì¥ì‚¬í•­
            version_recommendations = version_analysis.get("version_recommendations", {})
            if version_recommendations:
                recommended_version = version_recommendations.get("recommended_version")
                if recommended_version:
                    lines.append(f"**ê¶Œì¥ ë²„ì „**: {recommended_version}")
                    lines.append("")
                
                recommendations = version_recommendations.get("recommendations", [])
                if recommendations:
                    lines.append("### ë²„ì „ ê´€ë ¨ ê¶Œì¥ì‚¬í•­")
                    lines.append("")
                    for rec in recommendations:
                        lines.append(f"- {rec}")
                    lines.append("")
        
        # ê¸°ìˆ ìŠ¤íƒ ë¶„ì„
        tech_stack_analysis = integrated_results.get("tech_stack_analysis", {})
        if tech_stack_analysis:
            lines.extend([
                "## ğŸ›  ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„",
                ""
            ])
            
            tech_recommendations = tech_stack_analysis.get("recommendations", [])
            if tech_recommendations:
                for rec in tech_recommendations[:10]:  # ìƒìœ„ 10ê°œ
                    lines.append(f"- {rec}")
                lines.append("")
        
        lines.extend([
            "## ğŸ’¡ í†µí•© ê¶Œì¥ì‚¬í•­",
            "",
            "### ë²„ì „ ì—…ê·¸ë ˆì´ë“œ",
            "- ìµœì‹  ì•ˆì • ë²„ì „ìœ¼ë¡œì˜ ì—…ê·¸ë ˆì´ë“œ ê²€í† ",
            "- ì„±ëŠ¥ íšŒê·€ê°€ ìˆëŠ” ë²„ì „ ì‚¬ìš© ì¤‘ë‹¨",
            "",
            "### ê¸°ìˆ ìŠ¤íƒ ìµœì í™”", 
            "- ê° ê¸°ìˆ ìŠ¤íƒì— ìµœì í™”ëœ ëª¨ë¸ ì„ íƒ",
            "- ì„±ëŠ¥ì´ ë‚®ì€ ê¸°ìˆ ìŠ¤íƒì˜ í”„ë¡¬í”„íŠ¸ ê°œì„ ",
            "",
            "### ì§€ì†ì  ëª¨ë‹ˆí„°ë§",
            "- ì •ê¸°ì ì¸ ë²„ì „ë³„ ì„±ëŠ¥ ì¶”ì ",
            "- ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹",
            ""
        ])
        
        return "\n".join(lines)
    
    def _init_gemini_client(self):
        """Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                logger.warning("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹¤íŒ¨ ì´ìœ  ë²ˆì—­ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
                self.gemini_client = None
                return
            
            self.gemini_client = GeminiClient(api_key=api_key, model_name="gemini-2.5-flash")
            logger.info("Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"Gemini í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.gemini_client = None
    
    def _translate_failure_reason_with_gemini(self, failure_reason: str) -> str:
        """Geminië¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¤íŒ¨ ì´ìœ ë¥¼ í•œê¸€ë¡œ ë²ˆì—­
        
        Args:
            failure_reason: ì˜ì–´ ì‹¤íŒ¨ ì´ìœ 
            
        Returns:
            í•œê¸€ ë²ˆì—­ëœ ì‹¤íŒ¨ ì´ìœ 
        """
        if not self.gemini_client or not failure_reason:
            return failure_reason
        
        try:
            
            system_instruction = "ë‹¤ìŒ DeepEval í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ì´ìœ ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•´ì£¼ì„¸ìš”. ê¸°ìˆ ì  ìš©ì–´ëŠ” ì ì ˆíˆ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ë˜, ì˜ë¯¸ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë˜ë„ë¡ í•´ì£¼ì„¸ìš”."
            
            messages = [{
                "role": "user", 
                "content": f"ì›ë¬¸: {failure_reason}\n\në²ˆì—­:"
            }]
            
            response = self.gemini_client.query(
                messages=messages,
                system_instruction=system_instruction
            )
            
            translated = response.strip()
            
            # ë²ˆì—­ ê²°ê³¼ ê²€ì¦
            if translated and len(translated) > 0:
                return translated
            else:
                logger.warning(f"ë²ˆì—­ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì›ë³¸ ë°˜í™˜: {failure_reason}")
                return failure_reason
                
        except Exception as e:
            logger.error(f"Gemini ë²ˆì—­ ì‹¤íŒ¨: {e}")
            return failure_reason
    
    def _batch_translate_failure_reasons(self, failure_reasons: List[str]) -> List[str]:
        """ì—¬ëŸ¬ ì‹¤íŒ¨ ì´ìœ ë¥¼ ë°°ì¹˜ë¡œ ë²ˆì—­
        
        Args:
            failure_reasons: ì˜ì–´ ì‹¤íŒ¨ ì´ìœ  ëª©ë¡
            
        Returns:
            í•œê¸€ ë²ˆì—­ëœ ì‹¤íŒ¨ ì´ìœ  ëª©ë¡
        """
        if not self.gemini_client:
            return failure_reasons
        
        translated_reasons = []
        
        # ë°°ì¹˜ í¬ê¸° ì œí•œ (API í•œë„ ê³ ë ¤)
        batch_size = 10
        
        for i in range(0, len(failure_reasons), batch_size):
            batch = failure_reasons[i:i + batch_size]
            batch_translated = []
            
            for reason in batch:
                translated = self._translate_failure_reason_with_gemini(reason)
                batch_translated.append(translated)
            
            translated_reasons.extend(batch_translated)
            
            # API í˜¸ì¶œ ê°„ê²© ì¡°ì ˆ
            if len(failure_reasons) > batch_size:
                import time
                time.sleep(0.5)
        
        return translated_reasons
    
    def _generate_model_failure_analysis(self, log_results: Dict[str, List]) -> Dict[str, Any]:
        """ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„° ìƒì„±
        
        Args:
            log_results: ëª¨ë¸ë³„ ë¡œê·¸ ê²°ê³¼
            
        Returns:
            ëª¨ë¸ë³„ ì‹¤íŒ¨ ë¶„ì„ ë°ì´í„°
        """
        model_failures = {}
        
        for model_name, test_results in log_results.items():
            failure_count = 0
            failed_metrics = {
                'correctness': [],
                'clarity': [],
                'actionability': [],
                'json_correctness': []
            }
            failure_reasons = []
            
            for test_case in test_results:
                has_failure = False
                
                # ê° ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ í™•ì¸
                if not test_case.correctness.passed:
                    has_failure = True
                    failed_metrics['correctness'].append({
                        'confidence': test_case.correctness.score,
                        'reason': test_case.correctness.reason or "ì •í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.clarity.passed:
                    has_failure = True
                    failed_metrics['clarity'].append({
                        'confidence': test_case.clarity.score,
                        'reason': test_case.clarity.reason or "ëª…í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.actionability.passed:
                    has_failure = True
                    failed_metrics['actionability'].append({
                        'confidence': test_case.actionability.score,
                        'reason': test_case.actionability.reason or "ì‹¤í–‰ê°€ëŠ¥ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if not test_case.json_correctness.passed:
                    has_failure = True
                    failed_metrics['json_correctness'].append({
                        'confidence': test_case.json_correctness.score,
                        'reason': test_case.json_correctness.reason or "JSON ì •í™•ì„± ê²€ì‚¬ ì‹¤íŒ¨"
                    })
                
                if has_failure:
                    failure_count += 1
                    
                    # ì‹¤íŒ¨ ì´ìœ  ìˆ˜ì§‘
                    for metric_name, metric_failures in failed_metrics.items():
                        if metric_failures:
                            latest_failure = metric_failures[-1]
                            if latest_failure['reason'] not in failure_reasons:
                                failure_reasons.append(latest_failure['reason'])
            
            # ì‹¤íŒ¨ ì´ìœ  ë²ˆì—­
            translated_reasons = self._batch_translate_failure_reasons(failure_reasons)
            
            # ë©”íŠ¸ë¦­ë³„ ì‹¤íŒ¨ ìš”ì•½
            metric_summary = {}
            for metric_name, failures in failed_metrics.items():
                if failures:
                    metric_summary[metric_name] = {
                        'failure_count': len(failures),
                        'avg_confidence': sum(f['confidence'] for f in failures) / len(failures) if failures else 0,
                        'common_reasons': list(set(f['reason'] for f in failures))[:3]  # ìƒìœ„ 3ê°œ
                    }
            
            model_failures[model_name] = {
                'total_failures': failure_count,
                'total_tests': len(test_results),
                'failure_rate': failure_count / len(test_results) if test_results else 0,
                'failed_metrics': metric_summary,
                'failure_reasons': translated_reasons,
            }
        
        return model_failures