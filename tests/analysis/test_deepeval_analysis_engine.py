"""DeepEval ë¶„ì„ ì—”ì§„ V2 í…ŒìŠ¤íŠ¸"""

import unittest
from unittest.mock import patch, MagicMock, mock_open
import tempfile
import shutil
import json
from pathlib import Path
from datetime import datetime

from selvage_eval.analysis.deepeval_analysis_engine import DeepEvalAnalysisEngine
from selvage_eval.analysis.deepeval_log_parser import TestCaseResult, MetricScore


class TestDeepEvalAnalysisEngine(unittest.TestCase):
    """DeepEval ë¶„ì„ ì—”ì§„ í…ŒìŠ¤íŠ¸"""
    
    def setUp(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        self.temp_dir = tempfile.mkdtemp()
        self.temp_path = Path(self.temp_dir)
        
        # ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.engine = DeepEvalAnalysisEngine(output_dir=str(self.temp_path / "output"))
        
        # ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë°ì´í„°
        self.sample_test_results = [
            TestCaseResult(
                correctness=MetricScore(score=0.85, passed=True, reason="Good correctness"),
                clarity=MetricScore(score=0.80, passed=True, reason="Clear output"),
                actionability=MetricScore(score=0.75, passed=True, reason="Actionable suggestions"),
                json_correctness=MetricScore(score=1.0, passed=True, reason="Valid JSON"),
                input_data="Sample input",
                actual_output="Sample output",
                raw_content="Raw log content for test case 1"
            ),
            TestCaseResult(
                correctness=MetricScore(score=0.60, passed=False, reason="Missed some issues"),
                clarity=MetricScore(score=0.70, passed=True, reason="Mostly clear"),
                actionability=MetricScore(score=0.65, passed=False, reason="Vague suggestions"),
                json_correctness=MetricScore(score=0.95, passed=True, reason="Valid JSON with minor issues"),
                input_data="Sample input 2",
                actual_output="Sample output 2",
                raw_content="Raw log content for test case 2"
            )
        ]
    
    def tearDown(self):
        """í…ŒìŠ¤íŠ¸ ì •ë¦¬"""
        # ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
        shutil.rmtree(self.temp_dir)
    
    def test_init(self):
        """ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸"""
        self.assertIsNotNone(self.engine.log_parser)
        self.assertIsNotNone(self.engine.metric_aggregator)
        self.assertIsNotNone(self.engine.failure_analyzer)
        self.assertIsNotNone(self.engine.model_comparator)
        self.assertIsNotNone(self.engine.tech_stack_analyzer)
        self.assertIsNotNone(self.engine.version_analyzer)
        self.assertIsNotNone(self.engine.visualizer)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self.assertTrue(self.engine.output_dir.exists())
    
    def test_extract_model_name_from_path(self):
        """ëª¨ë¸ëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        # deepeval_results_model_name.log íŒ¨í„´
        log_path1 = Path("/path/to/deepeval_results_gpt_4.log")
        model_name1 = self.engine._extract_model_name_from_path(log_path1)
        self.assertEqual(model_name1, "gpt_4")
        
        # model_name.log íŒ¨í„´
        log_path2 = Path("/path/to/claude_3.log") 
        model_name2 = self.engine._extract_model_name_from_path(log_path2)
        self.assertEqual(model_name2, "claude")
        
        # ê¸°ë³¸ê°’ í…ŒìŠ¤íŠ¸
        log_path3 = Path("/path/to/unknown.log")
        model_name3 = self.engine._extract_model_name_from_path(log_path3)
        self.assertEqual(model_name3, "unknown")
    
    @patch('selvage_eval.analysis.deepeval_analysis_engine.DeepEvalLogParser')
    def test_collect_log_results(self, mock_parser_class):
        """ë¡œê·¸ ê²°ê³¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_parser = MagicMock()
        mock_parser_class.return_value = mock_parser
        mock_parser.parse_log_file.return_value = iter(self.sample_test_results)
        
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "test_session"
        session_dir.mkdir()
        
        # ë¡œê·¸ íŒŒì¼ ìƒì„±
        log_file = session_dir / "deepeval_results_gpt_4.log"
        log_file.write_text("dummy log content")
        
        # ì—”ì§„ì— ëª¨í‚¹ëœ íŒŒì„œ í• ë‹¹
        self.engine.log_parser = mock_parser
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.engine._collect_log_results(session_dir)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("gpt_4", results)
        self.assertEqual(len(results["gpt_4"]), 2)
        mock_parser.parse_log_file.assert_called_once()
    
    def test_collect_log_results_multiple_models(self):
        """ë‹¤ì¤‘ ëª¨ë¸ ë¡œê·¸ ê²°ê³¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "multi_model_session"
        session_dir.mkdir()
        
        # ì‹¤ì œ ë¡œê·¸ íŒŒì¼ ë‚´ìš© ìƒì„±
        log_content_template = """==================================================
Test Case: {test_case}
Input: {input_data}
Expected: {expected}
Actual: {actual}

âœ… Correctness (score: {correctness_score}, reason: "{correctness_reason}", error: None)
âœ… Clarity (score: {clarity_score}, reason: "{clarity_reason}", error: None)
âœ… Actionability (score: {actionability_score}, reason: "{actionability_reason}", error: None)
âœ… JSON Correctness (score: {json_score}, reason: "{json_reason}", error: None)
=================================================="""
        
        # ì—¬ëŸ¬ ëª¨ë¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        models_data = {
            "gpt_4": {"correctness_score": 0.9, "clarity_score": 0.85, "actionability_score": 0.8, "json_score": 1.0},
            "claude_3": {"correctness_score": 0.85, "clarity_score": 0.9, "actionability_score": 0.75, "json_score": 0.95},
            "gemini_pro": {"correctness_score": 0.8, "clarity_score": 0.8, "actionability_score": 0.7, "json_score": 0.9}
        }
        
        for model_name, scores in models_data.items():
            log_file = session_dir / f"deepeval_results_{model_name}.log"
            log_content = log_content_template.format(
                test_case=f"Test case for {model_name}",
                input_data=f"Input for {model_name}",
                expected=f"Expected output for {model_name}",
                actual=f"Actual output for {model_name}",
                correctness_score=scores["correctness_score"],
                correctness_reason=f"Good analysis by {model_name}",
                clarity_score=scores["clarity_score"],
                clarity_reason=f"Clear explanation by {model_name}",
                actionability_score=scores["actionability_score"],
                actionability_reason=f"Actionable suggestions by {model_name}",
                json_score=scores["json_score"],
                json_reason=f"Valid JSON by {model_name}"
            )
            log_file.write_text(log_content)
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.engine._collect_log_results(session_dir)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertEqual(len(results), 3)
        self.assertIn("gpt_4", results)
        self.assertIn("claude_3", results) 
        self.assertIn("gemini_pro", results)
        
        # ê° ëª¨ë¸ì´ í•˜ë‚˜ì”©ì˜ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ê°€ì§€ëŠ”ì§€ í™•ì¸
        for model_name in models_data.keys():
            self.assertEqual(len(results[model_name]), 1)
    
    def test_collect_log_results_nested_directories(self):
        """ì¤‘ì²© ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ ê²°ê³¼ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "nested_session"
        session_dir.mkdir()
        
        # ì¤‘ì²©ëœ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        sub_dir1 = session_dir / "model_results" / "gpt"
        sub_dir1.mkdir(parents=True)
        sub_dir2 = session_dir / "evaluation" / "claude"
        sub_dir2.mkdir(parents=True)
        
        # ì¤‘ì²©ëœ ìœ„ì¹˜ì— ë¡œê·¸ íŒŒì¼ ìƒì„±
        log_file1 = sub_dir1 / "deepeval_results_gpt_4.log"
        log_file1.write_text("""==================================================
Test Case: Nested test case 1
Input: Nested input 1
Expected: Nested expected 1
Actual: Nested actual 1

âœ… Correctness (score: 0.8, reason: "Good nested analysis", error: None)
âœ… Clarity (score: 0.75, reason: "Clear nested output", error: None)
âœ… Actionability (score: 0.7, reason: "Actionable nested suggestions", error: None)
âœ… JSON Correctness (score: 0.95, reason: "Valid nested JSON", error: None)
==================================================""")
        
        log_file2 = sub_dir2 / "deepeval_results_claude_3.log"  
        log_file2.write_text("""==================================================
Test Case: Nested test case 2
Input: Nested input 2
Expected: Nested expected 2
Actual: Nested actual 2

âœ… Correctness (score: 0.85, reason: "Good nested analysis 2", error: None)
âœ… Clarity (score: 0.9, reason: "Clear nested output 2", error: None)
âœ… Actionability (score: 0.8, reason: "Actionable nested suggestions 2", error: None)
âœ… JSON Correctness (score: 1.0, reason: "Valid nested JSON 2", error: None)
==================================================""")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.engine._collect_log_results(session_dir)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertEqual(len(results), 2)
        self.assertIn("gpt_4", results)
        self.assertIn("claude_3", results)
        self.assertEqual(len(results["gpt_4"]), 1)
        self.assertEqual(len(results["claude_3"]), 1)
    
    def test_collect_log_results_malformed_logs(self):
        """ì˜ëª»ëœ í˜•ì‹ì˜ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "malformed_session"
        session_dir.mkdir()
        
        # ì˜¬ë°”ë¥¸ ë¡œê·¸ íŒŒì¼ ìƒì„±
        good_log = session_dir / "deepeval_results_gpt_4.log"
        good_log.write_text("""==================================================
Test Case: Good test case
Input: Good input
Expected: Good expected
Actual: Good actual

âœ… Correctness (score: 0.8, reason: "Good analysis", error: None)
âœ… Clarity (score: 0.75, reason: "Clear output", error: None)
âœ… Actionability (score: 0.7, reason: "Actionable suggestions", error: None)
âœ… JSON Correctness (score: 0.95, reason: "Valid JSON", error: None)
==================================================""")
        
        # ì˜ëª»ëœ í˜•ì‹ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        bad_log = session_dir / "deepeval_results_claude_3.log"
        bad_log.write_text("This is not a valid log file format")
        
        # ë¹ˆ ë¡œê·¸ íŒŒì¼ ìƒì„±
        empty_log = session_dir / "deepeval_results_gemini_pro.log"
        empty_log.write_text("")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.engine._collect_log_results(session_dir)
        
        # ê²°ê³¼ ê²€ì¦: ì˜¬ë°”ë¥¸ íŒŒì¼ë§Œ íŒŒì‹±ë˜ì–´ì•¼ í•¨
        self.assertIn("gpt_4", results)
        self.assertEqual(len(results["gpt_4"]), 1)
        
        # ì˜ëª»ëœ íŒŒì¼ë“¤ì€ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì•„ì•¼ í•¨
        # (ë¡œê·¸ íŒŒì„œê°€ ë¹ˆ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ë¯€ë¡œ)
        for model_name in ["claude_3", "gemini_pro"]:
            if model_name in results:
                # ë¡œê·¸ íŒŒì„œê°€ ë¹ˆ ë¡œê·¸ë¥¼ í•˜ë‚˜ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŒ
                self.assertLessEqual(len(results[model_name]), 1)
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.DeepEvalAnalysisEngine._perform_comprehensive_analysis')
    def test_perform_comprehensive_analysis(self, mock_analysis, mock_collect):
        """ì¢…í•© ë¶„ì„ ìˆ˜í–‰ í…ŒìŠ¤íŠ¸"""
        # ëª¨í‚¹ ì„¤ì •
        mock_log_results = {"gpt_4": self.sample_test_results}
        
        # ëª¨í‚¹ëœ ë¶„ì„ ê²°ê³¼
        mock_analysis_result = {
            "analysis_type": "single_session",
            "model_comparison": {
                "model_statistics": {"gpt_4": {"overall_score": 0.75}},
                "recommendations": ["Test recommendation"]
            },
            "failure_analysis": {
                "total_failures": 1,
                "by_metric": {"correctness": {"total_failures": 1}}
            },
            "data_summary": {
                "total_test_cases": 2,
                "successful_evaluations": 1,
                "failed_evaluations": 1,
                "models_count": 1
            }
        }
        mock_analysis.return_value = mock_analysis_result
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine._perform_comprehensive_analysis(mock_log_results)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertEqual(result["analysis_type"], "single_session")
        self.assertIn("model_comparison", result)
        self.assertIn("failure_analysis", result)
        self.assertIn("data_summary", result)
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    @patch.object(DeepEvalAnalysisEngine, '_perform_comprehensive_analysis')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VisualizationGenerator')
    def test_analyze_session(self, mock_viz_class, mock_analysis, mock_collect):
        """ì„¸ì…˜ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "test_session"
        session_dir.mkdir()
        
        # ëª¨í‚¹ ì„¤ì •
        mock_collect.return_value = {"gpt_4": self.sample_test_results}
        mock_analysis.return_value = {
            "analysis_type": "single_session",
            "model_comparison": {"model_statistics": {"gpt_4": {}}},
            "failure_analysis": {"total_failures": 1},
            "data_summary": {"total_test_cases": 2}
        }
        
        # ì‹œê°í™” ëª¨í‚¹
        mock_viz = MagicMock()
        mock_viz_class.return_value = mock_viz
        mock_viz.generate_comprehensive_dashboard.return_value = ["chart1.html", "chart2.html"]
        mock_viz.create_summary_report.return_value = "summary.html"
        
        # ì—”ì§„ì˜ visualizerë¥¼ ëª¨í‚¹ëœ ê°ì²´ë¡œ êµì²´
        self.engine.visualizer = mock_viz
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine.analyze_session(str(session_dir))
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("analysis_metadata", result)
        self.assertIn("files_generated", result)
        self.assertIn("markdown_report", result["files_generated"])
        self.assertIn("json_data", result["files_generated"])
        
        # ìƒì„±ëœ íŒŒì¼ë“¤ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        markdown_path = Path(result["files_generated"]["markdown_report"])
        json_path = Path(result["files_generated"]["json_data"])
        self.assertTrue(markdown_path.exists())
        self.assertTrue(json_path.exists())
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    @patch.object(DeepEvalAnalysisEngine, '_perform_comprehensive_analysis')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VisualizationGenerator')
    def test_analyze_session_visualization_content(self, mock_viz_class, mock_analysis, mock_collect):
        """ì„¸ì…˜ ë¶„ì„ ì‹œê°í™” ë‚´ìš© ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "viz_test_session"
        session_dir.mkdir()
        
        # ëª¨í‚¹ ì„¤ì •
        mock_collect.return_value = {"gpt_4": self.sample_test_results}
        mock_analysis.return_value = {
            "analysis_type": "single_session",
            "model_comparison": {
                "model_statistics": {"gpt_4": {"overall_score": 0.8}},
                "comparison_table": {"table_data": [{"model_name": "gpt_4", "overall_score": 0.8}]}
            },
            "failure_analysis": {"total_failures": 1},
            "data_summary": {"total_test_cases": 2}
        }
        
        # ì‹œê°í™” ëª¨í‚¹
        mock_viz = MagicMock()
        mock_viz_class.return_value = mock_viz
        mock_viz.generate_comprehensive_dashboard.return_value = ["performance_chart.html", "failure_chart.html"]
        mock_viz.create_summary_report.return_value = "comprehensive_summary.html"
        
        # ì—”ì§„ì˜ visualizerë¥¼ ëª¨í‚¹ëœ ê°ì²´ë¡œ êµì²´
        self.engine.visualizer = mock_viz
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine.analyze_session(str(session_dir))
        
        # ì‹œê°í™” ë©”ì„œë“œ í˜¸ì¶œ ê²€ì¦
        mock_viz.generate_comprehensive_dashboard.assert_called_once()
        mock_viz.create_summary_report.assert_called_once()
        
        # ì‹œê°í™” íŒŒì¼ì´ ê²°ê³¼ì— í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        viz_files = result["files_generated"]["visualization_files"]
        self.assertIn("performance_chart.html", viz_files)
        self.assertIn("failure_chart.html", viz_files)
        self.assertIn("comprehensive_summary.html", viz_files)
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        metadata = result["analysis_metadata"]
        self.assertEqual(metadata["total_test_cases"], 2)
        self.assertEqual(metadata["models_analyzed"], ["gpt_4"])
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    @patch.object(DeepEvalAnalysisEngine, '_perform_comprehensive_analysis')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VisualizationGenerator')
    def test_analyze_session_visualization_error_handling(self, mock_viz_class, mock_analysis, mock_collect):
        """ì„¸ì…˜ ë¶„ì„ ì‹œê°í™” ì˜¤ë¥˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "viz_error_session"
        session_dir.mkdir()
        
        # ëª¨í‚¹ ì„¤ì •
        mock_collect.return_value = {"gpt_4": self.sample_test_results}
        mock_analysis.return_value = {
            "analysis_type": "single_session",
            "model_comparison": {"model_statistics": {"gpt_4": {}}},
            "failure_analysis": {"total_failures": 1},
            "data_summary": {"total_test_cases": 2}
        }
        
        # ì‹œê°í™” ì˜¤ë¥˜ ëª¨í‚¹
        mock_viz = MagicMock()
        mock_viz_class.return_value = mock_viz
        mock_viz.generate_comprehensive_dashboard.side_effect = Exception("Visualization error")
        mock_viz.create_summary_report.side_effect = Exception("Summary error")
        
        # ì—”ì§„ì˜ visualizerë¥¼ ëª¨í‚¹ëœ ê°ì²´ë¡œ êµì²´
        self.engine.visualizer = mock_viz
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ì‹œê°í™” ì˜¤ë¥˜ê°€ ìˆì–´ë„ ë¶„ì„ì´ ê³„ì†ë˜ì–´ì•¼ í•¨
        result = self.engine.analyze_session(str(session_dir))
        
        # ê¸°ë³¸ ë¶„ì„ ê²°ê³¼ëŠ” ì—¬ì „íˆ ìƒì„±ë˜ì–´ì•¼ í•¨
        self.assertIn("analysis_metadata", result)
        self.assertIn("files_generated", result)
        self.assertIn("markdown_report", result["files_generated"])
        self.assertIn("json_data", result["files_generated"])
        
        # ì‹œê°í™” íŒŒì¼ ëª©ë¡ì´ ë¹ˆ ê²ƒì´ì–´ì•¼ í•¨
        self.assertEqual(result["files_generated"]["visualization_files"], [])
    
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VersionComparisonAnalyzer')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.TechStackAnalyzer')
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VisualizationGenerator')
    def test_analyze_multiple_sessions(self, mock_viz_class, mock_tech_class, mock_version_class):
        """ë‹¤ì¤‘ ì„¸ì…˜ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        base_dir = self.temp_path / "multi_sessions"
        base_dir.mkdir()
        
        # ëª¨í‚¹ ì„¤ì •
        mock_version_analyzer = MagicMock()
        mock_tech_analyzer = MagicMock()
        mock_viz = MagicMock()
        
        mock_version_class.return_value = mock_version_analyzer
        mock_tech_class.return_value = mock_tech_analyzer  
        mock_viz_class.return_value = mock_viz
        
        # ë²„ì „ ë°ì´í„° ëª¨í‚¹
        mock_version_data = {
            "v1.0.0": {
                "sessions": [{"session_dir": "session1", "results": []}]
            }
        }
        mock_version_analyzer.collect_version_data.return_value = mock_version_data
        mock_version_analyzer.analyze_version_progression.return_value = {
            "version_timeline": [],
            "performance_trends": {},
            "version_recommendations": {"recommended_version": "v1.0.0"}
        }
        
        # ê¸°ìˆ ìŠ¤íƒ ë¶„ì„ ëª¨í‚¹
        mock_tech_analyzer.analyze_tech_stack_performance.return_value = {
            "by_tech_stack": {},
            "recommendations": ["Tech recommendation"]
        }
        
        # ì‹œê°í™” ëª¨í‚¹
        mock_viz.generate_comprehensive_dashboard.return_value = ["multi_chart.html"]
        
        # ì—”ì§„ì˜ ë¶„ì„ê¸°ë“¤ì„ ëª¨í‚¹ëœ ê°ì²´ë¡œ êµì²´
        self.engine.version_analyzer = mock_version_analyzer
        self.engine.tech_stack_analyzer = mock_tech_analyzer
        self.engine.visualizer = mock_viz
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine.analyze_multiple_sessions(str(base_dir))
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("analysis_metadata", result)
        self.assertIn("files_generated", result)
        self.assertEqual(result["analysis_metadata"]["analysis_type"], "multi_session")
        
        # ë¶„ì„ê¸° ë©”ì„œë“œë“¤ì´ í˜¸ì¶œë˜ì—ˆëŠ”ì§€ í™•ì¸
        mock_version_analyzer.collect_version_data.assert_called_once()
        mock_version_analyzer.analyze_version_progression.assert_called_once()
        mock_tech_analyzer.analyze_tech_stack_performance.assert_called_once()
    
    def test_generate_markdown_report(self):
        """ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼
        analysis_results = {
            "analysis_type": "single_session",
            "data_summary": {
                "total_test_cases": 10,
                "successful_evaluations": 8,
                "failed_evaluations": 2,
                "models_count": 2
            },
            "model_comparison": {
                "recommendations": ["Best model: gpt-4", "Good performance overall"],
                "comparison_table": {
                    "table_data": [
                        {
                            "model_name": "gpt-4",
                            "overall_score": 0.85,
                            "grade": "A",
                            "overall_rank": 1,
                            "correctness_score": 0.90,
                            "clarity_score": 0.80,
                            "actionability_score": 0.85,
                            "json_correctness_score": 1.0
                        }
                    ]
                }
            },
            "failure_analysis": {
                "total_failures": 2,
                "by_metric": {
                    "correctness": {"total_failures": 1, "failure_rate": 0.1, "avg_confidence": 0.85}
                },
                "critical_patterns": [
                    {"category": "missing_issues", "count": 1, "percentage": 50.0, "reason": "Failed to identify issues"}
                ]
            }
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        markdown = self.engine._generate_markdown_report(analysis_results)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("# DeepEval ë¶„ì„ ë³´ê³ ì„œ (V2)", markdown)
        self.assertIn("ğŸ“Š ë°ì´í„° ìš”ì•½", markdown)
        self.assertIn("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", markdown)
        self.assertIn("ğŸ” ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„", markdown)
        self.assertIn("ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­", markdown)
        
        # ë°ì´í„° ë‚´ìš© ê²€ì¦
        self.assertIn("10", markdown)  # ì´ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        self.assertIn("gpt-4", markdown)  # ëª¨ë¸ëª…
        self.assertIn("0.850", markdown)  # ì ìˆ˜
    
    def test_generate_multi_session_markdown_report(self):
        """ë‹¤ì¤‘ ì„¸ì…˜ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # ìƒ˜í”Œ í†µí•© ê²°ê³¼
        integrated_results = {
            "analysis_type": "multi_session",
            "data_summary": {
                "total_versions": 3,
                "total_sessions": 10
            },
            "version_analysis": {
                "version_recommendations": {
                    "recommended_version": "v1.2.0",
                    "recommendations": ["Version recommendation 1", "Version recommendation 2"]
                }
            },
            "tech_stack_analysis": {
                "recommendations": ["Tech recommendation 1", "Tech recommendation 2"]
            }
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        markdown = self.engine._generate_multi_session_markdown_report(integrated_results)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("# ë‹¤ì¤‘ ì„¸ì…˜ ë¶„ì„ ë³´ê³ ì„œ", markdown)
        self.assertIn("ğŸ“Š ë¶„ì„ ê°œìš”", markdown)
        self.assertIn("ğŸ“ˆ ë²„ì „ë³„ ì„±ëŠ¥ ë¶„ì„", markdown)
        self.assertIn("ğŸ›  ê¸°ìˆ ìŠ¤íƒë³„ ì„±ëŠ¥ ë¶„ì„", markdown)
        self.assertIn("ğŸ’¡ í†µí•© ê¶Œì¥ì‚¬í•­", markdown)
        
        # ë°ì´í„° ë‚´ìš© ê²€ì¦
        self.assertIn("v1.2.0", markdown)  # ê¶Œì¥ ë²„ì „
        self.assertIn("3", markdown)  # ë²„ì „ ìˆ˜
        self.assertIn("10", markdown)  # ì„¸ì…˜ ìˆ˜
    
    def test_error_handling_no_session_path(self):
        """ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì„¸ì…˜ ê²½ë¡œ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        non_existent_path = "/non/existent/path"
        
        with self.assertRaises(FileNotFoundError):
            self.engine.analyze_session(non_existent_path)
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    def test_error_handling_no_results(self, mock_collect):
        """ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "empty_session"
        session_dir.mkdir()
        
        # ë¹ˆ ê²°ê³¼ ë°˜í™˜ ëª¨í‚¹
        mock_collect.return_value = {}
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²€ì¦
        with self.assertRaises(ValueError):
            self.engine.analyze_session(str(session_dir))
    
    def test_error_handling_no_base_path(self):
        """ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê¸°ë³¸ ê²½ë¡œ ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        non_existent_path = "/non/existent/base/path"
        
        with self.assertRaises(FileNotFoundError):
            self.engine.analyze_multiple_sessions(non_existent_path)
    
    @patch('selvage_eval.analysis.deepeval_analysis_engine.VersionComparisonAnalyzer')
    def test_error_handling_no_version_data(self, mock_version_class):
        """ë²„ì „ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì—ëŸ¬ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        base_dir = self.temp_path / "empty_base"
        base_dir.mkdir()
        
        # ë¹ˆ ë²„ì „ ë°ì´í„° ë°˜í™˜ ëª¨í‚¹
        mock_version_analyzer = MagicMock()
        mock_version_class.return_value = mock_version_analyzer
        mock_version_analyzer.collect_version_data.return_value = {}
        
        # ì—”ì§„ì˜ ë²„ì „ ë¶„ì„ê¸°ë¥¼ ëª¨í‚¹ëœ ê°ì²´ë¡œ êµì²´
        self.engine.version_analyzer = mock_version_analyzer
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë° ê²€ì¦
        with self.assertRaises(ValueError):
            self.engine.analyze_multiple_sessions(str(base_dir))
    
    def test_error_handling_log_parsing_failure(self):
        """ë¡œê·¸ íŒŒì‹± ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "parse_error_session"
        session_dir.mkdir()
        
        # ë¡œê·¸ íŒŒì¼ ìƒì„± (ì‹¤ì œ ë‚´ìš©ì€ íŒŒì„œì— ì˜í•´ ì²˜ë¦¬ë¨)
        log_file = session_dir / "deepeval_results_error_model.log"
        log_file.write_text("some log content")
        
        # ë¡œê·¸ íŒŒì„œê°€ ì˜ˆì™¸ë¥¼ ë˜ì§€ë„ë¡ ëª¨í‚¹
        original_parser = self.engine.log_parser
        self.engine.log_parser = MagicMock()
        self.engine.log_parser.parse_log_file.side_effect = Exception("Log parsing failed")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        results = self.engine._collect_log_results(session_dir)
        
        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í•´ë‹¹ íŒŒì¼ì€ ê²°ê³¼ì—ì„œ ì œì™¸ë˜ì–´ì•¼ í•¨
        self.assertEqual(results, {})
        
        # ì›ë˜ íŒŒì„œ ë³µì›
        self.engine.log_parser = original_parser
    
    def test_error_handling_comprehensive_analysis_failure(self):
        """ì¢…í•© ë¶„ì„ ì‹¤íŒ¨ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
        log_results = {"gpt_4": self.sample_test_results}
        
        # ëª¨ë¸ ë¹„êµê¸°ê°€ ì˜ˆì™¸ë¥¼ ë˜ì§€ë„ë¡ ëª¨í‚¹
        original_comparator = self.engine.model_comparator
        self.engine.model_comparator = MagicMock()
        self.engine.model_comparator.compare_models.side_effect = Exception("Model comparison failed")
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine._perform_comprehensive_analysis(log_results)
        
        # ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ì •ë³´ê°€ ë°˜í™˜ë˜ì–´ì•¼ í•¨
        self.assertIn("error", result)
        self.assertIn("ë¶„ì„ ì‹¤íŒ¨", result["error"])
        
        # ì›ë˜ ë¹„êµê¸° ë³µì›
        self.engine.model_comparator = original_comparator
    
    def test_error_handling_incomplete_metric_data(self):
        """ë¶ˆì™„ì „í•œ ë©”íŠ¸ë¦­ ë°ì´í„° ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        from selvage_eval.analysis.deepeval_log_parser import TestCaseResult, MetricScore
        
        # ì¼ë¶€ ë©”íŠ¸ë¦­ì´ ëˆ„ë½ëœ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
        incomplete_results = [
            TestCaseResult(
                correctness=MetricScore(score=0.8, passed=True, reason="Good"),
                clarity=MetricScore(score=0.0, passed=False, reason=""),  # ë¹ˆ ë°ì´í„°
                actionability=MetricScore(score=0.7, passed=True, reason="Actionable"),
                json_correctness=MetricScore(score=1.0, passed=True, reason="Valid JSON"),
                input_data="Test input",
                actual_output="Test output",
                raw_content="Raw content"
            )
        ]
        
        log_results = {"incomplete_model": incomplete_results}
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine._perform_comprehensive_analysis(log_results)
        
        # ë¶ˆì™„ì „í•œ ë°ì´í„°ë¼ë„ ë¶„ì„ì´ ì™„ë£Œë˜ì–´ì•¼ í•¨
        self.assertEqual(result["analysis_type"], "single_session")
        self.assertIn("model_comparison", result)
        self.assertIn("failure_analysis", result)
        self.assertIn("data_summary", result)
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ë¡œ ë¶„ë¥˜ë˜ì–´ì•¼ í•¨
        self.assertEqual(result["data_summary"]["failed_evaluations"], 1)
    
    def test_error_handling_memory_pressure(self):
        """ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜ í…ŒìŠ¤íŠ¸"""
        # ëŒ€ìš©ëŸ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
        large_results = []
        for i in range(1000):  # ë§ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
            large_results.append(TestCaseResult(
                correctness=MetricScore(score=0.8, passed=True, reason=f"Good analysis {i}"),
                clarity=MetricScore(score=0.75, passed=True, reason=f"Clear output {i}"),
                actionability=MetricScore(score=0.7, passed=True, reason=f"Actionable {i}"),
                json_correctness=MetricScore(score=1.0, passed=True, reason=f"Valid JSON {i}"),
                input_data=f"Large test input {i}" * 100,  # í° ë°ì´í„°
                actual_output=f"Large test output {i}" * 100,
                raw_content=f"Large raw content {i}" * 100
            ))
        
        log_results = {"large_model": large_results}
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤íŒ¨í•˜ì§€ ì•Šì•„ì•¼ í•¨
        result = self.engine._perform_comprehensive_analysis(log_results)
        
        # ëŒ€ìš©ëŸ‰ ë°ì´í„°ë„ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        self.assertEqual(result["analysis_type"], "single_session")
        self.assertEqual(result["data_summary"]["total_test_cases"], 1000)
    
    def test_error_handling_file_permission_error(self):
        """íŒŒì¼ ê¶Œí•œ ì˜¤ë¥˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
        # ê¶Œí•œì´ ì—†ëŠ” ë””ë ‰í† ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        restricted_dir = self.temp_path / "restricted_session"
        restricted_dir.mkdir()
        
        # ë¡œê·¸ íŒŒì¼ ìƒì„±
        log_file = restricted_dir / "deepeval_results_restricted.log"
        log_file.write_text("test content")
        
        # íŒŒì¼ ì‹œìŠ¤í…œ ì‘ì—…ì´ ì‹¤íŒ¨í•˜ë„ë¡ ëª¨í‚¹
        with patch('builtins.open', side_effect=PermissionError("Permission denied")):
            # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            results = self.engine._collect_log_results(restricted_dir)
            
            # ê¶Œí•œ ì˜¤ë¥˜ ì‹œ í•´ë‹¹ íŒŒì¼ì€ ê²°ê³¼ì—ì„œ ì œì™¸ë˜ì–´ì•¼ í•¨
            self.assertEqual(results, {})
    
    def test_error_handling_corrupted_data_structures(self):
        """ì†ìƒëœ ë°ì´í„° êµ¬ì¡° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
        # ì˜ëª»ëœ êµ¬ì¡°ì˜ ë¡œê·¸ ê²°ê³¼ ì‹œë®¬ë ˆì´ì…˜
        corrupted_results = [
            # ì •ìƒì ì¸ ê²°ê³¼
            self.sample_test_results[0],
            # ì†ìƒëœ ê²°ê³¼ (ì˜ëª»ëœ ì ìˆ˜ ê°’)
            TestCaseResult(
                correctness=MetricScore(score=-1.0, passed=False, reason="Invalid score"),  # ì˜ëª»ëœ ì ìˆ˜
                clarity=MetricScore(score=0.8, passed=True, reason="Clear"),
                actionability=MetricScore(score=0.7, passed=True, reason="Actionable"),
                json_correctness=MetricScore(score=1.0, passed=True, reason="Valid JSON"),
                input_data="Test input",
                actual_output="Test output",
                raw_content="Raw content"
            )
        ]
        
        log_results = {"corrupted_model": corrupted_results}
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ì†ìƒëœ ë°ì´í„°ê°€ ìˆì–´ë„ ë¶„ì„ì´ ê³„ì†ë˜ì–´ì•¼ í•¨
        result = self.engine._perform_comprehensive_analysis(log_results)
        
        # ë¶„ì„ì´ ì™„ë£Œë˜ì–´ì•¼ í•˜ê³ , ì†ìƒëœ ë°ì´í„°ëŠ” ì ì ˆíˆ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        self.assertEqual(result["analysis_type"], "single_session")
        self.assertIn("model_comparison", result)
        self.assertIn("failure_analysis", result)
    
    def test_end_to_end_integration_workflow(self):
        """ì—”ë“œíˆ¬ì—”ë“œ í†µí•© ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
        # ë³µí•©ì ì¸ ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤ ì‹œë®¬ë ˆì´ì…˜
        session_dir = self.temp_path / "e2e_session"
        session_dir.mkdir()
        
        # ì—¬ëŸ¬ ëª¨ë¸ì˜ ë‹¤ì–‘í•œ ì„±ëŠ¥ ë¡œê·¸ ìƒì„±
        model_logs = {
            "gpt_4": {
                "test_cases": [
                    {"correctness": 0.95, "clarity": 0.9, "actionability": 0.85, "json": 1.0},
                    {"correctness": 0.9, "clarity": 0.85, "actionability": 0.8, "json": 0.95},
                    {"correctness": 0.85, "clarity": 0.8, "actionability": 0.75, "json": 1.0}
                ]
            },
            "claude_3": {
                "test_cases": [
                    {"correctness": 0.8, "clarity": 0.95, "actionability": 0.9, "json": 1.0},
                    {"correctness": 0.85, "clarity": 0.9, "actionability": 0.85, "json": 0.98},
                    {"correctness": 0.75, "clarity": 0.85, "actionability": 0.8, "json": 1.0}
                ]
            },
            "gemini_pro": {
                "test_cases": [
                    {"correctness": 0.7, "clarity": 0.8, "actionability": 0.7, "json": 0.9},
                    {"correctness": 0.75, "clarity": 0.75, "actionability": 0.65, "json": 0.85},
                    {"correctness": 0.6, "clarity": 0.7, "actionability": 0.6, "json": 0.8}
                ]
            }
        }
        
        # ê° ëª¨ë¸ì˜ ë¡œê·¸ íŒŒì¼ ìƒì„±
        for model_name, model_data in model_logs.items():
            log_content_parts = []
            for i, test_case in enumerate(model_data["test_cases"]):
                log_content_parts.append(f"""==================================================
Test Case: E2E Test Case {i+1} for {model_name}
Input: Complex input data for {model_name} test {i+1}
Expected: Expected comprehensive output {i+1}
Actual: Generated output by {model_name} for test {i+1}

âœ… Correctness (score: {test_case["correctness"]}, reason: "Analysis quality assessment", error: None)
âœ… Clarity (score: {test_case["clarity"]}, reason: "Output clarity evaluation", error: None)
âœ… Actionability (score: {test_case["actionability"]}, reason: "Actionability assessment", error: None)
âœ… JSON Correctness (score: {test_case["json"]}, reason: "JSON format validation", error: None)
==================================================""")
            
            log_file = session_dir / f"deepeval_results_{model_name}.log"
            log_file.write_text("\n".join(log_content_parts))
        
        # ì „ì²´ ë¶„ì„ ì‹¤í–‰
        result = self.engine.analyze_session(str(session_dir))
        
        # í¬ê´„ì ì¸ ê²°ê³¼ ê²€ì¦
        self.assertIn("analysis_metadata", result)
        self.assertIn("files_generated", result)
        
        # ë©”íƒ€ë°ì´í„° ê²€ì¦
        metadata = result["analysis_metadata"]
        # ë¡œê·¸ íŒŒì„œê°€ ê° ëª¨ë¸ë‹¹ í•˜ë‚˜ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë§Œ íŒŒì‹±í•˜ë¯€ë¡œ ì´ 3ê°œ
        self.assertEqual(metadata["total_test_cases"], 3)  # 3 ëª¨ë¸ Ã— 1 í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ (ë¡œê·¸ íŒŒì„œ ë™ì‘)
        self.assertEqual(len(metadata["models_analyzed"]), 3)
        self.assertIn("gpt_4", metadata["models_analyzed"])
        self.assertIn("claude_3", metadata["models_analyzed"])
        self.assertIn("gemini_pro", metadata["models_analyzed"])
        
        # ìƒì„±ëœ íŒŒì¼ ê²€ì¦
        files = result["files_generated"]
        for file_type in ["markdown_report", "json_data"]:
            self.assertIn(file_type, files)
            file_path = Path(files[file_type])
            self.assertTrue(file_path.exists())
            self.assertGreater(file_path.stat().st_size, 0)  # íŒŒì¼ì´ ë¹„ì–´ìˆì§€ ì•Šì€ì§€
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ë‚´ìš© ê²€ì¦
        markdown_path = Path(files["markdown_report"])
        with open(markdown_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        self.assertIn("# DeepEval ë¶„ì„ ë³´ê³ ì„œ (V2)", markdown_content)
        self.assertIn("ğŸ“Š ë°ì´í„° ìš”ì•½", markdown_content)
        self.assertIn("ğŸ† ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ", markdown_content)
        self.assertIn("ğŸ’¡ ê²°ë¡  ë° ê¶Œì¥ì‚¬í•­", markdown_content)
        
        # JSON ë°ì´í„° ê²€ì¦
        json_path = Path(files["json_data"])
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        self.assertIn("model_comparison", json_data)
        self.assertIn("failure_analysis", json_data)
        self.assertIn("data_summary", json_data)
        self.assertEqual(json_data["data_summary"]["total_test_cases"], 3)
    
    def test_performance_benchmark(self):
        """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        import time
        
        # ì¤‘ê°„ ê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±
        session_dir = self.temp_path / "performance_session"
        session_dir.mkdir()
        
        # 100ê°œì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¥¼ ê°€ì§„ ë¡œê·¸ ìƒì„±
        test_cases = []
        for i in range(100):
            test_cases.append(f"""==================================================
Test Case: Performance Test {i+1}
Input: Performance input data {i+1}
Expected: Performance expected output {i+1}
Actual: Performance actual output {i+1}

âœ… Correctness (score: {0.7 + (i % 3) * 0.1}, reason: "Performance test {i+1}", error: None)
âœ… Clarity (score: {0.75 + (i % 4) * 0.05}, reason: "Clarity test {i+1}", error: None)
âœ… Actionability (score: {0.6 + (i % 5) * 0.08}, reason: "Actionability test {i+1}", error: None)
âœ… JSON Correctness (score: {0.9 + (i % 2) * 0.05}, reason: "JSON test {i+1}", error: None)
==================================================""")
        
        log_file = session_dir / "deepeval_results_performance_model.log"
        log_file.write_text("\n".join(test_cases))
        
        # ì„±ëŠ¥ ì¸¡ì •
        start_time = time.time()
        result = self.engine.analyze_session(str(session_dir))
        end_time = time.time()
        
        execution_time = end_time - start_time
        
        # ì„±ëŠ¥ ê²€ì¦
        self.assertLess(execution_time, 30.0)  # 30ì´ˆ ì´ë‚´ì— ì™„ë£Œë˜ì–´ì•¼ í•¨
        
        # ê²°ê³¼ ì •í™•ì„± ê²€ì¦
        # ë¡œê·¸ íŒŒì„œê°€ í•˜ë‚˜ì˜ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë¡œ ì²˜ë¦¬í•¨
        self.assertEqual(result["analysis_metadata"]["total_test_cases"], 1)
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°„ì ‘ ê²€ì¦ (ê²°ê³¼ê°€ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë¨)
        files = result["files_generated"]
        self.assertIn("markdown_report", files)
        self.assertIn("json_data", files)
        
        # ìƒì„±ëœ íŒŒì¼ í¬ê¸° ê²€ì¦
        markdown_path = Path(files["markdown_report"])
        json_path = Path(files["json_data"])
        
        self.assertGreater(markdown_path.stat().st_size, 1000)  # ìµœì†Œ 1KB
        self.assertGreater(json_path.stat().st_size, 500)  # ìµœì†Œ 500B
        
        # ì‹¤í–‰ ì‹œê°„ ì •ë³´ ë¡œê·¸ (ë””ë²„ê¹…ìš©)
        print(f"Performance test completed in {execution_time:.2f} seconds")
        print(f"Processed {100} test cases")
        print(f"Average time per test case: {execution_time/100:.4f} seconds")
    
    def test_extract_repo_name_from_session_path(self):
        """ì„¸ì…˜ ê²½ë¡œì—ì„œ ì €ì¥ì†Œëª… ì¶”ì¶œ í…ŒìŠ¤íŠ¸"""
        # ì•Œë ¤ì§„ ì €ì¥ì†Œëª…ì´ í¬í•¨ëœ ê²½ë¡œ
        session_path1 = Path("/path/to/cline-evaluation/session1")
        repo_name1 = self.engine._extract_repo_name_from_session_path(session_path1)
        self.assertEqual(repo_name1, "cline-evaluation")
        
        session_path2 = Path("/path/to/ecommerce-microservices/session2")
        repo_name2 = self.engine._extract_repo_name_from_session_path(session_path2)
        self.assertEqual(repo_name2, "ecommerce-microservices")
        
        # ì•Œë ¤ì§€ì§€ ì•Šì€ ì €ì¥ì†Œëª…
        session_path3 = Path("/path/to/unknown-repo/session3")
        repo_name3 = self.engine._extract_repo_name_from_session_path(session_path3)
        self.assertEqual(repo_name3, "session3")  # ë§ˆì§€ë§‰ ë””ë ‰í† ë¦¬ëª… ì‚¬ìš©
        
        # ì¶”ê°€ í…ŒìŠ¤íŠ¸: ë‹¤ë¥¸ ì•Œë ¤ì§„ íŒ¨í„´ë“¤
        session_path4 = Path("/home/user/kotlin-project/test-session")
        repo_name4 = self.engine._extract_repo_name_from_session_path(session_path4)
        self.assertEqual(repo_name4, "kotlin-project")
        
        session_path5 = Path("/workspace/selvage-eval/session_20240101")
        repo_name5 = self.engine._extract_repo_name_from_session_path(session_path5)
        self.assertEqual(repo_name5, "selvage-eval")
    
    def test_prepare_repo_results_from_versions(self):
        """ë²„ì „ ë°ì´í„°ì—ì„œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""
        # ìƒ˜í”Œ ë²„ì „ ë°ì´í„° ì¤€ë¹„
        version_data = {
            "v1.0.0": {
                "sessions": [
                    {
                        "session_dir": "/path/to/cline-evaluation/session1",
                        "results": [self.sample_test_results[0]]
                    },
                    {
                        "session_dir": "/path/to/ecommerce-microservices/session2", 
                        "results": [self.sample_test_results[1]]
                    }
                ]
            },
            "v1.1.0": {
                "sessions": [
                    {
                        "session_dir": "/path/to/cline-evaluation/session3",
                        "results": self.sample_test_results
                    }
                ]
            }
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        repo_results = self.engine._prepare_repo_results_from_versions(version_data)
        
        # ê²°ê³¼ ê²€ì¦
        self.assertIn("cline-evaluation", repo_results)
        self.assertIn("ecommerce-microservices", repo_results)
        
        # cline-evaluation ì €ì¥ì†Œ ê²€ì¦
        cline_repo = repo_results["cline-evaluation"]
        self.assertIn("version_v1.0.0", cline_repo)
        self.assertIn("version_v1.1.0", cline_repo)
        self.assertEqual(len(cline_repo["version_v1.0.0"]), 1)
        self.assertEqual(len(cline_repo["version_v1.1.0"]), 2)
        
        # ecommerce-microservices ì €ì¥ì†Œ ê²€ì¦
        ecommerce_repo = repo_results["ecommerce-microservices"]
        self.assertIn("version_v1.0.0", ecommerce_repo)
        self.assertEqual(len(ecommerce_repo["version_v1.0.0"]), 1)
    
    def test_prepare_repo_results_from_versions_empty_data(self):
        """ë¹ˆ ë²„ì „ ë°ì´í„°ì— ëŒ€í•œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""
        version_data = {}
        
        repo_results = self.engine._prepare_repo_results_from_versions(version_data)
        
        self.assertEqual(repo_results, {})
    
    def test_prepare_repo_results_from_versions_no_results(self):
        """ê²°ê³¼ê°€ ì—†ëŠ” ì„¸ì…˜ì— ëŒ€í•œ ì €ì¥ì†Œë³„ ê²°ê³¼ ì¤€ë¹„ í…ŒìŠ¤íŠ¸"""
        version_data = {
            "v1.0.0": {
                "sessions": [
                    {
                        "session_dir": "/path/to/test-repo/session1",
                        "results": []
                    }
                ]
            }
        }
        
        repo_results = self.engine._prepare_repo_results_from_versions(version_data)
        
        # ë¹ˆ ê²°ê³¼ë„ ì²˜ë¦¬ë˜ì–´ì•¼ í•¨
        # ë¹ˆ ê²°ê³¼ëŠ” ë”•ì…”ë„ˆë¦¬ì— í¬í•¨ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        if "session1" in repo_results:
            if "version_v1.0.0" in repo_results["session1"]:
                self.assertEqual(len(repo_results["session1"]["version_v1.0.0"]), 0)
        # ë¹ˆ ê²°ê³¼ë¡œ ì¸í•´ ì•„ë¬´ê²ƒë„ ìƒì„±ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
        self.assertIsInstance(repo_results, dict)
    
    @patch.object(DeepEvalAnalysisEngine, '_collect_log_results')
    @patch.object(DeepEvalAnalysisEngine, '_perform_comprehensive_analysis')
    def test_output_directory_creation(self, mock_analysis, mock_collect):
        """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìë™ ìƒì„± í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ìƒì„±
        session_dir = self.temp_path / "test_session"
        session_dir.mkdir()
        
        # ì‚¬ìš©ì ì§€ì • ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì¡´ì¬í•˜ì§€ ì•ŠìŒ)
        custom_output_dir = self.temp_path / "custom_output" / "nested"
        
        # ëª¨í‚¹ ì„¤ì •
        mock_collect.return_value = {"gpt_4": self.sample_test_results}
        mock_analysis.return_value = {
            "analysis_type": "single_session",
            "model_comparison": {"model_statistics": {"gpt_4": {}}},
            "failure_analysis": {"total_failures": 1},
            "data_summary": {"total_test_cases": 2}
        }
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        result = self.engine.analyze_session(str(session_dir), str(custom_output_dir))
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ê°€ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        self.assertTrue(custom_output_dir.exists())
        self.assertTrue(custom_output_dir.is_dir())
        
        # íŒŒì¼ë“¤ì´ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
        markdown_path = Path(result["files_generated"]["markdown_report"])
        self.assertTrue(str(markdown_path).startswith(str(custom_output_dir)))


if __name__ == "__main__":
    unittest.main()